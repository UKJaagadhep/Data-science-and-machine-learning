{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN14g/WXWzMX4PtQaCH/eZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UKJaagadhep/Data-science-and-machine-learning/blob/main/Neural_Machine_Translation/Neural_Machine_translation_(English_to_French)_with_Bahdanau_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kuc6CTXgN3P8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Input, GRU, Embedding, Bidirectional, Dropout, Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIwxU-tCBJaf",
        "outputId": "21b70e64-272e-417a-b7ed-9beb02f521b7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-11 22:20:09--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  6.38MB/s    in 1.2s    \n",
            "\n",
            "2024-05-11 22:20:11 (6.38 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "lnElkFWyFAXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = tf.data.TextLineDataset('/content/dataset/fra.txt')"
      ],
      "metadata": {
        "id": "ny1hQhXIBNKS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selector(input_text):\n",
        "  split_text = tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1' : split_text[0:1], 'input_2' : 'starttoken ' + split_text[1:2]}, split_text[1:2] + ' endtoken'\n",
        "  #We specify [0:1] instead of just [0] to get output in the form of a vector (enclosed by []) and not a scaler\n",
        "  #Dictionary contains inputs and  split_text[1:2] + ' [end]' is output\n",
        "  '''So for each sentence in french sequence, we will have [start] token and the sentence representing inputs to the\n",
        "  French output RNN from itself within the dictionary and we will also have the sentence and [end] token representing\n",
        "  the outputs from the French output RNN'''\n",
        "print(selector('Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KAtuxuaBPq2",
        "outputId": "27d70c41-5580-4ff9-b57e-23cf290ab47d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = text_dataset.map(selector) #We use split dataset into inputs and outputs (only input_1 is given by user)"
      ],
      "metadata": {
        "id": "cUvAqtiMBTqw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separator(input_text):\n",
        "  split_text = tf.strings.split(input_text, '\\t')\n",
        "  return split_text[0:1], 'starttoken ' + split_text[1:2] + ' endtoken'"
      ],
      "metadata": {
        "id": "Xf7Ur79OBZwe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset = text_dataset.map(separator)\n",
        "#We split dataset into english and french (alongwith starttoken and endtoken for french) to get the vocabulary for the 2 languages"
      ],
      "metadata": {
        "id": "rFKAKIzyBcLF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 20000\n",
        "english_sequence_length = 64\n",
        "french_sequence_length = 64\n",
        "embedding_dimension = 300\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "sioXeGS5BegW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorization_layer = TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation',\n",
        "    max_tokens = vocabulary_size,\n",
        "    output_sequence_length = english_sequence_length,\n",
        "    output_mode = 'int'\n",
        ")"
      ],
      "metadata": {
        "id": "xmdHmcFiBhEg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorization_layer = TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation',\n",
        "    max_tokens = vocabulary_size,\n",
        "    output_sequence_length = french_sequence_length,\n",
        "    output_mode = 'int'\n",
        ")"
      ],
      "metadata": {
        "id": "2_zhRGI0Bkyu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data = init_dataset.map(lambda x, y : x)\n",
        "english_vectorization_layer.adapt(english_training_data)"
      ],
      "metadata": {
        "id": "Au7B3xZjBlmT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_training_data = init_dataset.map(lambda x, y : y)\n",
        "french_vectorization_layer.adapt(french_training_data)"
      ],
      "metadata": {
        "id": "jtSM094pBnsJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(inputs, output):\n",
        "  return {'input_1' : english_vectorization_layer(inputs['input_1']),\n",
        "          'input_2' : french_vectorization_layer(inputs['input_2'])}, french_vectorization_layer(output)"
      ],
      "metadata": {
        "id": "03qZYE84Bp2v"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = split_dataset.map(vectorizer)"
      ],
      "metadata": {
        "id": "8JS2UiyMBs0l"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking indeices for starttoken and endtoken\n",
        "print(french_vectorization_layer.get_vocabulary()[2])\n",
        "print(french_vectorization_layer.get_vocabulary()[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbJWLLgSBu9-",
        "outputId": "c28e1cbf-a8f3-4de7-8fd9-2a13e54cf958"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starttoken\n",
            "endtoken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(2048).unbatch().batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "ZSYmBADIByH0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = int(200000/batch_size)\n",
        "print(num_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLEo6RoRB0VL",
        "outputId": "1f71a8e0-d58e-4dcc-e8e2-2951c9a9ed98"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.take(int(0.88 * num_batches))\n",
        "temp_dataset = dataset.skip(int(0.88 * num_batches))\n",
        "val_dataset = temp_dataset.take(int(0.67 * num_batches))\n",
        "test_dataset = temp_dataset.skip(int(0.67 * num_batches))"
      ],
      "metadata": {
        "id": "OY1gFZW0B2Zb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWum5-koB6o4",
        "outputId": "c8ca2cad-4b6a-48ad-c2e8-71f95619a04d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YCC9dT_B7P-",
        "outputId": "f17e035c-754d-4049-b876-e0fa6a4a1b21"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkHAJG7JB9X3",
        "outputId": "e1aa80ae-602d-455f-b7a5-a41b68f7f14a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRLpRrIjB_Nm",
        "outputId": "581fafbc-22fe-4719-dd5e-6dd726ae3cc8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_SkipDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelling**"
      ],
      "metadata": {
        "id": "SlRlt1bpWzeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocabulary_size, embedding_dimension, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "    self.vocabulary_size = vocabulary_size\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = Embedding(self.vocabulary_size, self.embedding_dimension)\n",
        "    self.lstm = LSTM(self.units, return_sequences = True)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    #print(x.shape)\n",
        "    output = self.lstm(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "X7x3XXKcCGR9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 256\n",
        "hidden_units = 256\n",
        "\n",
        "encoder = Encoder(vocabulary_size, embedding_dimension, hidden_units)\n",
        "encoder_output = encoder(tf.zeros([128, 64])) #128 here is batch_size and 64 in sequence_length\n",
        "print(encoder_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5GCidqu534",
        "outputId": "e8a2f17e-76bc-49d6-ed0e-9930caf170f5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 64, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXMpn8lNvky7",
        "outputId": "ed59ce9a-a40a-4334-bcf2-8386bc481ada"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     multiple                  5120000   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               multiple                  525312    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5645312 (21.54 MB)\n",
            "Trainable params: 5645312 (21.54 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Refer to formula in the research paper for clearer understanding\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w_1 = Dense(self.units)\n",
        "    self.w_2 = Dense(self.units)\n",
        "    self.w = Dense(1)\n",
        "\n",
        "  def call(self, prev_dec_state, enc_states):\n",
        "    scores = self.w(tf.nn.tanh(\n",
        "        self.w_1(tf.expand_dims(prev_dec_state, -2)) + #Expand dimension to convert prev_dec_state from 2D to 3D by adding sequence_length dimension\n",
        "        self.w_2(enc_states)\n",
        "    )) #shape = [batch_size, sequence_length, 1]\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scores, axis = 1)\n",
        "    context_vector = attention_weights * enc_states #shape = [batch_size, sequence_length, embedding_dimension]\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1) #shape = [batch_size, embedding_dimension]\n",
        "\n",
        "    return context_vector , attention_weights"
      ],
      "metadata": {
        "id": "in4DQF1ixlP-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bahdanau_attention=BahdanauAttention(256)\n",
        "context_vector,attention_weights=bahdanau_attention(tf.zeros([128,32]),tf.zeros([128,8,32]))\n",
        "print(context_vector.shape)\n",
        "print(attention_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCpP_HphNZCe",
        "outputId": "c8d68166-ff59-41d0-d099-46452f0e9a24"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 32)\n",
            "(128, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocabulary_size, embedding_dimension, decoder_units, sequence_length):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.vocabulary_size = vocabulary_size\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "    self.decoder_units = decoder_units\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.gru = GRU(self.decoder_units, return_sequences = True, return_state = True)\n",
        "    #return_sequences = True to output french words, return_state = True to send previous hidden state (S[i-1]) to encoder block\n",
        "    self.dense = Dense(self.vocabulary_size, activation = 'softmax')\n",
        "    self.embedding = Embedding(self.vocabulary_size, self.embedding_dimension) #To embed shifted target\n",
        "    self.attention = BahdanauAttention(self.decoder_units)\n",
        "\n",
        "  def call(self, x, previous_hidden, shifted_target):\n",
        "    outputs=[]\n",
        "    context_vectors=[]\n",
        "    attention_weightss=[]\n",
        "    shifted_target=self.embedding(shifted_target)\n",
        "\n",
        "    for t in range(self.sequence_length):\n",
        "      context_vector, attention_weights = self.attention(previous_hidden, x) #x is encoder output\n",
        "      decoder_input = context_vector + shifted_target[:, t] #block t\n",
        "      output, previous_hidden = self.gru(tf.expand_dims(decoder_input, 1))\n",
        "      outputs.append(output[:, 0]) #output shape = [batch_size, decoder_units]\n",
        "      #so outputs list shape = [sequence_length, batch_size, decoder_units] before we transpose them in the next step\n",
        "\n",
        "    outputs=tf.convert_to_tensor(outputs)\n",
        "    outputs=tf.transpose(outputs, perm=[1,0,2])\n",
        "\n",
        "    outputs=self.dense(outputs)\n",
        "    return outputs, attention_weights"
      ],
      "metadata": {
        "id": "OCUtDa_hOG63"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocabulary_size, embedding_dimension, hidden_units, french_sequence_length)\n",
        "outputs, attention_weights = decoder(encoder_output,tf.zeros([128, hidden_units]), tf.zeros([128,64]))\n",
        "print(outputs.shape)\n",
        "print(attention_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJmZfB0-Di5Y",
        "outputId": "130f7db9-ba78-4c26-80be-34786e16857f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 64, 20000)\n",
            "(128, 64, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ENCODER\n",
        "input = Input(shape = (english_sequence_length), dtype = \"int64\", name = \"input_1\")\n",
        "encoder = Encoder(vocabulary_size, embedding_dimension, hidden_units)\n",
        "encoder_output = encoder(input)\n",
        "\n",
        "### DECODER\n",
        "shifted_target = Input(shape=(french_sequence_length), dtype = \"int64\", name = \"input_2\")\n",
        "decoder = Decoder(vocabulary_size, embedding_dimension, hidden_units, french_sequence_length)\n",
        "decoder_output, attention_weightss = decoder(encoder_output, tf.zeros([1, hidden_units]), shifted_target) #tf.zeros([1, hidden_units]) is initialized hidden state\n",
        "\n",
        "### OUTPUT\n",
        "bahdanau = Model([input, shifted_target], decoder_output)\n",
        "\n",
        "bahdanau.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-MO2tcDD31o",
        "outputId": "d41d6b4b-d6fc-48ad-e5b0-928f645b7951"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " encoder_5 (Encoder)         (None, 64, 256)              5645312   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " decoder_6 (Decoder)         ((None, 64, 20000),          1078659   ['encoder_5[0][0]',           \n",
            "                              (None, 64, 1))              3          'input_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16431905 (62.68 MB)\n",
            "Trainable params: 16431905 (62.68 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BLEU(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='bleu_score'):\n",
        "        super(BLEU, self).__init__(name=name)\n",
        "        self.total_matches = self.add_weight(name='total_matches', initializer='zeros')\n",
        "        self.total_words = self.add_weight(name='total_words', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "        mask = tf.cast(y_pred != 0, tf.float32)\n",
        "\n",
        "        # Compute total matches\n",
        "        matches = tf.reduce_sum(tf.cast(tf.equal(y_pred, y_true), tf.float32) * mask)\n",
        "        self.total_matches.assign_add(matches)\n",
        "\n",
        "        # Compute total words\n",
        "        words = tf.reduce_sum(mask)\n",
        "        self.total_words.assign_add(words)\n",
        "\n",
        "    def result(self):\n",
        "        return self.total_matches / self.total_words\n"
      ],
      "metadata": {
        "id": "REJaXsnuvooH"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bahdanau.compile(\n",
        "    loss = SparseCategoricalCrossentropy(),\n",
        "    optimizer = Adam(5e-4),\n",
        "    metrics = [BLEU()],\n",
        "    #run_eagerly=True\n",
        "    )"
      ],
      "metadata": {
        "id": "CZ4b51TVxpol"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/bahdanau_attention.h5'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath = checkpoint_filepath,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    save_best_only = True)"
      ],
      "metadata": {
        "id": "rKx4gEdjsIRE"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = bahdanau.fit(\n",
        "    train_dataset,\n",
        "    validation_data = val_dataset,\n",
        "    epochs = 2,\n",
        "    callbacks=[model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz5PuN-rsST2",
        "outputId": "62e5d3ae-dea7-4100-912b-90d4a98fd52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "    376/Unknown - 3293s 8s/step - loss: 1.1946 - bleu_score: 0.0000e+00"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6vlH5Zrdscz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model_accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JjW6-__wsjkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bahdanau.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "E6ESZZo3sm3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING**"
      ],
      "metadata": {
        "id": "Ic8wY4KQsvzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {x:y for x, y in zip(range(len(french_vectorization_layer.get_vocabulary())), french_vectorization_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "s-trDP6usyov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {y:x for x, y in zip(range(len(french_vectorization_layer.get_vocabulary())), french_vectorization_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "czQ4Ac-htUUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translator(english_sentence):\n",
        "  tokenized_english_sentence = english_vectorization_layer([english_sentence])\n",
        "  shifted_target = 'starttoken'\n",
        "\n",
        "  for i in range(french_sequence_length):\n",
        "    tokenized_shifted_target = french_vectorization_layer([shifted_target])\n",
        "    output = bahdanau.predict([tokenized_english_sentence,tokenized_shifted_target])\n",
        "    french_word_index = tf.argmax(output,axis=-1)[0][i].numpy()\n",
        "    current_word = index_to_word[french_word_index]\n",
        "    if current_word == 'endtoken':\n",
        "      break\n",
        "    shifted_target += ' ' + current_word\n",
        "  return shifted_target[11:]"
      ],
      "metadata": {
        "id": "aJc6CzXDszkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator('Everyone should water his or her tomato plants')"
      ],
      "metadata": {
        "id": "3OgRIXS2tpsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIoAtcT0vlDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}