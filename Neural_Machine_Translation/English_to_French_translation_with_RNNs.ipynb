{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6l6OK9pUQs+RvXZIAnqxm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UKJaagadhep/Data-science-and-machine-learning/blob/main/Neural_Machine_Translation/English_to_French_translation_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Input, GRU, Embedding, Bidirectional, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wi5P2r4GFP0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Downloading**"
      ],
      "metadata": {
        "id": "ZBTWzPuJDpOF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYP1lTxFDZEQ",
        "outputId": "f4ba21c4-83e4-4e9a-d248-0ae62234ca27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-09 14:27:05--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  19.9MB/s    in 0.4s    \n",
            "\n",
            "2024-05-09 14:27:06 (19.9 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "lnElkFWyFAXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = tf.data.TextLineDataset('/content/dataset/fra.txt')"
      ],
      "metadata": {
        "id": "-RLBZnyEDoWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in text_dataset.take(5):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7AhYNZ_FVUr",
        "outputId": "8f68bcbf-7352-46ec-f15b-87596be85335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tBouge !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_count = 0\n",
        "for i in text_dataset.take(1000):\n",
        "  len_count += len(tf.strings.split(i, \"\"))\n",
        "print(len_count/100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z_pXhEUGr2d",
        "outputId": "bead8269-2366-44b1-bb3a-172201edc894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def selector(input_text):\n",
        "  split_text = tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1' : split_text[0:1], 'input_2' : 'starttoken ' + split_text[1:2]}, split_text[1:2] + ' endtoken'\n",
        "  #We specify [0:1] instead of just [0] to get output in the form of a vector (enclosed by []) and not a scaler\n",
        "  #Dictionary contains inputs and  split_text[1:2] + ' [end]' is output\n",
        "  '''So for each sentence in french sequence, we will have [start] token and the sentence representing inputs to the\n",
        "  French output RNN from itself within the dictionary and we will also have the sentence and [end] token representing\n",
        "  the outputs from the French output RNN'''\n",
        "print(selector('Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vSvUVPHLGeP",
        "outputId": "dd4a7c4a-d6dc-4c60-87b2-9e7f058eb122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = text_dataset.map(selector) #We use split dataset into inputs and outputs (only input_1 is given by user)"
      ],
      "metadata": {
        "id": "p6b4Gh1wNDDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separator(input_text):\n",
        "  split_text = tf.strings.split(input_text, '\\t')\n",
        "  return split_text[0:1], 'starttoken ' + split_text[1:2] + ' endtoken'"
      ],
      "metadata": {
        "id": "Zkb9x6cOOC-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset = text_dataset.map(separator)\n",
        "#We split dataset into english and french (alongwith starttoken and endtoken for french) to get the vocabulary for the 2 languages"
      ],
      "metadata": {
        "id": "39fFSLxZO-BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(separator('Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmvPNlNMQEHY",
        "outputId": "072d3a0b-b4ff-4f25-8314-667ce1430c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va ! endtoken'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 20000\n",
        "english_sequence_length = 64\n",
        "french_sequence_length = 64\n",
        "embedding_dimension = 300\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "7O41279uF8hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorization_layer = TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation',\n",
        "    max_tokens = vocabulary_size,\n",
        "    output_sequence_length = english_sequence_length,\n",
        "    output_mode = 'int'\n",
        ")"
      ],
      "metadata": {
        "id": "yTn4QOQxIKQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorization_layer = TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation',\n",
        "    max_tokens = vocabulary_size,\n",
        "    output_sequence_length = french_sequence_length,\n",
        "    output_mode = 'int'\n",
        ")"
      ],
      "metadata": {
        "id": "N9vpzZnsJYVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data = init_dataset.map(lambda x, y : x)\n",
        "english_vectorization_layer.adapt(english_training_data)"
      ],
      "metadata": {
        "id": "sNaFaIIhJc78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_training_data = init_dataset.map(lambda x, y : y)\n",
        "french_vectorization_layer.adapt(french_training_data)"
      ],
      "metadata": {
        "id": "cxKxVrFwNWlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(english_vectorization_layer.get_vocabulary()))\n",
        "print(len(french_vectorization_layer.get_vocabulary()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DnJm9C2NrKa",
        "outputId": "2b62185d-232f-4df7-b3c0-7f336eb1e5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16952\n",
            "20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(inputs, output):\n",
        "  return {'input_1' : english_vectorization_layer(inputs['input_1']),\n",
        "          'input_2' : french_vectorization_layer(inputs['input_2'])}, french_vectorization_layer(output)"
      ],
      "metadata": {
        "id": "ol2ENhRQN-G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = split_dataset.map(vectorizer)"
      ],
      "metadata": {
        "id": "UvLSbFXAOdYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset.take(4):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MazB1f2XOnjz",
        "outputId": "4732eb49-1db3-4072-ee38-054be0fdd972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[  2, 104,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[104,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>)\n",
            "({'input_1': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[  2, 811,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[811,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>)\n",
            "({'input_1': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[  2,  23, 620,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[ 23, 620,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>)\n",
            "({'input_1': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[   2, 2789,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0]])>}, <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[2789,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0]])>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking indeices for starttoken and endtoken\n",
        "print(french_vectorization_layer.get_vocabulary()[2])\n",
        "print(french_vectorization_layer.get_vocabulary()[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFma56P7PDhh",
        "outputId": "ee448068-eb1d-4af4-bdda-a5200cb45e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starttoken\n",
            "endtoken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(2048).unbatch().batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "YBo7i09XSxtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = int(200000/batch_size)\n",
        "print(num_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir2zMRwaS3U3",
        "outputId": "3d774348-93a9-49c0-f4c5-fabca29b6205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.take(int(0.88 * num_batches))\n",
        "temp_dataset = dataset.skip(int(0.88 * num_batches))\n",
        "val_dataset = temp_dataset.take(int(0.67 * num_batches))\n",
        "test_dataset = temp_dataset.skip(int(0.67 * num_batches))"
      ],
      "metadata": {
        "id": "m3jY8EsTTCho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgYVCo1MU8E8",
        "outputId": "d2f82078-c2fa-4766-c5ae-9db9d3701fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-pJK_NdU9KI",
        "outputId": "a91ba815-a8de-4b6a-837b-5ff28295c3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqwmV-TSU-71",
        "outputId": "47d2976b-1471-46b1-fcc6-ed5f83ad6f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgNjV-U0VAm5",
        "outputId": "1dd499a0-26fe-430b-acdd-548e250e8046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_SkipDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelling**"
      ],
      "metadata": {
        "id": "SlRlt1bpWzeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_units = 256"
      ],
      "metadata": {
        "id": "xkZPhwcNWtr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ENCODER\n",
        "english_input = Input(shape = (english_sequence_length), dtype = 'int64', name = 'input_1')\n",
        "english_embedding = Embedding(vocabulary_size, embedding_dimension, name = \"english_embedding\")(english_input)\n",
        "encoded_input = Bidirectional(GRU(num_units), name = \"encoded_input\")(english_embedding)\n",
        "\n",
        "#DECODER\n",
        "right_shifted_target_french_input = Input(shape = (french_sequence_length), dtype = 'int64', name = \"input_2\")\n",
        "french_input_embedding = Embedding(vocabulary_size, embedding_dimension, name = \"french_input_embedding\")(right_shifted_target_french_input)\n",
        "encoded_french_input = GRU(num_units * 2, return_sequences = True, name = \"encoded_french_input\")(french_input_embedding, initial_state = encoded_input)\n",
        "#We have num_units * 2 units here because encoded_input is bidirectional with each direction GRU having num_units units\n",
        "\n",
        "#OUTPUT\n",
        "dropout = Dropout(0.5)(encoded_french_input)\n",
        "outputs = Dense(vocabulary_size, activation = \"softmax\")(dropout)\n",
        "#At each block in the sequence of french_sequence_length blocks, we want to choose one word out of vocabulary_size words\n",
        "\n",
        "seq2seq_gru_model = Model([english_input, right_shifted_target_french_input], outputs)\n",
        "\n",
        "seq2seq_gru_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgxNHngnvhEl",
        "outputId": "82b163ff-51b3-4c83-cea4-570e04e08372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " english_embedding (Embeddi  (None, 64, 300)              6000000   ['input_1[0][0]']             \n",
            " ng)                                                                                              \n",
            "                                                                                                  \n",
            " french_input_embedding (Em  (None, 64, 300)              6000000   ['input_2[0][0]']             \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " encoded_input (Bidirection  (None, 512)                  857088    ['english_embedding[0][0]']   \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " encoded_french_input (GRU)  (None, 64, 512)              1250304   ['french_input_embedding[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'encoded_input[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 64, 512)              0         ['encoded_french_input[0][0]']\n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 64, 20000)            1026000   ['dropout_5[0][0]']           \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24367392 (92.95 MB)\n",
            "Trainable params: 24367392 (92.95 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BLEU(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name = 'bleu_score'):\n",
        "    super(BLEU, self).__init__()\n",
        "    self.bleu_score = 0\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight = None):\n",
        "    y_pred = tf.argmax(y_pred, -1)\n",
        "\n",
        "    self.bleu_score = 0\n",
        "\n",
        "    for i,j in zip(y_pred, y_true):\n",
        "      tf.autograph.experimental.set_loop_options() #For looping through zip\n",
        "      total_words = tf.math.count_nonzero(i)\n",
        "      total_matches = 0\n",
        "\n",
        "      for word in i:\n",
        "        if word == 0: #Predicted sentence is over\n",
        "          break\n",
        "        for q in range(len(j)):\n",
        "          if j[q] == 0: #Actual sentence is over\n",
        "            break\n",
        "          if word == j[q]:\n",
        "            total_matches += 1\n",
        "            j = tf.boolean_mask(j,[False if y==q else True for y in range(len(j))]) #To strike present words so they don't get considered again\n",
        "            #The word indices that get boolean_mask as False get removed\n",
        "            break\n",
        "\n",
        "      self.bleu_score += total_matches / total_words\n",
        "\n",
        "  def result(self):\n",
        "    return self.bleu_score / batch_size"
      ],
      "metadata": {
        "id": "aphwW0s29_fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_gru_model.compile(metrics = [BLEU()], loss = SparseCategoricalCrossentropy(), optimizer = Adam(1e-4), #run_eagerly = True\n",
        "\n",
        "                          )"
      ],
      "metadata": {
        "id": "63l5oz0t2jFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/seq2seq_gru_model.h5'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath = checkpoint_filepath,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    save_best_only = True)"
      ],
      "metadata": {
        "id": "Xw8I11NNBtd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histroy = seq2seq_gru_model.fit(train_dataset, epochs = 10, validation_data = val_dataset, callbacks = [model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVS3F9LuBaH-",
        "outputId": "d07ffd9c-e9b6-47d5-b715-e5013914a9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "    441/Unknown - 3527s 8s/step - loss: 1.4588 - accuracy: 0.9231"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "avFNN80qCyZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model_accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wUjhp7IOC94c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_gru_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "bDKSUsqgDPLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "Un1TChG9B2i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {x:y for x, y in zip(range(len(french_vectorization_layer.get_vocabulary())), french_vectorization_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "2vJPoepDR_ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {y:x for x, y in zip(range(len(french_vectorization_layer.get_vocabulary())), french_vectorization_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "FbUh2he7TUKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translator(english_sentence):\n",
        "  tokenized_english_sentence = english_vectorization_layer([english_sentence])\n",
        "\n",
        "  shifted_target = 'starttoken'\n",
        "\n",
        "  for i in range(french_sequence_length):\n",
        "    tokenized_shifted_target = french_vectorization_layer([shifted_target])\n",
        "\n",
        "    output = seq2seq_gru_model([tokenized_english_sentence, tokenized_shifted_target])\n",
        "\n",
        "    french_sentence = tf.argmax(output, axis = -1)\n",
        "    french_word_index = french_sentence[0][i].numpy()\n",
        "    current_word = index_to_word[french_word_index]\n",
        "\n",
        "    if current_word == 'endtoken':\n",
        "      break\n",
        "    shifted_target += ' ' + current_word\n",
        "  return shifted_target[11:] # We specify [11:] to avoid having starttoken in answer\n",
        "\n"
      ],
      "metadata": {
        "id": "mbPzW0MEDZeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator('How is the weather today?')"
      ],
      "metadata": {
        "id": "gDeZf8sImD-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}