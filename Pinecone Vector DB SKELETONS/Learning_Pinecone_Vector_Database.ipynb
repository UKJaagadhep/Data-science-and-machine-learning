{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzzPHukIb3uR",
        "outputId": "d36a69ba-883b-49b9-f89e-a365b2c14c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Requirement already satisfied: pinecone-client[grpc] in /usr/local/lib/python3.10/dist-packages (3.2.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client[grpc]) (2024.6.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client[grpc]) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client[grpc]) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client[grpc]) (2.0.7)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client[grpc]) (1.63.1)\n",
            "Collecting grpc-gateway-protoc-gen-openapiv2==0.1.0 (from pinecone-client[grpc])\n",
            "  Downloading grpc_gateway_protoc_gen_openapiv2-0.1.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: grpcio>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client[grpc]) (1.64.1)\n",
            "Collecting lz4>=3.1.3 (from pinecone-client[grpc])\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<3.21.0,>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client[grpc]) (3.20.3)\n",
            "Installing collected packages: lz4, grpc-gateway-protoc-gen-openapiv2\n",
            "Successfully installed grpc-gateway-protoc-gen-openapiv2-0.1.0 lz4-4.3.3\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install openai\n",
        "!pip install pinecone-client[grpc]\n",
        "!pip install tiktoken #utility for embeddings class in OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community langchain_pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GVPM_Z9hPQG",
        "outputId": "4718d16f-f6e4-470b-fe9b-18be91906b98"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.6)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.10)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.4.1)\n",
            "Collecting pinecone-client<4.0.0,>=3.2.2 (from langchain_pinecone)\n",
            "  Downloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (2.7.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.5)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2024.6.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (2.18.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: pinecone-client, langchain_pinecone\n",
            "  Attempting uninstall: pinecone-client\n",
            "    Found existing installation: pinecone-client 4.1.1\n",
            "    Uninstalling pinecone-client-4.1.1:\n",
            "      Successfully uninstalled pinecone-client-4.1.1\n",
            "Successfully installed langchain_pinecone-0.1.1 pinecone-client-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import Pinecone #store and query embeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone.grpc import PineconeGRPC\n",
        "from langchain.chains import RetrievalQA\n",
        "'''RetrievalQA is a chain that performs retrieval-based question answering. It combines retrieval (finding relevant documents) with\n",
        "a QA model to answer questions based on those documents.'''\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "#OpenAIEmbeddings generates embeddings for text using OpenAI's models. These embeddings can then be stored in a vector database like Pinecone for later retrieval.\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader #loads PDFs from a directory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "'''RecursiveCharacterTextSplitter splits text into smaller chunks based on character count, which is useful for processing and\n",
        "embedding long documents that exceed the model's input size limitations.'''\n",
        "import os\n",
        "import pinecone"
      ],
      "metadata": {
        "id": "sG7a4qv4cLqd"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "id": "17gmLh5DhNpP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader('pdfs')"
      ],
      "metadata": {
        "id": "FcwHY4zgvYhU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "Rduv8TwNv7nO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJD4R-YkxK8e",
        "outputId": "439db4ad-5070-45a6-b188-d8c0b09a785b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Citation: Tao, L.; Xie, Z.; Xu, D.; Ma,\\nK.; Qiu, Q.; Pan, S.; Huang, B.\\nGeographic Named Entity\\nRecognition by Employing Natural\\nLanguage Processing and an\\nImproved BERT Model. ISPRS Int. J.\\nGeo‑Inf. 2022 ,11, 598. https://doi.org/\\n10.3390/ijgi11120598\\nAcademic Editors: Maria Antonia\\nBrovelli and Wolfgang Kainz\\nReceived: 15 September 2022\\nAccepted: 24 November 2022\\nPublished: 28 November 2022\\nPublisher’s Note: MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional affil‑\\niations.\\nCopyright: © 2022 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license ( https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\n International Journal of\\nGeo-Information\\nArticle\\nGeographic Named Entity Recognition by Employing Natural\\nLanguage Processing and an Improved BERT Model\\nLiufeng Tao1,2, Zhong Xie1,2, Dexin Xu3, Kai Ma4,5, Qinjun Qiu1,2,6,*, Shengyong Pan7and Bo Huang7\\n1School of Computer Science, China University of Geosciences, Wuhan 430074, China\\n2Beijing Key Laboratory of Urban Spatial Information Engineering, Beijing 100038, China\\n3Wuhan Geomatics Institute, Wuhan 430074, China\\n4Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering,\\nChina Three Gorges University, Yichang 443002, China\\n5College of Computer and Information Technology, China Three Gorges University, Yichang 443002, China\\n6Hubei Key Laboratory of Intelligent Geo‑Information Processing, China University of Geosciences,\\nWuhan 430074, China\\n7Wuhan Zondy Cyber Science & Technology Co., Ltd., Wuhan 430074, China\\n*Correspondence: qiuqinjun@cug.edu.cn\\nAbstract: Toponym recognition, or the challenge of detecting place names that have a similar refer‑\\nent, is involved in a number of activities connected to geographical information retrieval and geo‑\\ngraphical information sciences. This research focuses on recognizing Chinese toponyms from social\\nmedia communications. While broad named entity recognition methods are frequently used to lo‑\\ncate places, their accuracy is hampered by the many linguistic abnormalities seen in social media\\nposts, such as informal sentence constructions, name abbreviations, and misspellings. In this study,\\nwe describe a Chinese toponym identification model based on a hybrid neural network that was\\ncreated with these linguistic inconsistencies in mind. Our method adds a number of improvements\\nto a standard bidirectional recurrent neural network model to help with location detection in social\\nmedia messages. We demonstrate the results of a wide‑ranging evaluation of the performance of dif‑\\nferent supervised machine learning methods, which have the natural advantage of avoiding human\\ndesign features. A set of controlled experiments with four test datasets (one constructed and three\\npublic datasets) demonstrates the performance of supervised machine learning that can achieve good\\nresults on the task, significantly outperforming seven baseline models.\\nKeywords: geographic named entity recognition; social media message; natural language processing;\\nBERT; toponyms recognition\\n1. Introduction\\nOnline social media platforms, especially microblog platforms such as Wechat and\\nWeibo, are responsive to real‑world events and are useful for gathering situational infor‑\\nmation in real time [ 1–4]. Geographic locations are often described in these messages. For\\nexample, Weibo is widely used in disaster response and rescue, such as earthquakes, floods,\\nfire, and terrorist attacks. The cornerstone of the aforementioned application is called geop‑\\narsing [ 5–7]. Geoparsing is a difficult natural language processing (NLP) task that aligns\\nnaturally stated things in free text spatially (written or obtained through automatic tran‑\\nscription) [ 7–9]. It is important to understand the distinction between geographic parsing\\nand geocoding. The input to geographic parsing does not include any information about\\nthe places indicated in the input. In geocoding, a valid textual representation of the loca‑\\ntion (address) is the input. As a result, the geocoder needs to merely look up the supplied\\naddress’s coordinates in a gazetteer. Geocoding is difficult due to the fact that it deals\\nwith raw natural language data. In this paper, we show initial progress in creating the\\nfirst geographic name parsing system for a Chinese language. To achieve this goal, the\\nISPRS Int. J. Geo‑Inf. 2022 ,11, 598. https://doi.org/10.3390/ijgi11120598 https://www.mdpi.com/journal/ijgi', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 2 of 22\\nfirst subprocess of our approach is to identify the location of the mentioned contents; this\\nsubprocess is called entity recognition (NER) in NLP [ 10–13].\\nThere are single‑word place names, such as Beijing, Shanghai, Zhejiang, etc. There are\\nalso long place names composed of multiple words, such as Ejin Jinqi Saihantaolai Sumu\\nTownship (Inner Mongolia Autonomous Region); however, most of the place names are\\n1~5 words in length. The distribution of characters used in Chinese place names is also\\nrelatively scattered, but there are relatively concentrated features of place names within a\\ncertain range. For example, there are 3685 characters used in the gazetteer of China, and\\nthe specific frequency of use is relatively scattered. However, some of these words and\\ntheir word combinations only appear in toponyms, such as “Gacha” in Inner Mongolia,\\nwhile most of them are common words with strong word formation ability and often ap‑\\npear in nontoponymic words, such as “Suqian” in “Su” and “Qian”. Based on the above\\npoints, Chinese place names can be roughly divided into simple and complex names. Sim‑\\nple names refer to those with short lengths (1~5 words) and common characters, such as\\nBeijing, Beijing Municipality, Hetao Plain, Hongyashan Reservoir, etc. Complex names re‑\\nfer to those with long lengths (more than five words) or with characters and words that are\\nmore remote. Therefore, it is technically difficult to identify Chinese place names accurately,\\nand it has become an important research direction in the field of geographic information.\\nThe existing methods for recognizing geographical names can be divided into three\\ntypes of methods: rule‑based methods, statistical methods, and machine learning meth‑\\nods [ 14,15]. Rule‑based methods refer to the manual summarization of various word for‑\\nmations and syntactic rules and recognition by rule matching methods. This method is\\nmore intuitive, easy to understand and extend, and works better and faster for small‑scale\\ncorpus testing. However, the design of rules relies on professional language knowledge\\nand domain knowledge, is time‑consuming to compile, is difficult to cover comprehen‑\\nsively, and has poor portability and robustness. The statistical‑based approach does not\\nrequire specialized language knowledge, is more robust and flexible than the rule‑based\\napproach, and is highly portable, but the system does not express language determinism\\nwell and requires a large‑scale, more comprehensive manually annotated training corpus.\\nWith the accumulation of big data and the continuous enhancement of computer perfor‑\\nmance, deep‑learning‑based place‑name‑extraction methods have been developed rapidly.\\nDeep learning models are application‑friendly and robust and can automatically learn and\\nextract key features from text, achieving remarkable results in Chinese place‑name recog‑\\nnition. The most commonly used model is bidirectional long short‑term memory (LSTM),\\nwhich is based on the evolution of recurrent neural networks (RNNs) and overcomes the\\nshortcomings of RNNs in long‑dependent sentences. The two‑way LSTM uses two LSTM\\nhidden layers with opposite directions to further solve the problem that sequence tagging\\ncan only use information from above and not below. In the current research on Chinese\\nplace‑name recognition, the main problem focuses on the recognition of complex Chinese\\nplace names. Since the length of complex place names is usually long and the use of words\\nand word collocation is relatively small, the above models often have difficulty determin‑\\ning the boundaries of place names.\\nIn this paper, we propose a hybrid neural network model for Chinese place‑name\\nrecognition based on a bidirectional encoder of the lite bidirectional encoder representa‑\\ntions from transformers (ALBERTs) model for word vector extraction to improve the text\\nvector representation ability and effectively identify irregular place names and place‑name\\nabbreviations. The bidirectional long‑ and short‑term memory (BiLSTM) neural network\\nlayer captures the semantic information in both directions in the sentence to better deter‑\\nmine the entity boundaries. The global optimal token sequence is obtained by the condi‑\\ntional random field (CRF) layer.\\nThe main contributions of this research are listed as follows:\\n(1) TPCNER, a large self‑annotated corpus of geographic domains with seven cate‑\\ngories and 64,063 labeled samples, was gathered and built. This corpus has more entity\\ncategories and larger sample sizes than the preceding corpora. The efficiency of the TPC‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 3 of 22\\nNER highlighted in this study was further demonstrated by the assessment experimental\\nfindings in Section 5.4.\\n(2) A novel Chinese NER (CNER) model for the geographic domain via the improved\\nALBERT pretraining model and BiLSTM–CRF was proposed. By learning word‑level fea‑\\nture representation through the ALBERT layer and extracting text contextual semantic fea‑\\ntures through the BiLSTM layer, the CRF layer obtains the global optimal token sequence\\nand finally improves the overall performance of the proposed model.\\n(3) The performance of ALBERT–BiLSTM–CRF was evaluated by using a range of\\nstandard models on TPCNER, MSRA, RenMinRiBao, and Boson. Furthermore, several\\nspecific details were studied and debated, such as the efficacy of BiLSTM and the use of\\nthe CRF mechanism. Through thorough comparisons with other advanced models, com‑\\nprehensive experimental findings on domain‑specific and generic datasets confirmed the\\nproposed model’s effective performance.\\nThe rest of the paper is organized as follows: The current work on named‑entity\\nrecognition is described in Section 2. The processing of corpus creation is described in\\nSection 3. The recommended procedure for identifying geographic entities is presented\\nand described in Section 4. Section 5contains the experimental data and results. The con‑\\nclusion and recommendations for further study are presented in Section 6.\\n2. Related Work\\nToponym information contains spatial location information, so toponym recognition\\ncan be applied to emergency‑disaster reduction, public‑opinion monitoring, urban plan‑\\nning, and other fields [ 16–18]. For example, information such as place names and nat‑\\nural disasters occurring there can be extracted from social media messages released by\\nthe public, such as Twitter. Social media messages can also assist in the monitoring and\\nmanagement of public opinion. Managers can identify social public‑security events from\\nwebpages and social media texts and extract key spatial location information, realize the\\nmonitoring and early warning of social public security incidents, and improve the effi‑\\nciency of social and public security time disposal. To protect public privacy, social media\\nwill selectively hide the specific user location obtained in real time, which makes the extrac‑\\ntion of geographic location information more complicated. In June 2019, Twitter officially\\nremoved the precise geotagging feature. This change may reduce the geographic informa‑\\ntion contained in tweets, complicate location judgment, and make the task of recognizing\\nand geolocating locations from tweet content more urgent when dealing with emergen‑\\ncies [ 15]. We mainly identify the long text toponymic information in more detail to make\\nits characteristics more obvious and facilitate the development of subsequent tasks.\\nThe recognition methods of Chinese toponyms are mainly divided into three types,\\nnamely dictionary and rule‑based methods, statistical‑based machine learning methods,\\nand deep learning methods [ 19–21]. The rule‑based method mainly carries out place‑name\\nmatching and recognition by manually summarizing various word‑formation rules (the\\ndefects of this method have been summarized in Section 1), which can be combined with\\nthe current popular deep learning methods to supplement professional vocabulary and\\nimprove the robustness of the training model. The number of toponyms will increase at\\nan extremely fast rate with the development of the region, and frequently the same location\\nis represented by multiple toponyms. Therefore, the construction of a definitive gazetteer\\ncannot be achieved, and automatic place‑name recognition is still worth studying.\\nThe method based on statistics is more flexible than the rule method, which trans‑\\nforms place‑name recognition into a serialization annotation problem, but this method\\ndepends on the selection of feature templates and has poor generalization ability. Com‑\\nmon machine learning algorithms include the hidden Markov model (HMM), maximum\\nentropy Markov model (MEMM), and CRF model. Among them, the CRF model can im‑\\nplement effective feature‑selection and feature‑induction algorithms for sequence‑labeling\\ntasks. That is, users can evaluate the effect of automatically generated features on data\\nabstraction. Therefore, combining a CRF model with subsequent large‑scale pretraining', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 4 of 22\\nmodels, such as the BERT–CRF model, can achieve excellent sequence labeling results, thus\\nproviding ideas for more accurate place‑name recognition.\\nThe named‑entity identification approach based on neural networks is extensively uti‑\\nlized in text information extraction in numerous sectors, thanks to the rapid growth of deep\\nlearning. Different from traditional machine learning algorithms, the model trained by a\\ndeep neural network has the characteristics of end‑to‑end data input and output. It makes\\nthe model training process more capable of reducing artificial interference to directly com‑\\nplete specific tasks according to the original data input, and there is no need to manually\\nset data characteristics [ 22]. The RNN is especially good at processing sequence data. The\\nevolved LSTM, BiLSTM, and bidirectional gated recurrent unit (BiGRU) networks often\\ncombine the CRF layer to realize the task of named entity recognition. The more common\\nBiLSTM–CRF model, which combines the advantages of BiLSTM and CRF, not only can\\nretain the context information to process long text but also fully use sentence‑level tag in‑\\nformation thanks to a CRF layer. Similarly, the BiGRU–CRF model is widely used.\\nIn recent years, large‑scale pretraining models have rapidly become the preferred\\nmethod of natural language processing due to their outstanding performance. On this ba‑\\nsis, downstream task processing can make the recognition results of the hybrid model more\\naccurate [ 23,24]. Common pretraining models include the generative pretrained trans‑\\nformer (GPT), BERT, enhanced representation through knowledge integration (ERNIE),\\netc. The current popular BERT model works from the encoder of the bidirectional trans‑\\nformer model [ 25,26]. We choose the BERT‑wwm model as the pretraining model, which is\\ntrained by the Research Center for Social Computing and Information Retrieval and iFLY‑\\nTEK AI Research in China. It is an open‑source Chinese pretraining language model, using\\nwhole‑word masking technology, which can better realize the task of Chinese place‑name\\nrecognition. On the basis of BERT‑wwm, BERT‑wwm‑ext expands the pretraining dataset\\nand increases the number of training iterations during model training.\\nToponym recognition is a subtask of named entity recognition, which belongs to the\\ninformation extraction task. We want to extract place‑name information from long text\\ndata, but the model trained by the general corpus is still lacking in the accuracy and gran‑\\nularity of toponym recognition. In response to the above problems, we used a Chinese cor‑\\npus containing only toponym annotations and designed a hybrid neural network to train\\nthe model to obtain a model that performs better in the task of Chinese toponym recogni‑\\ntion. This model improves upon the general effect and low granularity of the traditional\\nnamed‑entity recognition model in toponym recognition.\\n3. Corpus Preparation and Annotation\\nTo address the limited Chinese NER corpus, a new corpus, TPCNER, was collected\\nand constructed. The dataset was further extended with entity categories based on earlier\\nstudies and eventually contained 7 entity categories and 64,063 annotated samples.\\n3.1. NER Tag Sets\\nNamed entities in the geographic domain, such as organizations, water systems, and\\nlandforms, are very different from those in the general domain. Geographic entities require\\na large amount of domain‑specific knowledge, thus making annotation difficult to a certain\\nextent. In this paper, based on the existing research [ 24], the entity categories are further\\ndivided into more granular entities, such as residential land and facilities, landforms, and\\nwater systems. Some categories were also considered, such as transportation, pipelines,\\nboundaries, and political areas with other regions. Finally, as indicated in Table 1, seven\\nfine‑grained groups emerge. To guarantee the integrity of the CNER categories, we prede‑\\nfine the category “others” in this work to characterize some conceptual and uncertain en‑\\ntities for later growth. Furthermore, this article exclusively considers entity types that are\\nrelevant to the geographic domain (e.g., toponym and organization), rather than generic\\nentities such as individuals (e.g., personal name). In the future, the corpus will be released\\nas well. Tables 2–4show the details of Boson, MSRA, and RenMinRiBao.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 5 of 22\\nTable 1. Details of entity categories in TPCNER.\\nID Entity Tags Abbreviation Description Example\\n1 Water System WATA manmade building or natural structure\\nassociated with water in nature.Tongji Canal, Huaihe\\nRiver Basin\\n2Residential land\\nand facilitiesRLFA place where human beings live or engage in\\nproductive life.Shaanxi Kiln\\n3 Transportation TRA Human‑built buildings related to transportation. Longxia Railway\\n4 Pipelines PIP Pipelines laid by humans. Natural gas pipeline\\n5Boundaries,\\nRegions, and\\nOther AreasBROThe corresponding boundaries that humans have\\ndrawn on the land to facilitate management.Hubei Province\\n6 Landforms LAN Includes natural and artificial landforms. Himalayas\\n7 Organization ORG Includes the names of relevant organizations.Wuhan Zhongdi\\nDigital Technology Co.\\nTable 2. Details of entity categories in Boson.\\nID Entity Tags Abbreviation Description Example\\n1 Location LOC A spatial distribution, location, or place occupied. China\\n2 Org_name ORG Includes the names of relevant organizations.Wuhan Zhongdi\\nDigital Technology Co.\\nTable 3. Details of entity categories in MSRA.\\nID Entity Tags Abbreviation Description Example\\n1 NS NS A spatial distribution, location, or place occupied. Yufeng Mountain\\n2 NT NT Includes the names of relevant organizations.China University of\\nGeosciences\\nTable 4. Details of entity categories in RenMinRiBao.\\nID Entity Tags Abbreviation Description Example\\n1 NS NS A spatial distribution, location, or place occupied. Hubei\\n2 NT NT Includes the names of relevant organizations.China University of\\nGeosciences\\n3.2. Corpus Collection and Annotation\\nIn this paper, a large‑scale annotated corpus, TPCNER, is established with the Baidu\\nEncyclopedia and the Chinese Encyclopedia of Chinese Geography as source data (approx‑\\nimately 2 million words) and with reference to the geographically named entity annotation\\nsystem designed in this paper.\\nTo ensure consistency and accuracy, the work in this paper is presented in two main ar‑\\neas. First, a new TPCNER annotation tool, ChineseNERAnno, was developed (see Figure 1).\\nThe complete process of this tool is depicted in Figure 1. The suggested technique employs\\na lexicon of terms linked to the geographic domain for automated annotation, which helps\\nto ensure entity consistency. Second, new entities can be dynamically extended into the\\ndictionary, thus reducing the manual annotation time and increasing the annotation speed.\\nA new TPCNER corpus was finally constructed, consisting of 7 categories, 650,725 entities,\\nand 64,063 samples preprocessed and annotated. This procedure took three months to com‑\\nplete under the supervision of domain experts. Table 5shows certain TPCNER examples\\nin further detail. Figure 2shows data on the statistical distribution of individual entities in\\nTPCNER, demonstrating that the training set’s distribution is similar to the validation set’s\\ndistribution. The logic and utility of the corpus designated in this work are also argued in\\nSection 5.4.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 4}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 6 of 22\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  6 of 22  \\n To ensure consisten cy and accuracy, the work in this paper is presented in two main \\nareas. First, a new TPCNER annotation tool, ChineseNERAnno, was developed  (see Fig-\\nure 1) . The complete process of this tool is depicted in Figure 1. The suggested technique \\nemploys a lexicon of terms linked to the geographic domain for automated annotation, \\nwhich helps to ensure entity consistency. Second, new entities can be dynamically ex-\\ntended into the dictionary, thus reducing the manual annotation time and increasing the \\nannotation speed.  A new TPCNER corpus was finally constructed, consisting of 7 catego-\\nries, 650,725 entities, and 64,063 samples preprocessed and annotated. This procedure \\ntook three months to complete under the supervision of domain experts. Table 5 shows \\ncertain TPCNER examples in further detail. Figure 2 shows data on the statistical distri-\\nbution of individual entities in TPCNER, demonstrating that the training set ’s distribution \\nis similar to the validation set ’s distribution. The logic and utility of the corpus designated \\nin this work are also argued in Section 5.4.  \\nRaw DataAuto \\nAnnotationManual \\nValidationExportLabeled\\nData\\nFormats LexiconBIO\\nBIOES\\nBMES \\n  \\n   \\n \\nUpdate\\n \\nFigure  1. Overall workflow of ChineseNERAnno.  \\nTable  5. Some samples of TPCNER.  \\nSentence(en):  Anji County is a county in Huzhou City, Zhejiang Province, a famous bamboo producing area \\nin China and one of the key forestry counties in Zhejiang Province.  \\nLabel(en):  Anji County; Zhejiang Province; Huzhou City; China; Zhejiang Province  \\nSentence(en):  The Bailong  River is a tributary of the upper reaches of the Jialing River in the Yangtze River \\nsystem, and is a geographically important dividing line in China, along with the Qinling and \\nHuai Rivers.  \\nLabel(en):  Bailong River; Yangtze River; Jialing River; Qinling  River; Huai River; China  \\n3.3. Analysis of Corpus Features  \\nGeographic entity names are the most important distinction between geographic en-\\ntities and in the field of geographic information  (see Table 6) . Entity names with location \\ninformation, mainly composed of basic geographic information elements, can be seen with \\nambiguity and diversity. In this paper, we analyze the descriptive features of geographic \\nentities in texts by studying relevant national stan dards and the literature. We then inte-\\ngrate geographically named entity categories and descriptive features by manual collation \\nand data fusion with national standards as the benchmark. The distribution for each cat-\\negory in TPCNER is shown in Figure 2.  \\nThe descriptions of geographic entity names in Chinese texts are characterized by \\nvagueness, uncertainty, and diversity. In this paper, we analyze the descriptive features \\nof geographic entity names in texts. The five main descriptive features are summarized as \\nfollows : \\n(1) The names of geographic entities are diverse, with free and scattered words or \\nphrases, but with relatively concentrated coverage, for example, the name of the commu-\\nnity “Daijiashanzhuang”, within which there are “Daijiashanzhuang Phase 1” and “Dai-\\njiashanzhuang Phase 2”.  \\nFigure 1. Overall workflow of ChineseNERAnno.\\nTable 5. Some samples of TPCNER.\\nSentence(en):Anji County is a county in Huzhou City, Zhejiang Province, a famous bamboo producing area in\\nChina and one of the key forestry counties in Zhejiang Province.\\nLabel(en): Anji County; Zhejiang Province; Huzhou City; China; Zhejiang Province\\nSentence(en):The Bailong River is a tributary of the upper reaches of the Jialing River in the Yangtze River system,\\nand is a geographically important dividing line in China, along with the Qinling and Huai Rivers.\\nLabel(en): Bailong River; Yangtze River; Jialing River; Qinling River; Huai River; China\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  7 of 22  \\n (2) The names of geographical entities have a certain pattern, often ending with char-\\nacteristic words , such as “province, road, mountain”, for example, “Hubei Province” and \\n“Luma Road, Hongshan District, Hubei Province”.  \\n(3) The names of geographical entities are often followed by location words; for ex-\\nample, “Huangshan” is a place name, and “Huangshan North” is a complete geographical \\nnaming entity.  \\n(4) Most of the names of geographical entities are in the form of nouns, but  sometimes \\nthey are used as modifiers to modify other entities, such as “Bagong Mountain Tofu”.  \\n(5) The names of geographical entities are named and unnamed ; that is,  some geo-\\ngraphical entities do not have specific names, and their spatial locations need t o be deter-\\nmined through the contextual relationship. For example, the “swimming pool” in the \\n“swimming pool in the west area of the university” is the name of a geographical entity, \\nbut its spatial location needs to be determined by the previous informatio n. \\n \\nFigure  2. Distribution for each category in TPCNER.  \\nTable 6. Comparative information between TPCNER and other datasets.  \\nDatasets  Examples  Classes  Size  Entity Size  Max Length  Min Length  Avg Length  \\nBoson  Obama [person_name]  also wel-\\ncomed Cameron [person_name]  us-\\ning a number of authentic \\nBritish [location]  vernaculars,  6 1.78 M  3417  36 1 18.5 \\nMSRA  the scope of the survey in-\\nvolved the Forbidden City [loca-\\ntion], the Museum of History [lo-\\ncation ], the Institute of Ancient \\nResearch [organization_name] , the Pe-\\nking University and Tsinghua \\nLibrary [location] , the Beitu [location ], \\nthe Japanese archives and \\nmore than twenty others.  3 10.4 M  80,884  40 1 20.5 \\nRenMinRiBao  New Year Concert in Bei-\\njing [location ] 3 10.1 M  12,718 35 1 18 \\nTPCNER  The sample examples are \\nlisted in Table 2.  7 7.32 M  64,063 18 2 10 \\n  \\nFigure 2. Distribution for each category in TPCNER.\\n3.3. Analysis of Corpus Features\\nGeographic entity names are the most important distinction between geographic en‑\\ntities and in the field of geographic information (see Table 6). Entity names with location\\ninformation, mainly composed of basic geographic information elements, can be seen with\\nambiguity and diversity. In this paper, we analyze the descriptive features of geographic\\nentities in texts by studying relevant national standards and the literature. We then inte‑\\ngrate geographically named entity categories and descriptive features by manual collation\\nand data fusion with national standards as the benchmark. The distribution for each cate‑\\ngory in TPCNER is shown in Figure 2.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 7 of 22\\nTable 6. Comparative information between TPCNER and other datasets.\\nDatasets Examples Classes Size Entity Size Max Length Min Length Avg Length\\nBosonObama [person_name] also welcomed\\nCameron [person_name] using a number of\\nauthentic British [location] vernaculars,6 1.78 M 3417 36 1 18.5\\nMSRAthe scope of the survey involved the\\nForbidden City [location] , the Museum of\\nHistory [location] , the Institute of Ancient\\nResearch [organization_name] , the Peking\\nUniversity and Tsinghua Library [location] ,\\nthe Beitu [location] , the Japanese archives and\\nmore than twenty others.3 10.4 M 80,884 40 1 20.5\\nRenMinRiBao New Year Concert in Beijing [location] 3 10.1 M 12,718 35 1 18\\nTPCNER The sample examples are listed in Table 2. 7 7.32 M 64,063 18 2 10\\nThe descriptions of geographic entity names in Chinese texts are characterized by\\nvagueness, uncertainty, and diversity. In this paper, we analyze the descriptive features\\nof geographic entity names in texts. The five main descriptive features are summarized as\\nfollows:\\n(1) The names of geographic entities are diverse, with free and scattered words or\\nphrases, but with relatively concentrated coverage, for example, the name of the commu‑\\nnity “Daijiashanzhuang”, within which there are “Daijiashanzhuang Phase 1” and “Daiji‑\\nashanzhuang Phase 2”.\\n(2) The names of geographical entities have a certain pattern, often ending with char‑\\nacteristic words, such as “province, road, mountain”, for example, “Hubei Province” and\\n“Luma Road, Hongshan District, Hubei Province”.\\n(3) The names of geographical entities are often followed by location words; for ex‑\\nample, “Huangshan” is a place name, and “Huangshan North” is a complete geographical\\nnaming entity.\\n(4) Most of the names of geographical entities are in the form of nouns, but sometimes\\nthey are used as modifiers to modify other entities, such as “Bagong Mountain Tofu”.\\n(5) The names of geographical entities are named and unnamed; that is, some geo‑\\ngraphical entities do not have specific names, and their spatial locations need to be de‑\\ntermined through the contextual relationship. For example, the “swimming pool” in the\\n“swimming pool in the west area of the university” is the name of a geographical entity,\\nbut its spatial location needs to be determined by the previous information.\\n4. The Hybrid Deep Learning Model\\n4.1. Overall Framework and Workflow of the Model\\nIn this paper, we propose a hybrid neural network model for Chinese place‑name\\nrecognition. The overall structure of the model is shown in Figure 3, and the whole model\\nis divided into five parts: the input layer, ALBERT layer, BiLSTM layer, CRF layer, and\\noutput layer.\\nWe present our model from bottom to top, characterizing the layers of the neural net‑\\nwork. The input layer contains the individual words of a message which are used as the\\ninput to the model.\\nThe next layer represents each word as vectors, using a pretraining approach. It uses\\npretrained word embeddings to represent the words in the input sequence. In particular,\\nwe use ALBERT, which captures the different semantics of a word under varied contexts.\\nNote that the pretrained word embeddings capture the semantics of words based on their\\ntypical usage contexts and therefore provide static representations of words; by contrast,\\nALBERT provides a dynamic representation for a word by modeling the particular sen‑\\ntence within which the word is used. This layer captures four different aspects of a word,\\nand their representation vectors are concatenated together into a large vector to represent\\neach input word. These vectors are then used as the input to next layer, which is a BiLSTM', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 8 of 22\\nlayer consisting of two layers of LSTM cells: one forward layer capturing information be‑\\nfore the target word and one backward layer capturing information after the target word.\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  8 of 22  \\n 4. The Hybrid Deep Learning Model  \\n4.1. Overall Framework and Workflow of the Model  \\nIn this paper, we propose a hybrid neural network model for Chinese place -name \\nrecognition. The overall structure of the model is shown in Figure 3, and the whole model \\nis divided into five parts: the input layer, ALBERT layer, BiLSTM layer, CRF layer, and \\noutput layer.  \\n \\nFigure  3. The overall architecture of the proposed model.  \\nWe present our model from bottom to top, characterizing the layers of the neural \\nnetwork. The input layer contains the individual words of a message which are used as \\nthe input to the model.  \\nThe next layer represents each word as vectors , using a pretraining approach. It uses \\npretrained word embeddings to represent the words in the input sequence. In particular, \\nwe use ALBERT, which captures the different semantics of a word under varied contexts. \\nNote that the pretrained word embeddings capture the semantics of words based on their \\ntypical usage contexts and therefore provide static representations of words; by contrast, \\nALBERT provides a dynamic representation for a word by modeling the particular sen-\\ntence within which the word is used. This layer c aptures four different aspects of a word, \\nand their representation vectors are concatenated together into a large vector to represent \\neach input word. These vectors are then used as the input to next layer, which is a BiLSTM \\nlayer consisting of two layers of LSTM cells: one forward layer capturing information be-\\nfore the target word and one backward layer capturing information after the target word.  \\nThe BiLSTM layer  combines the outputs of the two LSTM layers and feeds the com-\\nbined output into a fully conne cted layer. Then the next layer is a CRF layer , which takes \\nthe output from the fully connected layer and performs sequence labeling. The CRF layer \\nuses the standard BIEO model from NER research to label each word but focuses on loca-\\ntions. Thus, a word is annotated as either “B –L” (the beginning of a location phrase), “I –\\nL” (inside a location phrase), “E –L” (end a location phrase), or “O” (outside a location \\nphrase).  \\nThe workflow of the model is as follows:  \\n(1) First, the dataset is composed of text X  (X1, X2, …, Xn), which is input to the AL-\\nBERT layer, where Xi denotes the i-th word in the input text.  \\nFigure 3. The overall architecture of the proposed model.\\nThe BiLSTM layer combines the outputs of the two LSTM layers and feeds the com‑\\nbined output into a fully connected layer. Then the next layer is a CRF layer, which takes\\nthe output from the fully connected layer and performs sequence labeling. The CRF layer\\nuses the standard BIEO model from NER research to label each word but focuses on lo‑\\ncations. Thus, a word is annotated as either “B–L” (the beginning of a location phrase),\\n“I–L” (inside a location phrase), “E–L” (end a location phrase), or “O” (outside a location\\nphrase).\\nThe workflow of the model is as follows:\\n(1) First, the dataset is composed of text X (X 1, X2,. . ., Xn), which is input to the\\nALBERT layer, where Xidenotes the i‑th word in the input text.\\n(2) The input text data are serialized in the ALBERT layer, and the model generates\\nfeature vectors, Ci, based on each word, Xi, in the text to enhance the text vector repre‑\\nsentation and transforms Ciinto word vectors, E = ( E1,E2,. . .,En), with location features\\nbased on Transformer (Trm) in the word vector representation layer of ALBERT.\\n(3) Using Eias the input of each time step of the bidirectional LSTM layer and perform‑\\ning feature calculation, the forward LSTM F= (F1,F2,. . .,Fn) and the reverse\\nLSTM B = ( B1,B2,. . .,Bn) of the BiLSTM layer are used to extract the contextual features\\nand generate the feature matrix, H= (H1,H2,. . .,Hn), by position splicing to capture the\\nsemantic information in both directions in the sentence.\\n(4) Consider the transfer features between annotations in the CRF layer, obtain the de‑\\npendencies between adjacent labels, and output the corresponding labels Y(Y1,Y2,. . .,Yn)\\nto obtain the final annotation results.\\n4.2. BERT and ALBERT Pretraining Models\\nThe pretraining model provides a better initialization parameter for the neural net‑\\nwork, accelerates the convergence of the neural network, and provides better generaliza‑\\ntion ability on the target task. The development of pretraining models is divided into two\\nstages: shallow word embedding and deep coding. The shallow word embedding mod‑\\nels mainly use the current word and previous word information for training; they only\\nconsider the local information of the text and fail to effectively use the overall information', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 9 of 22\\nof the text [ 22,27]. BERT uses a bidirectional transformer network structure with stronger\\nepistemic capability to train the corpus and achieve a deep bidirectional representation\\nfor pretraining [ 25]. The BERT model’s “masked language model” (MLM) can fuse the\\nleft and right contexts of the current word. BERT has achieved remarkable results in tasks\\nsuch as named‑entity recognition [ 28], text classification, machine translation [ 29], etc. The\\nnext sentence prediction (NSP) captures sentence‑level representations and obtains seman‑\\ntically rich, high‑quality feature representation vectors.\\nHowever, the BERT model contains hundreds of millions of parameters, and the model\\ntraining is easily limited by hardware memory. The ALBERT model is a lightweight pre‑\\ntrained language model that is based on the BERT model [ 30]. The BERT model uses a\\nbidirectional transformer encoder to obtain the feature representation of text, and its model\\nstructure is shown in Figure 4. ALBERT has only 10% of the number of parameters of the\\noriginal BERT model but retains the accuracy of the BERT model.\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  9 of 22  \\n (2) The input text data are serialized in the ALBERT layer, and the model generates \\nfeature vectors , Ci, based on each word , Xi, in the text to enhance the te xt vector represen-\\ntation and transforms Ci into word vectors , E = ( E1, E2, …, En), with location features based \\non Transformer  (Trm) in the word vector representation layer of ALBERT.  \\n(3) Using Ei as the input of each time step of the bidirectional LSTM layer and per-\\nforming feature calculation, the forward LSTM F = (F1, F2, …, Fn) and the reverse LSTM B \\n= (B1, B2, …, Bn) of the BiLSTM layer are used to extract the contextual features and gener-\\nate t he feature matrix , H = (H1, H2, …, Hn), by position splicing to capture the semantic \\ninformation in both directions in the sentence.  \\n(4) Consider the transfer features between annotations in the CRF layer, obtain the \\ndependencies between adjacent labels, a nd output the corresponding labels Y (Y1, Y2, …, \\nYn) to obtain the final annotation results.  \\n4.2. BERT and ALBERT Pretraining Models  \\nThe pretraining model provides a better initialization parameter for the neural net-\\nwork, accelerates the convergence of the neural network, and provides better generaliza-\\ntion ability on the target task. The development of pretraining models is divided into  two \\nstages: shallow word embedding and deep coding. The shallow word embedding models \\nmainly use the current word and previous word information for training ; they  only con-\\nsider the local information of the text and fail to effectively use the overall info rmation of \\nthe text [22,27]. BERT uses a bidirectional transformer network structure with stronger \\nepistemic capability to train the corpus and achieve a deep bidirectional representation \\nfor pretraining [25]. The BERT model’s “masked language model” (MLM)  can fuse the left \\nand right contexts of the current word. BERT has achieved remarkable results in tasks \\nsuch as named -entity recognition [28], text classification, machine translation [29], etc. The \\nnext sentence prediction (NSP) captures sentence -level r epresentations and obtains se-\\nmantically rich, high -quality feature representation vectors.  \\nHowever, the BERT model contains hundreds of millions of parameters, and the \\nmodel training is easily limited by hardware memory. The ALBERT model is a light-\\nweight p retrained language model that is based on the BERT model [30]. The BERT model \\nuses a bidirectional transformer encoder to obtain the feature representation of text, and \\nits model structure is shown in Figure 4. ALBERT has only 10% of the number of param-\\neters of the original BERT model but retains the accuracy of the BERT model.  \\n \\nFigure 4 . Basic structure of the ALBERT model.  \\nThe transformer structure of the BERT model is composed of an encoder and de-\\ncoder . The encoder part mainly consists of six identical layers, and each layer consists of \\ntwo sub -layers, the multi -head self -attention mechanism and the fully connected feed -\\nFigure 4. Basic structure of the ALBERT model.\\nThe transformer structure of the BERT model is composed of an encoder and decoder.\\nThe encoder part mainly consists of six identical layers, and each layer consists of two\\nsub‑layers, the multi‑head self‑attention mechanism and the fully connected feed‑forward\\nnetwork, respectively. Since each sub‑layer is added with residual connection and normal‑\\nization, the output of the sub‑layer can be represented as shown in the following equation:\\nsub_layer _output =LayerNorm (x+(SubLayer (x))) (1)\\nThe multi‑head self‑attention mechanism projects the three matrices, namely Q,V, and\\nK, by hdifferent linear transformations and finally splices the different attention results.\\nThe main calculation equation is shown below:\\nattention _output =Attention (Q,K,V) (2)\\nMultiHead =Concat (head 1, . . . , head h)WO(3)\\nhead i=Attention\\x10\\nQW iQ,KW iK,VW iV\\x11\\n(4)\\nFor the decoder part, the basic structure is similar to the encoder part, but with the\\naddition of a sub‑layer of attention.\\nALBERT uses two methods to reduce the number of parameters: (i) factorized em‑\\nbedding parameterization, which separates the size of the hidden layer from the size of\\nthe lexical embedding matrix by decomposing the huge lexical embedding matrix into\\ntwo smaller matrices; and (ii) cross‑layer parameter sharing, which significantly reduces', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 10 of 22\\nthe number of parameters of the model by sharing the parameters of the neural layer of\\nthe model without significantly affecting its performance.\\nIn the figure, C= (C1,C2,. . .,Cn) indicates that each character in the sequence is\\ntrained by a multilayer bidirectional transformer (Trm) encoder to finally obtain the feature\\nvector of the text, denoted as E= (E1,E2,. . .,En). After the input text is first processed by\\nword embedding, the positional information encoding (positional encoding) of each word\\nin that sentence is added. The model learns more text features by combining multiple self‑\\nattentive layers to form multi‑head attention. The output of the multi‑head attention‑based\\nlayer is passed through the Add&Nom layer, where “ Add” means adding the input and\\noutput of the multi‑head attention layer, and “Norm” means normalization. The result,\\nafter passing through the Add&Nom layer, is passed to the feed‑forward neural layer (Feed\\nForward) and outputted by the Add&Norm layer.\\nThe ALBERT used in this paper has several design features that enhance its perfor‑\\nmance on the task of toponym recognition from social media messages. First, our pre‑\\nsented ALBERT uses the pretrained word embeddings that are specifically derived from\\nsocial media messages. We performed the following steps on the basis of the collected\\ntext data: (1) cleaning the data—we removed the messy codes and incomplete sentences\\nto ensure that the sentences were smooth; (2) cutting the sentences—we added [CLS],\\n[SEP], [MASK], etc., to each text item to obtain 25.6 GB of training data; and (3) training\\ncorpus—we trained on 3090 GPU for 4 days, with the epoch set to 100,000 and learning\\nrate set to 5 ×10−5.\\nWe used the GloVe word embeddings (the number of tokens is 54,238, and the dictio‑\\nnary size is 399 KB) that were trained on 2 billion texts, with 11 billion tokens and 1.8 million\\nvocabulary items collected from Baidu Encyclopedia, Weibo, WeChat, etc. These word em‑\\nbeddings, specifically trained on a large social‑media‑messages corpus, include many ver‑\\nnacular words and unregistered words used by people in social media messages. Previous\\ngeoparsing and NER models typically use word embeddings trained on well‑formatted\\ntext, such as news articles, and many vernacular words are not covered by those embed‑\\ndings. When that happens, an embedding for a generic unknown token is usually used\\nto represent this vernacular word and, as a result, the actual semantics of the word are\\nlost. Second, compared with the basic BiLSTM–CRF model, our presented model adds an\\nALBERT layer to capture the dynamic and contextualized semantics of words.\\n4.3. BiLSTM Layer\\nRecurrent neural networks are more suitable for sequence annotation tasks due to\\ntheir ability to remember the historical information of text sequences. An LSTM model was\\nproposed in the literature [ 31–34] that incorporates specially designed memory units in the\\nhidden layer compared to RNNs and can better solve the problem of gradient explosion\\nor gradient disappearance that RNNs tend to have as the sequence length increases. The\\nneuron structure of the LSTM model is shown in Figure 5.\\nThe LSTM network consists of three gate structures and one state unit; these gate\\nstructures include input gates, oblivion gates, and output gates. The input gate determines\\nhow much of the input to the network is saved to the cell state at the current moment. The\\nforgetting gate selectively discards certain information. The output gate determines the\\nfinal output value based on the cell state. The long‑term dependency problem of recurrent\\nneural networks can be better solved by the three‑gate structure to maintain and update\\nthe state for long‑term memory function. A typical LSTM network structure can be repre‑\\nsented formally in Equations (5)–(10):\\nit=σ(Wi·[ht−1,xt]+bi) (5)\\nft=σ\\x10\\nWf·[ht−1,xt]+bf\\x11\\n(6)\\not=σ(Wo·[ht−1,xt]+bo) (7)', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 11 of 22\\neCt=tanh(Wc·[ht−1,xt]+bC) (8)\\nCt=ft⊗Ct−1+it⊗eCt (9)\\nht=ot⊗tan h (Ct) (10)\\nwhere xtrepresents the input word at moment t;itrepresents the memory gate; ftrepre‑\\nsents the forget gate; otrepresents the output gate; Ctrepresents the cell state; eCtrepresents\\nthe temporary cell state; htrepresents the hidden state output at each time step; ht−1repre‑\\nsents the hidden state at the previous moment; Ct−1represents the cell state at the previous\\nmoment; Wi,Wf,Wo, and Wcrepresent the weight matrix at the current state; and bi,bf,bo,\\nand bCdenote the offset of the current state, respectively.\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  11 of 22  \\n Recurrent neural networks are more suitable for sequence annotation tasks due to \\ntheir ability to remember the historical information of text sequences. A n LSTM model  \\nwas proposed in the literature [31–34] that incorporates specially designed memory units \\nin the hidden layer compared to RNNs and can better solve the problem of gradient ex-\\nplosion or gradient disappearance that RNNs tend to have as the sequence length in-\\ncreases. The neuron structure of the LSTM model is shown in Figure 5.  \\n \\nFigure  5. Neuron structure of LSTM.  \\nThe LSTM network consists of three gate structures and one state unit; these gate \\nstructures include input gates, oblivion gates, and output gates. The input gate determines \\nhow much of the input to the network is saved to the cell state at the current mom ent. The \\nforgetting gate selectively discards certain information. The output gate determines the \\nfinal output value based on the cell state. The long -term dependency problem of recurrent \\nneural networks can be better solved by the three -gate structure to maintain and update \\nthe state for long -term memory function. A typical LSTM network structure can be repre-\\nsented formally in Equation s (5)–(10):  \\n𝑖𝑡=𝜎(𝑊𝑖⋅[ℎ𝑡−1，𝑥𝑡]+𝑏𝑖) (5) \\n𝑓𝑡=𝜎(𝑊𝑓⋅[ℎ𝑡−1，𝑥𝑡]+𝑏𝑓) (6) \\n𝑜𝑡=𝜎(𝑊𝑜⋅[ℎ𝑡−1，𝑥𝑡]+𝑏𝑜) (7) \\n𝐶̃𝑡=tanℎ(𝑊𝑐⋅[ℎ𝑡−1,𝑥𝑡]+𝑏𝐶) (8) \\n𝐶𝑡=𝑓𝑡⨂𝐶𝑡−1+𝑖𝑡⨂𝐶̃𝑡 (9) \\nℎ𝑡=𝑜𝑡⨂𝑡𝑎𝑛 ℎ(𝐶𝑡) (10) \\nwhere xt represents the input word at moment t; it represents the memory gate ; ft repre-\\nsents the forget gate ; ot represents the output gate ; Ct represents the cell state ; 𝐶̃𝑡 repre-\\nsents the temporary cell state ; ht represents the hidden state output at each time step ; ht−1 \\nrepresents the hidden state at the previous moment ; Ct−1 represents the cell state at the \\nprevious moment ; Wi, Wf, Wo, and Wc represent the weight matrix at the current state ; and \\nbi, bf, bo, and bC denote the offset of the current state, respectively.  \\n4.4. CRF Layer  \\nThe conditional random field model is a discriminative probabilistic model [ 34]. The \\nconditional random field m odel combines the advantages of the HMM and maximum \\nFigure 5. Neuron structure of LSTM.\\n4.4. CRF Layer\\nThe conditional random field model is a discriminative probabilistic model [ 34]. The\\nconditional random field model combines the advantages of the HMM and maximum en‑\\ntropy model (MEM). It addresses the strict independence assumption condition of the hid‑\\nden Markov model, avoids the disadvantages of the local optimum and labeling bias prob‑\\nlem of the maximum entropy model, is suitable for the labeling of sequence data CRF,\\nconsiders the sequential problem among labels, and obtains the global optimal labeling\\nsequence through the relationship of adjacent labels, adding constraints to the final pre‑\\ndicted labels. For example, a tag starting with “B” is not followed by an “O” class tag, and\\na tag starting with “E” cannot be sequentially connected with tag “I” sequence. Assuming\\nthat the model input, x= (x1,x2,. . .,xn), has a sequence of tags, y= (y1,y2,. . .,yn), the\\nscore vector of the sentence can be calculated by Equation (7):\\nscore (x,y)=n\\n∑\\nj=1Pi,yi+n\\n∑\\nj=0Ayi,yi+1(11)\\nwhere Pi,yiis the probability of the yilabel of the character, and A is the transfer proba‑\\nbility matrix. The CRF score vector is normalized and trained by using the log‑likelihood\\nfunction as the loss function, as shown in Equation (8):\\nlg(P(y|x)) = score (x,y)−lg(∑\\ny′∈Yxexp\\x00\\nscore\\x00\\nx,y′\\x01\\x01) (12)\\nIn the prediction phase, the network model is labeled by using the Viterbi algorithm\\nto obtain the optimal sequence, as shown in Equation (9):\\ny∗=arg max score\\x00\\nx,y′\\x01\\ny′∈Yx (13)', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 12 of 22\\n5. Results and Discussion\\nOn several datasets, the proposed model’s performance was compared to that of other\\ndeep learning models. Many parts of the experimental data were evaluated and debated. Ten‑\\nsorFlow was used to implement the models on a single NVIDIA GeForce RTX 3090 GPU. Due\\nto the nature of replication, these results may change somewhat from the originals.\\n5.1. Dataset, Evaluation Metrics, and Hyperparameters\\nPerformance measures : The experiment’s measurements were precision (P), recall\\n(R), and the F1‑score (F1). Precision measures the percentage of correctly identified to‑\\nponyms (true positives, TPs) among all the toponyms recognized by a model, which in‑\\nclude both true positives and false positives (FPs). Recall measures the percentage of cor‑\\nrectly identified toponyms among all the toponyms that are annotated as ground truth,\\nwhich include true positives and false negatives (FNs). The F‑score is the harmonic mean\\nof precision and recall [ 35]. It is high when both precision and recall are fairly high, and it\\nis low if either of the two is low [ 36].\\nTraining process : In this study, we trained word embeddings on a Wikipedia corpus\\nby using the word2vec tool in advance, and we concatenated consecutive words to repre‑\\nsent an entity when the entity had multiple words. More importantly, the word embed‑\\ndings obtained by word2vec were used as the initial representation of words. We treated\\nthem as parameters and modified them in the training process, which can provide a better\\nrepresentation of words.\\nTesting methodology : We used 10‑fold cross‑validation for testing and reported the\\naverage score of 10 independent runs. This resulted in a total of 100 different splits into\\ntraining/testing subsets.\\nEach Chinese character was treated as a token, and TPCNER was coded by using the\\nBIO tagging technique. To avoid overfitting, the dropout was adjusted to 0.5. Due to the\\nlikelihood of contextual reliance between neighboring phrases, the maximum length of the\\nsamples was considered to be the maximum training length, and we noted that dividing\\nthe samples might result in semantic loss. To assist the convergence of all models, the\\nnumber of epochs was fixed to 100. At a ratio of 8:2, all datasets were randomly partitioned\\ninto training and validation sets. Table 7compares TPCNER to other datasets and provides\\ncomparative data. Table 8lists the remaining hyperparameters.\\nTable 7. Comparative information between TPCNER and other datasets.\\nDataset Classes Size Entity Size Max Length Min Length Avg Length\\nBoson 6 1827 kb 3417 36 1 19\\nMSRA 3 7.92 MB 19,871 47 1 24\\nRenMinRiBao 3 10,421 kb 12,718 35 1 18\\nTPCNER 7 7.32 MB 64,063 18 2 10\\nTable 8. ALBERT model parameter.\\nNo. Parameter Value\\n1 Hidden size 768\\n2 Embedding size 128\\n3 Max position embeddings 512\\n4 No. of attention heads 12\\n5 No. of hidden layers 12\\n5.2. Baselines\\nTo evaluate the effect of our presented model, we empirically compared our method\\n(ALBERT–BiLSTM–CRF) with six strong baselines (DBN, DM_NLP, NeuroTPR, Chese‑\\nBERTTP, ChineseTR, and GazPNE2). In order to guarantee a relatively fair comparison,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 11}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 13 of 22\\nfor these baselines, we employed their publicly released source codes and followed the\\nparameter settings reported in their papers.\\n• DBN is an adapted toponym recognition approach based on deep belief network\\n(DBN) by exploring two key issues: word representation and model interpretation\\nproposed by [ 37].\\n• DM_NLP is a general model based on BiLSTM–CRF, proposed by [ 38].\\n• NeuroTPR is a Neuro‑net ToPonym Recognition model designed specifically with\\nthese linguistic irregularities in mind, proposed by [ 39].\\n• ChineseBERTTP is a deep neural network named BERT–BiLSTM–CRF, which extends\\na basic bidirectional recurrent neural network model (BiLSTM) with the pretraining\\nbidirectional encoder representation from transformers (BERT) representation to han‑\\ndle the toponym recognition task in Chinese text [ 40].\\n• ChineseTR is a weakly supervised Chinese toponym recognition architecture that\\nleverages a training dataset creator that generates training datasets automatically based\\non word collections and associated word frequencies from various texts and an exten‑\\nsion recognizer that employs a basic bidirectional recurrent neural network based on\\nparticular features designed for toponym recognition proposed by [ 41].\\n• GazPNE2 is a general approach for extracting place names from tweets, named GazPNE2.\\nIt combines global gazetteers (i.e., OpenStreetMap and GeoNames), deep learning,\\nand pretrained transformer models (i.e., BERT and BERTweet), which require no man‑\\nually annotated data [ 42].\\n5.3. Experiments on TPCNER\\nIn this study, the HMM, CRF, BiLSTM–CRF, IDCNN–CRF, IDCNN–CRF2, BiLSTM\\n–Attention–CRF, BERT–BiLSTM–CRF, BERT–BiGRU–CRF, ALBERT–BiLSTM, ALBERT old\\n–BiLSTM–CRF (original ALBERT), and ALBERT ours–BiLSTM–CRF (our presented ALBERT)\\nmodels were used to test the TPCNER dataset, and the performance of named‑entity recog‑\\nnition was evaluated by four indices: accuracy, precision, recall, and F1‑score. The experi‑\\nmental results are shown in Table 9. The following results can be observed:\\nTable 9. Results of different models on TPCNER.\\nModel Accuracy Precision Recall F1‑Score\\nHMM 80.9% −0.16% 80.4% 81.5% 80.7%\\nCRF 83.8% + 0.03% 83.8% 84.1% 84%\\nBiLSTM–CRF 86.1% −0.02% 97.9% 76.6% 86.0%\\nIDCNN–CRF 86.5% + 0.11% 97.9% 77.1% 86.2%\\nIDCNN–CRF2 88.2% + 0.25% 98.0% 79.5% 87.8%\\nBiLSTM–Attention–CRF 89.1% −0.09% 97.5% 72.8% 83.4%\\nBERT–BiLSTM–CRF 91.1% −0.08% 92.8% 91.5% 92.1%\\nBERT–BiGRU–CRF 93.4% −0.28% 93.9% 94.9% 94.4%\\nALBERT old–BiLSTM 88.1% + 0.12% 91.2% 90.7% 90.9%\\nALBERT ours–BiLSTM 92.7% + 0.17% 94.1% 94.5% 94.3%\\nALBERT old–BiLSTM–CRF 90.5% + 0.03% 92.5% 94.4% 93.4%\\nALBERT ours–BiLSTM–CRF 97.8% + 0.07% 96.1% 96.2% 96.1%\\n(1) Compared with the non‑neural‑network models (i.e., HMM and CRF), neural net‑\\nwork models improve the performance significantly, as the performance of the former de‑\\nteriorates quickly, while the latter can maintain a reasonable performance. This is due to\\nthe fact that most of the features used in non‑neural‑network models come from human‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 12}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 14 of 22\\ndesigned features, which suffer from accumulated errors that may lead to performance\\ndegradation.\\n(2) We can see that these eleven models achieved a good performance on the TPC‑\\nNER dataset, and their accuracy, precision, recall, and F1‑scores frequently exceeded 80%.\\nAmong them, the ALBERT ours–BiLSTM–CRF model has the best test effect, and its accu‑\\nracy, precision, recall, and F1‑score are 97.8%, 96.1%, 96.2%, and 96.1%, respectively. Com‑\\npared with the other nine models, this model has a better named‑entity recognition effect\\non the TPCNER dataset. In particular, our re‑trained ALBERT model improved by 7.8%\\ncompared to the original ALBERT model.\\n(3) In addition, IDCNN–CRF2 achieved a better performance than IDCNN–CRF, and\\nIDCNN–CRF and BiLSTM–CRF obtained almost the same performance; both of these re‑\\nsults indicate that IDCNN utilizes dilated convolution to speed up training and does not\\nenhance sequence features to improve performance.\\nWe continued our experiments by comparing ALBERT–BiLSTM–CRF with six deep‑\\nlearning‑based models. The performance of these models on the TPCNER dataset is re‑\\nported in Table 10. We made the following observations:\\nTable 10. Comparison with previous works on TPCNER.\\nModel Precision Recall F1‑Score\\nDBN 0.781 0.774 0.78\\nDM_NLP 0.838 0.841 0.84\\nNeuroTPR 0.871 0.872 0.87\\nChineseBERTTP 0.89 0.894 0.89\\nChineseTR 0.85 0.86 0.85\\nGazPNE2 0.835 0.849 0.84\\nALBERT ours–BiLSTM–CRF 0.961 0.962 0.961\\n(1) ALBERT–BiLSTM–CRF yields the highest precision with the same recall. More‑\\nover, ALBERT–BiLSTM–CRF obtains a constant and substantial improvement over Chi‑\\nneseBERTTP, which currently has the best results reported on this dataset, with higher\\nprecision for the same recall. We believe that the combination of specially designed AL‑\\nBERT features constitutes more significant features and promotes the extractor to make\\naccurate predictions.\\n(2) Compared with the basic BiLSTM–CRF model, ALBERT–BiLSTM–CRF performs\\nbetter in all three metrics, thus demonstrating the value of our improved designs, includ‑\\ning the specially designed ALBERT layers. Compared with DM_NLP and NeuroTPR,\\nALBERT–BiLSTM–CRF shows higher precision, a higher F1‑score, and similar recall.\\nAs expected from Tables 6and 7, for all datasets, ALBERT ours–BiLSTM–CRF achieves\\nthe best F1‑score, i.e., 96.1%. Compared with two weakly supervised deep‑learning mod‑\\nels (NeuroTPR and GazPNE2), our presented model performs better in all three metrics,\\nthus demonstrating the value of our improved design, which contains a fine‑tuned AL‑\\nBERT. The reason is that Chinese texts often include a considerable number of location\\nnames, which may not be covered by the basic BERT, including many vernacular words\\n(e.g., “Mengliang Mountains” and “Plateau”) and abbreviations (e.g., “Dida” and “CUG”)\\napplied by people. When this happens, generic unknown token embedding is usually used\\nto represent the vernacular word, and the actual semantics of the word is lost.\\n5.4. Experiments on the Public Dataset\\nTo better verify the performance of the TPCNER dataset and model, this paper also\\nuses the BiLSTM–CRF, IDCNN–CRF, IDCNN–CRF2, BiLSTM–Attention–CRF, BERT\\n–BiLSTM–CRF, BERT–BiGRU–CRF, ALBERT–BiLSTM, and ALBERT–BiLSTM–CRF mod‑\\nels to test on the Boson dataset, MSRA dataset, and RenMinRiBao dataset and uses preci‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 15 of 22\\nsion, recall, and F1‑scores to evaluate the named entity recognition performance. The ex‑\\nperimental results are shown in Table 11. The experimental results show that these eight\\nmodels achieve good results on these three datasets. Compared with the other seven mod‑\\nels, the ALBERT–BiLSTM–CRF model has the best performance. Its precision, recall, and\\nF1‑score of the three datasets are higher than those of the other models. Compared with\\nthe three public datasets, the named entity recognition effect of the TPCNER dataset is\\nbasically the same, reaching more than 95%. In addition, this paper also counts and visual‑\\nizes the F1‑score of the BERT–BiLSTM–CRF, BERT–BiGRU–CRF, ALBERT–BiLSTM, and\\nALBERT–BiLSTM–CRF models after hyperparameter tuning, as shown in Figure 6. It can\\nbe clearly seen from the figure that the F1‑score of the ALBERT–BiLSTM–CRF model is\\nsignificantly higher than that of the other four models.\\nTable 11. Results of models on the Boson, MSRA, and RenMinRiBao datasets.\\nModelsBoson MSRA RenMinRiBao\\nPrecision Recall F1‑Score Precision Recall F1‑Score Precision Recall F1‑Score\\nBiLSTM–CRF 0.887 0.791 0.836 0.901 0.859 0.876 0.922 0.915 0.918\\nIDCNN–CRF 0.891 0.809 0.848 0.979 0.777 0.866 0.931 0.933 0.932\\nIDCNN–CRF2 0.912 0.909 0.910 0.980 0.771 0.863 0.934 0.941 0.937\\nBiLSTM–Attention–CRF 0.922 0.917 0.919 0.973 0.765 0.841 0.953 0.960 0.956\\nBERT–BiLSTM–CRF 0.932 0.933 0.932 0.974 0.813 0.886 0.961 0.965 0.963\\nBERT–BiGRU–CRF 0.941 0.945 0.943 0.979 0.822 0.894 0.976 0.971 0.973\\nALBERT our–BiLSTM 0.951 0.956 0.953 0.981 0.881 0.928 0.981 0.979 0.980\\nALBERT our–BiLSTM–CRF 0.961 0.962 0.961 0.989 0.895 0.940 0.976 0.986 0.981\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  16 of 22  \\n  \\n \\nFigure  6. The F1 -scores of each model after hyperparameter tuning.  \\nA total of 36 controlled experiments were performed  to determine the best number \\nof labeled phrases from the created dataset and to analyze the size of the labeled dataset. \\nThe first trial used 1000 sentences, whereas the remaining tests used a range of 20 00 to \\n9000 sentences (with a step size of 1000). Figure 7 depicts the experimental outcomes in \\nterms of average accuracy and recall.  \\n  \\nFigure  7. Average F1 -score of the presented and baseline NER algorithms with different sizes of \\nlabeled data.  \\nAs seen in Figure 7, when 9000 sentences were used, the proposed algorithm \\nachieved an average F1 -score of 96.2%. The BERT –BiLSTM–CRF, BERT –BiGRU–CRF, and \\nALBERT –BiLSTM (the baseline) only achieved F1 -scores of 92.1%, 94.4%, and 95.3%, re-\\nspectively. This shows that the proposed NER algorithm outperforms the baseline.  \\n5.5. Ablation Analysis  \\nTo verify the effectiveness of pretraining on our approach, we design the following \\nvariant models and conduct experiments on the constructed dataset ( see Table 12). \\nTable 12. Experimental performance of variant models on the TPCNER dataset.  \\nModel  Precision  Recall  F1-Score  \\nFigure 6. The F1‑scores of each model after hyperparameter tuning.\\nA total of 36 controlled experiments were performed to determine the best number\\nof labeled phrases from the created dataset and to analyze the size of the labeled dataset.\\nThe first trial used 1000 sentences, whereas the remaining tests used a range of 2000 to\\n9000 sentences (with a step size of 1000). Figure 7depicts the experimental outcomes in\\nterms of average accuracy and recall.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 16 of 22\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  16 of 22  \\n  \\n \\nFigure  6. The F1 -scores of each model after hyperparameter tuning.  \\nA total of 36 controlled experiments were performed  to determine the best number \\nof labeled phrases from the created dataset and to analyze the size of the labeled dataset. \\nThe first trial used 1000 sentences, whereas the remaining tests used a range of 20 00 to \\n9000 sentences (with a step size of 1000). Figure 7 depicts the experimental outcomes in \\nterms of average accuracy and recall.  \\n  \\nFigure  7. Average F1 -score of the presented and baseline NER algorithms with different sizes of \\nlabeled data.  \\nAs seen in Figure 7, when 9000 sentences were used, the proposed algorithm \\nachieved an average F1 -score of 96.2%. The BERT –BiLSTM–CRF, BERT –BiGRU–CRF, and \\nALBERT –BiLSTM (the baseline) only achieved F1 -scores of 92.1%, 94.4%, and 95.3%, re-\\nspectively. This shows that the proposed NER algorithm outperforms the baseline.  \\n5.5. Ablation Analysis  \\nTo verify the effectiveness of pretraining on our approach, we design the following \\nvariant models and conduct experiments on the constructed dataset ( see Table 12). \\nTable 12. Experimental performance of variant models on the TPCNER dataset.  \\nModel  Precision  Recall  F1-Score  \\nFigure 7. Average F1‑score of the presented and baseline NER algorithms with different sizes of\\nlabeled data.\\nAs seen in Figure 7, when 9000 sentences were used, the proposed algorithm achieved\\nan average F1‑score of 96.2%. The BERT–BiLSTM–CRF, BERT–BiGRU–CRF, and ALBERT\\n–BiLSTM (the baseline) only achieved F1‑scores of 92.1%, 94.4%, and 95.3%, respectively.\\nThis shows that the proposed NER algorithm outperforms the baseline.\\n5.5. Ablation Analysis\\nTo verify the effectiveness of pretraining on our approach, we design the following\\nvariant models and conduct experiments on the constructed dataset (see Table 12).\\nTable 12. Experimental performance of variant models on the TPCNER dataset.\\nModel Precision Recall F1‑Score\\nBiLSTM–CRF 0.979 0.766 0.860\\n+BERT 0.928 0.915 0.921\\n+ALBERT old 0.925 0.844 0.883\\n+ALBERT our 0.961 0.962 0.961\\nTable 9show the experimental results of BiLSTM–CRF as a baseline method. In Table 9,\\nthe performance of BiLSTM–CRF in all evaluation metrics is poor, compared with other\\nmodels. Moreover, from the overall model F1‑score in Table 9, we found that the use of\\nthe BERT layer or the use of ALBERT layer is higher than the baseline method. This phe‑\\nnomenon shows the effectiveness of the combination of pretraining model.\\nCompared with BiLSTM–CRF, the F1 value of the model can be improved by using\\npretraining model (BERT) in Table 9. The reason may be that pretraining enables better\\ncharacterization of text sequence features. This phenomenon shows the effectiveness of\\nusing pretraining model.\\nCompared with Bi‑LSTM–CRF, the model using spatial attention has improved in\\nregard to the P, R, and F1‑score in Table 9. The reason may be that domain pretrained\\nmodels can better characterize geographic text features and then improves the extraction\\nability of BiLSTM encoding features. This phenomenon shows the effectiveness of using\\ngeographic pretraining model.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 15}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 17 of 22\\n5.6. Discussion\\n5.6.1. Ablation Study\\nWe focused on analyzing the constructed dataset (TPCNER). We examined an exam‑\\nple from the TPCNER corpus to see if the provided model could better detect items in the\\ngeographic domain. In this example, the entity “Gulou Hospital of Harbin Engineering\\nUniversity” appeared just twice in the training set. The entity “Gulou Hospital of Harbin\\nEngineering University” is recognized by the BERT–BiLSTM–CRF model as two entities,\\n“Harbin Engineering University” and “Gulou Hospital”, as shown in Table 13. Because\\nthese two items are more abundant in the training set, recognition without augmentation\\ninformation will be deceptive. Because of inaccurate boundary information, the BERT–\\nBiLSTM–CRF model wrongly classifies “Gulou Hospital of Harbin Engineering Univer‑\\nsity” as an entity. Because more extensive augmentation information is incorporated into\\nour suggested model, it provides accurate predictions. Furthermore, the terms “Harbin En‑\\ngineering University” and “Gulou Hospital” in the sample are similar, implying a tighter\\nrelationship between the entity’s characteristics.\\nTable 13. Results of an instance being predicted by different models. B represents begin, I represents\\ninside, E represents end, and O represents other.\\nOriginal sentence 黑龙江中部出现强降雨，其中哈尔滨工程大学古楼医院周边伴有冰雹。\\nSentence translation Heavy rainfall in central Heilongjiang, including hail in Harbin Yilan County.\\nSentence pinyin (Chinese\\nromanization)Hei Long Jiang Zhong Bu Chu Xian Qiang Jiang Yu, Qi Zhong Ha Er Bin Gong Cheng\\nDa Xue Gu Lou Yi Yuan Zhou Bian Ban You Bing Bao.\\nCorrect LabelHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/I–L gu/I–L\\nlou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\nbHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/E–L gu/B–L\\nlou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\nBERT–BiGRU–CRF predictHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/E–L gu/B–L\\nlou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\nALBERT ours–BiLSTM predictHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/E–L gu/B–L\\nlou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\nALBERT ours–BiLSTM–CRF predictHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/I–L gu/I–L\\nlou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\n5.6.2. Error Analysis\\nWe chose many sentences from the testing set and assessed their sample mistakes to\\nevaluate the real output of different models. Figure 8shows the recognition results of the\\nBERT–BiLSTM–CRF, BERT–BiGRU–CRF, ALBERT–BiLSTM, and ALBERT–BiLSTM–CRF\\nmodels in sample texts, where the red characters denote errors.\\nAs demonstrated in Figure 8, our model outperformed the others in terms of recog‑\\nnition, whereas the BERT–BiLSTM–CRF model failed to distinguish nested entities. For\\ninstance, the ALBERT–BiLSTM–CRF recognized “Yang Xinhe” as an entity in Case 1, but\\nother models cannot recognize this entity because it is a place name consisting of a per‑\\nson’s name, and many algorithms will recognize the person’s name. In Case 2, the BERT\\n–BiLSTM–CRF identifies “Horqin” as an entity, but the related characters “Right Wing\\nFront Banner Debs Town” placed at a long distance in the context were missed. The\\nALBERT –BiLSTM–CRF model successfully identified the fine‑grained nested entities “Deebs\\nTownship, Horqin Right Wing Front Banner” in Case 2.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 18 of 22\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  18 of 22  \\n ALBERT ours–BiLSTM pre-\\ndict Hei/B–L long /I–L jiang /E–L zhong /O bu/O di/O qu/O chu/O xian /O qiang /O jiang /O yu/O, \\n/O qi/O zhong /ha/B–L er/I–L bin/I–L gong /I–L cheng /I–L da/I–L xue/E–L gu/B–L lou/I–L \\nyi/I–L yuan /E–L zhou /O bian /O ban/O you/O bing /O bao/O. /O \\nALBERT ours–BiLSTM–CRF \\npredict  Hei/B–L long /I–L jiang /E–L zhong /O bu/O di/O qu/O chu/O xian /O qiang /O jiang /O yu/O, \\n/O qi/O zhong /ha/B–L er/I–L bin/I–L gong /I–L cheng /I–L da/I–L xue/I–L gu/I–L lou/I–L \\nyi/I–L yuan /E–L zhou /O bian /O ban/O you/O bing /O bao/O. /O \\n5.6.2. Error Analysis  \\nWe chose many sentences from the testing set and assessed their sample mistakes to \\nevaluate the real output of different models. Figure 8 shows the recognition results of the \\nBERT–BiLSTM–CRF, BERT –BiGRU–CRF, ALBERT –BiLSTM, and ALBERT –BiLSTM–CRF \\nmodels in sample texts, where the red characters denote erro rs. \\n \\nFigure  8. Error analysis of some typical cases. Blue represents standard place -name labeling, and \\nred represents model identification place names.  The translation of the sentence “杨信河以北的李庆\\n县发生了特大暴雨 ” is “Very heavy rainfall occurred in Liqing County north of Yang Xin River ”; the \\ntranslation of the sentence “风雹灾害致科尔沁右翼前旗德伯斯镇作物倒伏 ” is “Wind and hail disas-\\nter caused the Khorqin Right Wing Front Banner Debs town crop collapse ”. \\nAs demonstrated in Figure 8, our model outperformed the others in  terms of recog-\\nnition, whereas the BERT –BiLSTM–CRF model failed to distinguish nested entities. For \\ninstance, the ALBERT –BiLSTM–CRF recognized “Yang Xinhe” as an entity in Case 1, but \\nother models cannot recognize this entity because it is a place name con sisting of a per-\\nson’s name, and many algorithms will recognize the person’s name. In Case 2, the BERT –\\nBiLSTM–CRF identifies “Horqin” as an entity, but the related characters “Right Wing \\nFront Banner Debs Town” placed at a long distance in the context were missed. The AL-\\nBERT–BiLSTM–CRF model successfully identified the fine -grained nested entities “Deebs \\nTownship, Horqin Right Wing Front Banner” in Case 2.  \\nBy analyzing the recognition results, we found that (1) the reason for affecting the \\naccuracy of the mo del is that some of the names in the data contain toponymic words, \\nresulting in incorrect recall, e.g., “Yang Xinhe” and “Li Qingxian”; (2) the reason for the \\nlow recall is that some of the complex names are not correctly recognized, e.g., “Deebs \\nTown of H orqin Right -wing Front Banner” is not correctly recognized. For example, in \\n“wind and hail disaster caused crop collapse in Debs town of horqin right -wing front ban-\\nner”, “Debs town of horqin right -wing front banner” was not correctly identified; in the \\nface of rain and flood, Qiongzhong Li and Miao autonomous county urgently relocated. \\nThe name “Qiongzhong Li and Miao Autonomous County” was not correctly identified \\nin “35 people”. The reason is that the place name is long, the frequency of occurrence in \\nthe corpus is low, and the model does not learn enough, so it is not correctly recalled.  \\n5.6.3. Annotated Quality Analysis  \\nFigure 8. Error analysis of some typical cases. Blue represents standard place‑name label‑\\ning, and red represents model identification place names. The translation of the sentence “\\n杨信河以北的李庆县发生了特大暴雨 ” is “Very heavy rainfall occurred in Liqing County north of\\nYang Xin River”; the translation of the sentence “ 风雹灾害致科尔沁右翼前旗德伯斯镇作物倒伏 ” is\\n“Wind and hail disaster caused the Khorqin Right Wing Front Banner Debs town crop collapse”.\\nBy analyzing the recognition results, we found that (1) the reason for affecting the\\naccuracy of the model is that some of the names in the data contain toponymic words,\\nresulting in incorrect recall, e.g., “Yang Xinhe” and “Li Qingxian”; (2) the reason for the low\\nrecall is that some of the complex names are not correctly recognized, e.g., “Deebs Town\\nof Horqin Right‑wing Front Banner” is not correctly recognized. For example, in “wind\\nand hail disaster caused crop collapse in Debs town of horqin right‑wing front banner”,\\n“Debs town of horqin right‑wing front banner” was not correctly identified; in the face\\nof rain and flood, Qiongzhong Li and Miao autonomous county urgently relocated. The\\nname “Qiongzhong Li and Miao Autonomous County” was not correctly identified in “35\\npeople”. The reason is that the place name is long, the frequency of occurrence in the\\ncorpus is low, and the model does not learn enough, so it is not correctly recalled.\\n5.6.3. Annotated Quality Analysis\\nBiLSTM–CRF and IDCNN are considered the most basic models and were used to assess\\nthe quality of the annotated corpus, using hierarchical 10‑fold cross‑validation [ 36,37].At the\\nmacro level, the detailed experimental results presented in Table 6show that BiLSTM–CRF\\nand IDCNN achieve F1‑scores of 86% and 87%, respectively. At the micro level, BiLSTM\\n–CRF and IDCNN show excellent performance for traffic, water systems, and organization,\\nthus indicating the ease of identification of these categories. In particular, for organiza‑\\ntional agencies, the F1‑score of both models is 92.16% and 93.79%, respectively. Due to\\ndiscrepancies created by the absence of boundary characteristics and the mixed usage of\\ncharacters, digits, and letters, some things, such as extremely specified place names, mixed\\nplace names, and merged place names, are difficult to recognize. Figure 2demonstrated\\nhow the lack of data for some categories has an impact on performance. In addition, as\\nindicated in Figure 5, we mentioned several forecast mistakes. Overall, the assessment\\nfindings show that the corpus annotated in this study is reliable and may be utilized to\\nrecognize geographic domain entities.\\nThe confusion matrix in Figures 9and 10shows the number of toponyms that were\\nextracted from the dataset by using the proposed algorithm, as well as the number of\\ngold‑standard annotations, for each toponym class. Figure 10shows that the proposed\\nalgorithm has a relatively lower precision for the TRA toponym classes. This could be\\nattributed to data imbalance. The imbalance in entity number causes the algorithm to\\nfocus on minimizing classification errors for the entities with a larger number, while insuf‑\\nficiently considering the errors for the entities with a smaller number.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 19 of 22\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  19 of 22  \\n BiLSTM–CRF and IDCNN are considered the most basic models and were used to \\nassess the quality of the annotated corpus , using hierarchical 10 -fold cross -validation \\n[36,37]. At the macro level, the detailed experimental results presented in Table 6 show \\nthat BiLSTM –CRF and IDCNN achieve F1 -scores of 86% and 87%, respectively. At the \\nmicro level, BiLSTM –CRF and IDCNN show ex cellent performance for traffic, water sys-\\ntems, and organization, thus  indicat ing the ease of identification of these categories. In \\nparticular, for organizational agencies, the F1 -score of both models is 92.16% and 93.79%, \\nrespectively. Due to discrepanci es created by the absence of boundary characteristics and \\nthe mixed usage of characters, digits, and letters, some things, such as extremely specified \\nplace names, mixed place names, and merged place names, are difficult to recognize. Fig-\\nure 2 demonstrate d how the lack of data for some categories has an impact on perfor-\\nmance. In addition, as indicated in Figure 5, we mention ed several forecast mistakes. \\nOverall, the assessment findings show that the corpus annotated in this study is reliable \\nand may be util ized to recognize geographic domain entities.  \\nThe confusion matrix in Figures 9 and 10 shows the number of toponyms that were \\nextracted from the dataset by using the proposed algorithm, as well as the number of gold -\\nstandard annotations, for each toponym c lass. Figure 10 shows that the proposed algo-\\nrithm has a relatively lower precision for the TRA toponym classes. This could be at-\\ntributed to data imbalance. The imbalance in entity number causes the algorithm to focus \\non minimizing classification errors for  the entities with a larger number, while insuffi-\\nciently considering the errors for the entities with a smaller number.  \\n  \\nFigure  9. Confusion matrix for all extracted and gold -standard toponym from the constructed da-\\ntaset based on ALBERT –BiLSTM–CRF. WAT  = water system; RLF  = residential land and facilities; \\nTRA  = transportation; PIP  = pipelines; BRO  = boundaries, regions, and other areas; LAN  = land-\\nforms; ORG  = organization.  \\nFigure 9. Confusion matrix for all extracted and gold‑standard toponym from the constructed\\ndataset based on ALBERT–BiLSTM–CRF. WAT = water system; RLF = residential land and facilities;\\nTRA = transportation; PIP = pipelines; BRO = boundaries, regions, and other areas; LAN = landforms;\\nORG = organization.\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  20 of 22  \\n   \\nFigure 10. Confusion matrix for all extracted and gold -standard toponym from the constructed da-\\ntaset based on ALBERT –BiLSTM–CRF. WAT  = water system; RLF  = residential land and facilities; \\nTRA  = transportation; PIP  = pipelines; BRO  = boundaries, regions, and other areas; LAN  = land-\\nforms; ORG  = organization.  \\n6. Conclusio ns and Future Work  \\nIn this paper, we propose a hybrid neural network method for Chinese place -name \\nrecognition that solves the above problems by learning word -level feature representations \\nin the ALBERT layer, extracting contextual semantic features in the BiLSTM layer, and \\ngenerating optimal label sequences in the CRF layer. The experimental results show that \\nthe prop osed toponym recognition method has good performance in all evaluation indi-\\nces. We train ALBERT –BiLSTM–CRF by using a constructed human -annotated dataset \\nand three public datasets. We experimented with several training procedures and discov-\\nered that a mix of human -annotated data produces the greatest results. Evaluation exper-\\niments based on three test datasets, namely Boson, MSRA, and RenMinRiBao, demon-\\nstrate the improved performance of ALBERT –BiLSTM–CRF in comparison with a set of \\ndeep learning models. Thi s work attempted to serve as a resource for named -entity -recog-\\nnition studies in various geographic areas. We  will work on including more features and \\nmaking more sensible modifications to the weights of these features in the future.  \\n \\nAuthor Contributions:   Conceptualization, Liufeng Tao and Qinjun Qiu; methodology,  Qinjun \\nQiu; validation, Dexin Xu and Shengyong Pan; formal analysis, Kai Ma; investigation, Liufeng Tao; \\nresources, Qinjun Qiu; data curation, Liufeng  Tao; writing —original draft preparation, Liufeng Tao; \\nwriting—review and editing, Qinjun Qiu; supervision, Qinjun Qiu; funding acquisition, Zhong Xie, \\nQinjun Qiu and Bo Huang. All authors have read and agreed to the published version of the man-\\nuscript.  \\nFunding:   This study was financially supported by the National Natural Science Foundation of \\nChina (42050101), Beijing Key Laboratory of Urban Spatial Information Engineering (No.20220108) , \\nthe China Postdoctoral Science Foundation (No.2021M702991), Wu han Multi -Element Urban Geo-\\nlogical Survey Demonstration Project (WHDYS -2021 -014), the Open Research Project of The Hubei \\nFigure 10. Confusion matrix for all extracted and gold‑standard toponym from the constructed\\ndataset based on ALBERT–BiLSTM–CRF. WAT = water system; RLF = residential land and facilities;\\nTRA = transportation; PIP = pipelines; BRO = boundaries, regions, and other areas; LAN = landforms;\\nORG = organization.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 20 of 22\\n6. Conclusions and Future Work\\nIn this paper, we propose a hybrid neural network method for Chinese place‑name\\nrecognition that solves the above problems by learning word‑level feature representations\\nin the ALBERT layer, extracting contextual semantic features in the BiLSTM layer, and gen‑\\nerating optimal label sequences in the CRF layer. The experimental results show that the\\nproposed toponym recognition method has good performance in all evaluation indices.\\nWe train ALBERT–BiLSTM–CRF by using a constructed human‑annotated dataset and\\nthree public datasets. We experimented with several training procedures and discovered\\nthat a mix of human‑annotated data produces the greatest results. Evaluation experiments\\nbased on three test datasets, namely Boson, MSRA, and RenMinRiBao, demonstrate the im‑\\nproved performance of ALBERT–BiLSTM–CRF in comparison with a set of deep learning\\nmodels. This work attempted to serve as a resource for named‑entity‑recognition studies\\nin various geographic areas. We will work on including more features and making more\\nsensible modifications to the weights of these features in the future.\\nAuthor Contributions: Conceptualization, Liufeng Tao and Qinjun Qiu; methodology, Qinjun Qiu; val‑\\nidation, Dexin Xu and Shengyong Pan; formal analysis, Kai Ma; investigation, Liufeng Tao; resources,\\nQinjun Qiu; data curation, Liufeng Tao; writing—original draft preparation, Liufeng Tao; writing—\\nreview and editing, Qinjun Qiu; supervision, Qinjun Qiu; funding acquisition, Zhong Xie, Qinjun Qiu\\nand Bo Huang. All authors have read and agreed to the published version of the manuscript.\\nFunding: This study was financially supported by the National Natural Science Foundation of China\\n(42050101), Beijing Key Laboratory of Urban Spatial Information Engineering (No.20220108), the\\nChina Postdoctoral Science Foundation (No.2021M702991), Wuhan Multi‑Element Urban Geologi‑\\ncal Survey Demonstration Project (WHDYS‑2021‑014), the Open Research Project of The Hubei Key\\nLaboratory of Intelligent Geo‑Information Processing (No. KLIGIP‑2021A01), and Wuhan Science\\nand Technology Plan Project (No.2020010602012022).\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: All original data and codes can be found in the Zenodo ( https://zenodo.\\norg/record/6482711#.YmZxWMjAiAc ) and accessed on 1 January 2022.\\nAcknowledgments: The authors thank the four anonymous reviewers for the positive, constructive,\\nand valuable comments and suggestions.\\nConflicts of Interest: The authors declare that they have no known competing financial interests or\\npersonal relationships that could have appeared to influence the work reported in this paper.\\nReferences\\n1. Imran, M.; Castillo, C.; Diaz, F.; Vieweg, S. Processing social media messages in mass emergency: A survey. ACM Comput. Surv.\\n2015 ,47, 67. [ CrossRef ]\\n2. Silverman, L. Facebook, Twitter Replace 911 Calls for Stranded in Houston. 2017. Available online: https://www.npr.\\norg/sections/alltechconsidered/2017/08/28/546831780/texas‑police‑and‑residents‑turn‑to‑social‑media‑to‑communicateamid‑\\nharvey (accessed on 12 September 2017).\\n3. Yu, M.; Huang, Q.; Qin, H.; Scheele, C.; Yang, C. Deep learning for real‑time social media text classification for situation\\nawareness—Using hurricanes Sandy, Harvey, and Irma as case studies. Int. J. Digit. Earth 2019 ,12, 1230–1247. [ CrossRef ]\\n4. Hu, Y.; Mao, H.; McKenzie, G. A natural language processing and geospatial clustering framework for harvesting local place\\nnames from geotagged housing advertisements. Int. J. Geogr. Inf. Sci. 2018 ,33, 714–738. [ CrossRef ]\\n5. Freire, N.; Borbinha, J.; Calado, P.; Martins, B. A metadata geoparsing system for place name recognition and resolution in\\nmetadata records. In Proceedings of the 11th International ACM/IEEE Joint Conference on Digital Libraries, Ottawa, ON, Canada,\\n13–17 June 2011; pp. 339–348.\\n6. Gelernter, J.; Balaji, S. An algorithm for local geoparsing of microtext. Geoinformatica 2013 ,17, 635–667. [ CrossRef ]\\n7. Gritta, M.; Pilehvar, M.T.; Limsopatham, N.; Collier, N. What’s missing in geographical parsing? Lang. Resour. Eval. 2018 ,52,\\n603–623. [ CrossRef ]\\n8. Jones, C.B.; Purves, R.S. Geographical information retrieval. Int. J. Geogr. Inf. Sci. 2008 ,22, 219–228. [ CrossRef ]\\n9. Purves, R.S.; Clough, P.; Jones, C.B.; Hall, M.H.; Murdock, V. Geographic Information Retrieval: Progress and Challenges in\\nSpatial Search of Text. Found. Trends®Inf. Retr. 2018 ,12, 164–318. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 21 of 22\\n10. Derczynski, L.; Nichols, E.; Van Erp, M.; Limsopatham, N. Results of the WNUT2017 shared task on novel and emerging entity\\nrecognition. In Proceedings of the Third Workshop on Noisy User‑Generated Text, Copenhagen, Denmark, 7 September 2017;\\npp. 140–147.\\n11. Li, H.; Wang, M.; Baldwin, T.; Tomko, M.; Vasardani, M. UniMelb at SemEval‑2019 Task 12: Multi‑model combination for\\ntoponym resolution. In Proceedings of the 13th International Workshop on Semantic Evaluation, Minneapolis, MN, USA, 6–7\\nJune 2019; ACL: Stroudsburg, PA, USA; pp. 1313–1318.\\n12. Qiu, Q.; Xie, Z.; Wu, L.; Tao, L.; Li, W. BiLSTM‑CRF for geological named entity recognition from the geoscience literature. Earth\\nSci. Inform. 2019 ,12, 565–579. [ CrossRef ]\\n13. Qiu, Q.; Xie, Z.; Wu, L.; Tao, L. GNER: A generative model for geological named entity recognition without labeled data using\\ndeep learning. Earth Space Sci. 2019 ,6, 931–946. [ CrossRef ]\\n14. Santos, R.; Murrieta‑Flores, P.; Calado, P.; Martins, B. Toponym matching through deep neural networks. Int. J. Geogr. Inf. Sci.\\n2018 ,32, 324–348. [ CrossRef ]\\n15. Wang, J.; Hu, Y. Enhancing spatial and textual analysis with EUPEG: An extensible and unified platform for evaluating geop‑\\narsers. Trans. GIS 2019 ,23, 1393–1419. [ CrossRef ]\\n16. Herskovits, A. Language and Spatial Cognition: An interdisciplinary Study of Prepositions in English ; Cambridge University Press:\\nCambridge, UK, 1986.\\n17. Talmy, L. Toward a Cognitive Semantics: Concept Structuring Systems ; The MIT Press: Cambridge, MA, USA, 2000.\\n18. Stock, K.; Yousaf, J. Context‑aware automated interpretation of elaborate natural language descriptions of location through\\nlearning from empirical data. Int. J. Geogr. Inf. Sci. 2018 ,32, 1087–1116. [ CrossRef ]\\n19. Cohen, W.; Ravikumar, P.; Fienberg, S. A comparison of string distance metrics for namematching tasks. In Proceedings of KDD\\nWorkshop on Data Cleaning and Object Consolidation, Washington, DC, USA, 24–27 August 2003.\\n20. Moreau, E.; Yvon, F.; Capp, E.O. Robust similarity measures for named entities matching. In Proceedings of the International\\nConference on Computational Linguistics, Manchester, UK, 18–22 August 2008.\\n21. Santos, R.; Murrieta‑Flores, P.; Martins, B. Learning to combine multiple string similarity metrics for effective toponym matching.\\nInt. J. Digit. Earth 2018 ,11, 913–938. [ CrossRef ]\\n22. Ma, K.; Tan, Y.; Tian, M.; Xie, X.; Qiu, Q.; Li, S.; Wang, X. Extraction of temporal information from social media messages using\\nthe BERT model. Earth Sci. Inform. 2022 ,15, 573–584. [ CrossRef ]\\n23. Qiu, Q.; Xie, Z.; Ma, K.; Chen, Z.; Tao, L. Spatially oriented convolutional neural network for spatial relation extraction from\\nnatural language texts. Trans. GIS 2021 ,26, 839–866. [ CrossRef ]\\n24. Qiu, Q.; Xie, Z.; Ma, K.; Chen, Z.; Tao, L. Spatially oriented convolutional neural network for spatial relation extraction from\\nnatural language texts. Trans. GIS 2022 ,26, 839–866. [ CrossRef ]\\n25. Devlin, J.; Chang, M.W.; Lee, K.; Toutanova, K. Bert: Pre‑training of deep bidirectional transformers for language understanding.\\narXiv 2018 , arXiv:1810.04805.\\n26. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I. Language models are unsupervised multitask learners. OpenAI\\nblog 2019 ,1, 9.\\n27. Ling, W.; Dyer, C.; Black, A.W.; Trancoso, I. Two/too simple adaptations of word2vec for syntax problems. In Proceedings\\nof the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Denver, CO, USA, May–June 2015; pp. 1299–1304.\\n28. Lv, X.; Xie, Z.; Xu, D.; Jin, X.; Ma, K.; Tao, L.; Qiu, Q.; Pan, Y. Chinese Named Entity Recognition in the Geoscience Domain Based\\non BERT. Earth Space Sci. 2022 ,9, e2021EA002166. [ CrossRef ]\\n29. Ma, K.; Tian, M.; Tan, Y.; Xie, X.; Qiu, Q. What is this article about? Generative summarization with the BERT model in the\\ngeosciences domain. Earth Sci. Inform. 2021 ,15, 21–36. [ CrossRef ]\\n30. Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; Soricut, R. Albert: A lite bert for self‑supervised learning of language\\nrepresentations. arXiv 2019 , arXiv:1909.11942.\\n31. Hochreiter, S.; Schmidhuber, J. Long short‑term memory. Neural Comput. 1997 ,9, 1735–1780. [ CrossRef ]\\n32. Graves, A. Long short‑term memory. In Supervised Sequence Labelling with Recurrent Neural Networks ; Springer: Berlin/Heidelberg,\\nGermany, 2012; pp. 37–45.\\n33. Qiu, Q.; Xie, Z.; Wu, L.; Li, W. DGeoSegmenter: A dictionary‑based Chinese word segmenter for the geoscience domain. Comput.\\nGeosci. 2018 ,121, 1–11. [ CrossRef ]\\n34. Song, S.; Zhang, N.; Huang, H. Named entity recognition based on conditional random fields. Clust. Comput. 2017 ,22, 5195–5206.\\n[CrossRef ]\\n35. Guo, X.; Zhou, H.; Su, J.; Hao, X.; Tang, Z.; Diao, L.; Li, L. Chinese agricultural diseases and pests named entity recognition with\\nmulti‑scale local context features and self‑attention mechanism. Comput. Electron. Agric. 2020 ,179, 105830. [ CrossRef ]\\n36. Leitner, E.; Rehm, G.; Moreno‑Schneider, J. A dataset of german legal documents for named entity recognition. arXiv 2020 ,\\narXiv:2003.13016.\\n37. Wang, S.; Zhang, X.; Ye, P.; Du, M. Deep Belief Networks Based Toponym Recognition for Chinese Text. ISPRS Int. J. Geo‑Inf.\\n2018 ,7, 217. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 22 of 22\\n38. Wang, X.; Ma, C.; Zheng, H.; Liu, C.; Xie, P.; Li, L.; Si, L. DM NLP at SemEval 2018 Task 12: A pipeline system for toponym\\nresolution. In Proceedings of the 13th International Workshop on Semantic Evaluation, Minneapolis, MN, USA, 6–7 June 2019;\\npp. 917–923.\\n39. Wang, J.; Hu, Y.; Joseph, K. NeuroTPR: A neuro‑net toponym recognition model for extracting locations from social media\\nmessages. Trans. GIS 2020 ,24, 719–735. [ CrossRef ]\\n40. Ma, K.; Tan, Y.; Xie, Z.; Qiu, Q.; Chen, S. Chinese toponym recognition with variant neural structures from social media messages\\nbased on BERT methods. J. Geogr. Syst. 2022 ,24, 143–169. [ CrossRef ]\\n41. Qiu, Q.; Xie, Z.; Wang, S.; Zhu, Y.; Lv, H.; Sun, K. ChineseTR: A weakly supervised toponym recognition architecture based on\\nautomatic training data generator and deep neural network. Trans. GIS 2022 ,26, 1256–1279. [ CrossRef ]\\n42. Hu, X.; Zhou, Z.; Sun, Y.; Kersten, J.; Klan, F.; Fan, H.; Wiegmann, M. GazPNE2: A General Place Name Extractor for Microblogs\\nFusing Gazetteers and Pretrained Transformer Models. IEEE Internet Things J. 2022 ,9, 16259–16271. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 21})]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXN5dHB1xMHD",
        "outputId": "edb77567-7f06-4985-ec62-662e7ebb644c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Citation: Tao, L.; Xie, Z.; Xu, D.; Ma,\\nK.; Qiu, Q.; Pan, S.; Huang, B.\\nGeographic Named Entity\\nRecognition by Employing Natural\\nLanguage Processing and an\\nImproved BERT Model. ISPRS Int. J.\\nGeo‑Inf. 2022 ,11, 598. https://doi.org/\\n10.3390/ijgi11120598\\nAcademic Editors: Maria Antonia\\nBrovelli and Wolfgang Kainz\\nReceived: 15 September 2022\\nAccepted: 24 November 2022\\nPublished: 28 November 2022\\nPublisher’s Note: MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional affil‑\\niations.\\nCopyright: © 2022 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license ( https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\n International Journal of\\nGeo-Information\\nArticle\\nGeographic Named Entity Recognition by Employing Natural\\nLanguage Processing and an Improved BERT Model\\nLiufeng Tao1,2, Zhong Xie1,2, Dexin Xu3, Kai Ma4,5, Qinjun Qiu1,2,6,*, Shengyong Pan7and Bo Huang7\\n1School of Computer Science, China University of Geosciences, Wuhan 430074, China\\n2Beijing Key Laboratory of Urban Spatial Information Engineering, Beijing 100038, China\\n3Wuhan Geomatics Institute, Wuhan 430074, China\\n4Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering,\\nChina Three Gorges University, Yichang 443002, China\\n5College of Computer and Information Technology, China Three Gorges University, Yichang 443002, China\\n6Hubei Key Laboratory of Intelligent Geo‑Information Processing, China University of Geosciences,\\nWuhan 430074, China\\n7Wuhan Zondy Cyber Science & Technology Co., Ltd., Wuhan 430074, China\\n*Correspondence: qiuqinjun@cug.edu.cn\\nAbstract: Toponym recognition, or the challenge of detecting place names that have a similar refer‑\\nent, is involved in a number of activities connected to geographical information retrieval and geo‑\\ngraphical information sciences. This research focuses on recognizing Chinese toponyms from social\\nmedia communications. While broad named entity recognition methods are frequently used to lo‑\\ncate places, their accuracy is hampered by the many linguistic abnormalities seen in social media\\nposts, such as informal sentence constructions, name abbreviations, and misspellings. In this study,\\nwe describe a Chinese toponym identification model based on a hybrid neural network that was\\ncreated with these linguistic inconsistencies in mind. Our method adds a number of improvements\\nto a standard bidirectional recurrent neural network model to help with location detection in social\\nmedia messages. We demonstrate the results of a wide‑ranging evaluation of the performance of dif‑\\nferent supervised machine learning methods, which have the natural advantage of avoiding human\\ndesign features. A set of controlled experiments with four test datasets (one constructed and three\\npublic datasets) demonstrates the performance of supervised machine learning that can achieve good\\nresults on the task, significantly outperforming seven baseline models.\\nKeywords: geographic named entity recognition; social media message; natural language processing;\\nBERT; toponyms recognition\\n1. Introduction\\nOnline social media platforms, especially microblog platforms such as Wechat and\\nWeibo, are responsive to real‑world events and are useful for gathering situational infor‑\\nmation in real time [ 1–4]. Geographic locations are often described in these messages. For\\nexample, Weibo is widely used in disaster response and rescue, such as earthquakes, floods,\\nfire, and terrorist attacks. The cornerstone of the aforementioned application is called geop‑\\narsing [ 5–7]. Geoparsing is a difficult natural language processing (NLP) task that aligns\\nnaturally stated things in free text spatially (written or obtained through automatic tran‑\\nscription) [ 7–9]. It is important to understand the distinction between geographic parsing\\nand geocoding. The input to geographic parsing does not include any information about\\nthe places indicated in the input. In geocoding, a valid textual representation of the loca‑\\ntion (address) is the input. As a result, the geocoder needs to merely look up the supplied\\naddress’s coordinates in a gazetteer. Geocoding is difficult due to the fact that it deals\\nwith raw natural language data. In this paper, we show initial progress in creating the\\nfirst geographic name parsing system for a Chinese language. To achieve this goal, the\\nISPRS Int. J. Geo‑Inf. 2022 ,11, 598. https://doi.org/10.3390/ijgi11120598 https://www.mdpi.com/journal/ijgi', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)"
      ],
      "metadata": {
        "id": "VKd8I_vBzNdK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "8g7dymVO1mmS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvKcBbWg1rcL",
        "outputId": "2a4e559c-c081-4067-f85b-b88437b0f0bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Citation: Tao, L.; Xie, Z.; Xu, D.; Ma,\\nK.; Qiu, Q.; Pan, S.; Huang, B.\\nGeographic Named Entity\\nRecognition by Employing Natural\\nLanguage Processing and an\\nImproved BERT Model. ISPRS Int. J.\\nGeo‑Inf. 2022 ,11, 598. https://doi.org/\\n10.3390/ijgi11120598\\nAcademic Editors: Maria Antonia\\nBrovelli and Wolfgang Kainz\\nReceived: 15 September 2022\\nAccepted: 24 November 2022\\nPublished: 28 November 2022\\nPublisher’s Note: MDPI stays neutral\\nwith regard to jurisdictional claims in', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='published maps and institutional affil‑\\niations.\\nCopyright: © 2022 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license ( https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\n International Journal of\\nGeo-Information\\nArticle\\nGeographic Named Entity Recognition by Employing Natural\\nLanguage Processing and an Improved BERT Model', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='Liufeng Tao1,2, Zhong Xie1,2, Dexin Xu3, Kai Ma4,5, Qinjun Qiu1,2,6,*, Shengyong Pan7and Bo Huang7\\n1School of Computer Science, China University of Geosciences, Wuhan 430074, China\\n2Beijing Key Laboratory of Urban Spatial Information Engineering, Beijing 100038, China\\n3Wuhan Geomatics Institute, Wuhan 430074, China\\n4Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering,\\nChina Three Gorges University, Yichang 443002, China', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='5College of Computer and Information Technology, China Three Gorges University, Yichang 443002, China\\n6Hubei Key Laboratory of Intelligent Geo‑Information Processing, China University of Geosciences,\\nWuhan 430074, China\\n7Wuhan Zondy Cyber Science & Technology Co., Ltd., Wuhan 430074, China\\n*Correspondence: qiuqinjun@cug.edu.cn\\nAbstract: Toponym recognition, or the challenge of detecting place names that have a similar refer‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='ent, is involved in a number of activities connected to geographical information retrieval and geo‑\\ngraphical information sciences. This research focuses on recognizing Chinese toponyms from social\\nmedia communications. While broad named entity recognition methods are frequently used to lo‑\\ncate places, their accuracy is hampered by the many linguistic abnormalities seen in social media\\nposts, such as informal sentence constructions, name abbreviations, and misspellings. In this study,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='we describe a Chinese toponym identification model based on a hybrid neural network that was\\ncreated with these linguistic inconsistencies in mind. Our method adds a number of improvements\\nto a standard bidirectional recurrent neural network model to help with location detection in social\\nmedia messages. We demonstrate the results of a wide‑ranging evaluation of the performance of dif‑\\nferent supervised machine learning methods, which have the natural advantage of avoiding human', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='design features. A set of controlled experiments with four test datasets (one constructed and three\\npublic datasets) demonstrates the performance of supervised machine learning that can achieve good\\nresults on the task, significantly outperforming seven baseline models.\\nKeywords: geographic named entity recognition; social media message; natural language processing;\\nBERT; toponyms recognition\\n1. Introduction\\nOnline social media platforms, especially microblog platforms such as Wechat and', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='Weibo, are responsive to real‑world events and are useful for gathering situational infor‑\\nmation in real time [ 1–4]. Geographic locations are often described in these messages. For\\nexample, Weibo is widely used in disaster response and rescue, such as earthquakes, floods,\\nfire, and terrorist attacks. The cornerstone of the aforementioned application is called geop‑\\narsing [ 5–7]. Geoparsing is a difficult natural language processing (NLP) task that aligns', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='naturally stated things in free text spatially (written or obtained through automatic tran‑\\nscription) [ 7–9]. It is important to understand the distinction between geographic parsing\\nand geocoding. The input to geographic parsing does not include any information about\\nthe places indicated in the input. In geocoding, a valid textual representation of the loca‑\\ntion (address) is the input. As a result, the geocoder needs to merely look up the supplied', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='address’s coordinates in a gazetteer. Geocoding is difficult due to the fact that it deals\\nwith raw natural language data. In this paper, we show initial progress in creating the\\nfirst geographic name parsing system for a Chinese language. To achieve this goal, the\\nISPRS Int. J. Geo‑Inf. 2022 ,11, 598. https://doi.org/10.3390/ijgi11120598 https://www.mdpi.com/journal/ijgi', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 2 of 22\\nfirst subprocess of our approach is to identify the location of the mentioned contents; this\\nsubprocess is called entity recognition (NER) in NLP [ 10–13].\\nThere are single‑word place names, such as Beijing, Shanghai, Zhejiang, etc. There are\\nalso long place names composed of multiple words, such as Ejin Jinqi Saihantaolai Sumu\\nTownship (Inner Mongolia Autonomous Region); however, most of the place names are', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='1~5 words in length. The distribution of characters used in Chinese place names is also\\nrelatively scattered, but there are relatively concentrated features of place names within a\\ncertain range. For example, there are 3685 characters used in the gazetteer of China, and\\nthe specific frequency of use is relatively scattered. However, some of these words and\\ntheir word combinations only appear in toponyms, such as “Gacha” in Inner Mongolia,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='while most of them are common words with strong word formation ability and often ap‑\\npear in nontoponymic words, such as “Suqian” in “Su” and “Qian”. Based on the above\\npoints, Chinese place names can be roughly divided into simple and complex names. Sim‑\\nple names refer to those with short lengths (1~5 words) and common characters, such as\\nBeijing, Beijing Municipality, Hetao Plain, Hongyashan Reservoir, etc. Complex names re‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='fer to those with long lengths (more than five words) or with characters and words that are\\nmore remote. Therefore, it is technically difficult to identify Chinese place names accurately,\\nand it has become an important research direction in the field of geographic information.\\nThe existing methods for recognizing geographical names can be divided into three\\ntypes of methods: rule‑based methods, statistical methods, and machine learning meth‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='ods [ 14,15]. Rule‑based methods refer to the manual summarization of various word for‑\\nmations and syntactic rules and recognition by rule matching methods. This method is\\nmore intuitive, easy to understand and extend, and works better and faster for small‑scale\\ncorpus testing. However, the design of rules relies on professional language knowledge\\nand domain knowledge, is time‑consuming to compile, is difficult to cover comprehen‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='sively, and has poor portability and robustness. The statistical‑based approach does not\\nrequire specialized language knowledge, is more robust and flexible than the rule‑based\\napproach, and is highly portable, but the system does not express language determinism\\nwell and requires a large‑scale, more comprehensive manually annotated training corpus.\\nWith the accumulation of big data and the continuous enhancement of computer perfor‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='mance, deep‑learning‑based place‑name‑extraction methods have been developed rapidly.\\nDeep learning models are application‑friendly and robust and can automatically learn and\\nextract key features from text, achieving remarkable results in Chinese place‑name recog‑\\nnition. The most commonly used model is bidirectional long short‑term memory (LSTM),\\nwhich is based on the evolution of recurrent neural networks (RNNs) and overcomes the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='shortcomings of RNNs in long‑dependent sentences. The two‑way LSTM uses two LSTM\\nhidden layers with opposite directions to further solve the problem that sequence tagging\\ncan only use information from above and not below. In the current research on Chinese\\nplace‑name recognition, the main problem focuses on the recognition of complex Chinese\\nplace names. Since the length of complex place names is usually long and the use of words', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='and word collocation is relatively small, the above models often have difficulty determin‑\\ning the boundaries of place names.\\nIn this paper, we propose a hybrid neural network model for Chinese place‑name\\nrecognition based on a bidirectional encoder of the lite bidirectional encoder representa‑\\ntions from transformers (ALBERTs) model for word vector extraction to improve the text\\nvector representation ability and effectively identify irregular place names and place‑name', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='abbreviations. The bidirectional long‑ and short‑term memory (BiLSTM) neural network\\nlayer captures the semantic information in both directions in the sentence to better deter‑\\nmine the entity boundaries. The global optimal token sequence is obtained by the condi‑\\ntional random field (CRF) layer.\\nThe main contributions of this research are listed as follows:\\n(1) TPCNER, a large self‑annotated corpus of geographic domains with seven cate‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='gories and 64,063 labeled samples, was gathered and built. This corpus has more entity\\ncategories and larger sample sizes than the preceding corpora. The efficiency of the TPC‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 3 of 22\\nNER highlighted in this study was further demonstrated by the assessment experimental\\nfindings in Section 5.4.\\n(2) A novel Chinese NER (CNER) model for the geographic domain via the improved\\nALBERT pretraining model and BiLSTM–CRF was proposed. By learning word‑level fea‑\\nture representation through the ALBERT layer and extracting text contextual semantic fea‑\\ntures through the BiLSTM layer, the CRF layer obtains the global optimal token sequence', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='and finally improves the overall performance of the proposed model.\\n(3) The performance of ALBERT–BiLSTM–CRF was evaluated by using a range of\\nstandard models on TPCNER, MSRA, RenMinRiBao, and Boson. Furthermore, several\\nspecific details were studied and debated, such as the efficacy of BiLSTM and the use of\\nthe CRF mechanism. Through thorough comparisons with other advanced models, com‑\\nprehensive experimental findings on domain‑specific and generic datasets confirmed the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='proposed model’s effective performance.\\nThe rest of the paper is organized as follows: The current work on named‑entity\\nrecognition is described in Section 2. The processing of corpus creation is described in\\nSection 3. The recommended procedure for identifying geographic entities is presented\\nand described in Section 4. Section 5contains the experimental data and results. The con‑\\nclusion and recommendations for further study are presented in Section 6.\\n2. Related Work', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='2. Related Work\\nToponym information contains spatial location information, so toponym recognition\\ncan be applied to emergency‑disaster reduction, public‑opinion monitoring, urban plan‑\\nning, and other fields [ 16–18]. For example, information such as place names and nat‑\\nural disasters occurring there can be extracted from social media messages released by\\nthe public, such as Twitter. Social media messages can also assist in the monitoring and', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='management of public opinion. Managers can identify social public‑security events from\\nwebpages and social media texts and extract key spatial location information, realize the\\nmonitoring and early warning of social public security incidents, and improve the effi‑\\nciency of social and public security time disposal. To protect public privacy, social media\\nwill selectively hide the specific user location obtained in real time, which makes the extrac‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='tion of geographic location information more complicated. In June 2019, Twitter officially\\nremoved the precise geotagging feature. This change may reduce the geographic informa‑\\ntion contained in tweets, complicate location judgment, and make the task of recognizing\\nand geolocating locations from tweet content more urgent when dealing with emergen‑\\ncies [ 15]. We mainly identify the long text toponymic information in more detail to make', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='its characteristics more obvious and facilitate the development of subsequent tasks.\\nThe recognition methods of Chinese toponyms are mainly divided into three types,\\nnamely dictionary and rule‑based methods, statistical‑based machine learning methods,\\nand deep learning methods [ 19–21]. The rule‑based method mainly carries out place‑name\\nmatching and recognition by manually summarizing various word‑formation rules (the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='defects of this method have been summarized in Section 1), which can be combined with\\nthe current popular deep learning methods to supplement professional vocabulary and\\nimprove the robustness of the training model. The number of toponyms will increase at\\nan extremely fast rate with the development of the region, and frequently the same location\\nis represented by multiple toponyms. Therefore, the construction of a definitive gazetteer', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='cannot be achieved, and automatic place‑name recognition is still worth studying.\\nThe method based on statistics is more flexible than the rule method, which trans‑\\nforms place‑name recognition into a serialization annotation problem, but this method\\ndepends on the selection of feature templates and has poor generalization ability. Com‑\\nmon machine learning algorithms include the hidden Markov model (HMM), maximum\\nentropy Markov model (MEMM), and CRF model. Among them, the CRF model can im‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='plement effective feature‑selection and feature‑induction algorithms for sequence‑labeling\\ntasks. That is, users can evaluate the effect of automatically generated features on data\\nabstraction. Therefore, combining a CRF model with subsequent large‑scale pretraining', metadata={'source': 'pdfs/107. NER.pdf', 'page': 2}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 4 of 22\\nmodels, such as the BERT–CRF model, can achieve excellent sequence labeling results, thus\\nproviding ideas for more accurate place‑name recognition.\\nThe named‑entity identification approach based on neural networks is extensively uti‑\\nlized in text information extraction in numerous sectors, thanks to the rapid growth of deep\\nlearning. Different from traditional machine learning algorithms, the model trained by a', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='deep neural network has the characteristics of end‑to‑end data input and output. It makes\\nthe model training process more capable of reducing artificial interference to directly com‑\\nplete specific tasks according to the original data input, and there is no need to manually\\nset data characteristics [ 22]. The RNN is especially good at processing sequence data. The\\nevolved LSTM, BiLSTM, and bidirectional gated recurrent unit (BiGRU) networks often', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='combine the CRF layer to realize the task of named entity recognition. The more common\\nBiLSTM–CRF model, which combines the advantages of BiLSTM and CRF, not only can\\nretain the context information to process long text but also fully use sentence‑level tag in‑\\nformation thanks to a CRF layer. Similarly, the BiGRU–CRF model is widely used.\\nIn recent years, large‑scale pretraining models have rapidly become the preferred', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='method of natural language processing due to their outstanding performance. On this ba‑\\nsis, downstream task processing can make the recognition results of the hybrid model more\\naccurate [ 23,24]. Common pretraining models include the generative pretrained trans‑\\nformer (GPT), BERT, enhanced representation through knowledge integration (ERNIE),\\netc. The current popular BERT model works from the encoder of the bidirectional trans‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='former model [ 25,26]. We choose the BERT‑wwm model as the pretraining model, which is\\ntrained by the Research Center for Social Computing and Information Retrieval and iFLY‑\\nTEK AI Research in China. It is an open‑source Chinese pretraining language model, using\\nwhole‑word masking technology, which can better realize the task of Chinese place‑name\\nrecognition. On the basis of BERT‑wwm, BERT‑wwm‑ext expands the pretraining dataset', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='and increases the number of training iterations during model training.\\nToponym recognition is a subtask of named entity recognition, which belongs to the\\ninformation extraction task. We want to extract place‑name information from long text\\ndata, but the model trained by the general corpus is still lacking in the accuracy and gran‑\\nularity of toponym recognition. In response to the above problems, we used a Chinese cor‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='pus containing only toponym annotations and designed a hybrid neural network to train\\nthe model to obtain a model that performs better in the task of Chinese toponym recogni‑\\ntion. This model improves upon the general effect and low granularity of the traditional\\nnamed‑entity recognition model in toponym recognition.\\n3. Corpus Preparation and Annotation\\nTo address the limited Chinese NER corpus, a new corpus, TPCNER, was collected', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='and constructed. The dataset was further extended with entity categories based on earlier\\nstudies and eventually contained 7 entity categories and 64,063 annotated samples.\\n3.1. NER Tag Sets\\nNamed entities in the geographic domain, such as organizations, water systems, and\\nlandforms, are very different from those in the general domain. Geographic entities require\\na large amount of domain‑specific knowledge, thus making annotation difficult to a certain', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='extent. In this paper, based on the existing research [ 24], the entity categories are further\\ndivided into more granular entities, such as residential land and facilities, landforms, and\\nwater systems. Some categories were also considered, such as transportation, pipelines,\\nboundaries, and political areas with other regions. Finally, as indicated in Table 1, seven\\nfine‑grained groups emerge. To guarantee the integrity of the CNER categories, we prede‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='fine the category “others” in this work to characterize some conceptual and uncertain en‑\\ntities for later growth. Furthermore, this article exclusively considers entity types that are\\nrelevant to the geographic domain (e.g., toponym and organization), rather than generic\\nentities such as individuals (e.g., personal name). In the future, the corpus will be released\\nas well. Tables 2–4show the details of Boson, MSRA, and RenMinRiBao.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 3}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 5 of 22\\nTable 1. Details of entity categories in TPCNER.\\nID Entity Tags Abbreviation Description Example\\n1 Water System WATA manmade building or natural structure\\nassociated with water in nature.Tongji Canal, Huaihe\\nRiver Basin\\n2Residential land\\nand facilitiesRLFA place where human beings live or engage in\\nproductive life.Shaanxi Kiln\\n3 Transportation TRA Human‑built buildings related to transportation. Longxia Railway', metadata={'source': 'pdfs/107. NER.pdf', 'page': 4}),\n",
              " Document(page_content='4 Pipelines PIP Pipelines laid by humans. Natural gas pipeline\\n5Boundaries,\\nRegions, and\\nOther AreasBROThe corresponding boundaries that humans have\\ndrawn on the land to facilitate management.Hubei Province\\n6 Landforms LAN Includes natural and artificial landforms. Himalayas\\n7 Organization ORG Includes the names of relevant organizations.Wuhan Zhongdi\\nDigital Technology Co.\\nTable 2. Details of entity categories in Boson.\\nID Entity Tags Abbreviation Description Example', metadata={'source': 'pdfs/107. NER.pdf', 'page': 4}),\n",
              " Document(page_content='1 Location LOC A spatial distribution, location, or place occupied. China\\n2 Org_name ORG Includes the names of relevant organizations.Wuhan Zhongdi\\nDigital Technology Co.\\nTable 3. Details of entity categories in MSRA.\\nID Entity Tags Abbreviation Description Example\\n1 NS NS A spatial distribution, location, or place occupied. Yufeng Mountain\\n2 NT NT Includes the names of relevant organizations.China University of\\nGeosciences\\nTable 4. Details of entity categories in RenMinRiBao.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 4}),\n",
              " Document(page_content='ID Entity Tags Abbreviation Description Example\\n1 NS NS A spatial distribution, location, or place occupied. Hubei\\n2 NT NT Includes the names of relevant organizations.China University of\\nGeosciences\\n3.2. Corpus Collection and Annotation\\nIn this paper, a large‑scale annotated corpus, TPCNER, is established with the Baidu\\nEncyclopedia and the Chinese Encyclopedia of Chinese Geography as source data (approx‑\\nimately 2 million words) and with reference to the geographically named entity annotation', metadata={'source': 'pdfs/107. NER.pdf', 'page': 4}),\n",
              " Document(page_content='system designed in this paper.\\nTo ensure consistency and accuracy, the work in this paper is presented in two main ar‑\\neas. First, a new TPCNER annotation tool, ChineseNERAnno, was developed (see Figure 1).\\nThe complete process of this tool is depicted in Figure 1. The suggested technique employs\\na lexicon of terms linked to the geographic domain for automated annotation, which helps\\nto ensure entity consistency. Second, new entities can be dynamically extended into the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 4}),\n",
              " Document(page_content='dictionary, thus reducing the manual annotation time and increasing the annotation speed.\\nA new TPCNER corpus was finally constructed, consisting of 7 categories, 650,725 entities,\\nand 64,063 samples preprocessed and annotated. This procedure took three months to com‑\\nplete under the supervision of domain experts. Table 5shows certain TPCNER examples\\nin further detail. Figure 2shows data on the statistical distribution of individual entities in', metadata={'source': 'pdfs/107. NER.pdf', 'page': 4}),\n",
              " Document(page_content='TPCNER, demonstrating that the training set’s distribution is similar to the validation set’s\\ndistribution. The logic and utility of the corpus designated in this work are also argued in\\nSection 5.4.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 4}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 6 of 22\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  6 of 22  \\n To ensure consisten cy and accuracy, the work in this paper is presented in two main \\nareas. First, a new TPCNER annotation tool, ChineseNERAnno, was developed  (see Fig-\\nure 1) . The complete process of this tool is depicted in Figure 1. The suggested technique \\nemploys a lexicon of terms linked to the geographic domain for automated annotation,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='which helps to ensure entity consistency. Second, new entities can be dynamically ex-\\ntended into the dictionary, thus reducing the manual annotation time and increasing the \\nannotation speed.  A new TPCNER corpus was finally constructed, consisting of 7 catego-\\nries, 650,725 entities, and 64,063 samples preprocessed and annotated. This procedure \\ntook three months to complete under the supervision of domain experts. Table 5 shows', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='certain TPCNER examples in further detail. Figure 2 shows data on the statistical distri-\\nbution of individual entities in TPCNER, demonstrating that the training set ’s distribution \\nis similar to the validation set ’s distribution. The logic and utility of the corpus designated \\nin this work are also argued in Section 5.4.  \\nRaw DataAuto \\nAnnotationManual \\nValidationExportLabeled\\nData\\nFormats LexiconBIO\\nBIOES\\nBMES \\n  \\n   \\n \\nUpdate\\n \\nFigure  1. Overall workflow of ChineseNERAnno.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='Table  5. Some samples of TPCNER.  \\nSentence(en):  Anji County is a county in Huzhou City, Zhejiang Province, a famous bamboo producing area \\nin China and one of the key forestry counties in Zhejiang Province.  \\nLabel(en):  Anji County; Zhejiang Province; Huzhou City; China; Zhejiang Province  \\nSentence(en):  The Bailong  River is a tributary of the upper reaches of the Jialing River in the Yangtze River', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='system, and is a geographically important dividing line in China, along with the Qinling and \\nHuai Rivers.  \\nLabel(en):  Bailong River; Yangtze River; Jialing River; Qinling  River; Huai River; China  \\n3.3. Analysis of Corpus Features  \\nGeographic entity names are the most important distinction between geographic en-\\ntities and in the field of geographic information  (see Table 6) . Entity names with location', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='information, mainly composed of basic geographic information elements, can be seen with \\nambiguity and diversity. In this paper, we analyze the descriptive features of geographic \\nentities in texts by studying relevant national stan dards and the literature. We then inte-\\ngrate geographically named entity categories and descriptive features by manual collation \\nand data fusion with national standards as the benchmark. The distribution for each cat-\\negory in TPCNER is shown in Figure 2.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='The descriptions of geographic entity names in Chinese texts are characterized by \\nvagueness, uncertainty, and diversity. In this paper, we analyze the descriptive features \\nof geographic entity names in texts. The five main descriptive features are summarized as \\nfollows : \\n(1) The names of geographic entities are diverse, with free and scattered words or \\nphrases, but with relatively concentrated coverage, for example, the name of the commu-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='nity “Daijiashanzhuang”, within which there are “Daijiashanzhuang Phase 1” and “Dai-\\njiashanzhuang Phase 2”.  \\nFigure 1. Overall workflow of ChineseNERAnno.\\nTable 5. Some samples of TPCNER.\\nSentence(en):Anji County is a county in Huzhou City, Zhejiang Province, a famous bamboo producing area in\\nChina and one of the key forestry counties in Zhejiang Province.\\nLabel(en): Anji County; Zhejiang Province; Huzhou City; China; Zhejiang Province', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='Sentence(en):The Bailong River is a tributary of the upper reaches of the Jialing River in the Yangtze River system,\\nand is a geographically important dividing line in China, along with the Qinling and Huai Rivers.\\nLabel(en): Bailong River; Yangtze River; Jialing River; Qinling River; Huai River; China\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  7 of 22  \\n (2) The names of geographical entities have a certain pattern, often ending with char-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='acteristic words , such as “province, road, mountain”, for example, “Hubei Province” and \\n“Luma Road, Hongshan District, Hubei Province”.  \\n(3) The names of geographical entities are often followed by location words; for ex-\\nample, “Huangshan” is a place name, and “Huangshan North” is a complete geographical \\nnaming entity.  \\n(4) Most of the names of geographical entities are in the form of nouns, but  sometimes', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='they are used as modifiers to modify other entities, such as “Bagong Mountain Tofu”.  \\n(5) The names of geographical entities are named and unnamed ; that is,  some geo-\\ngraphical entities do not have specific names, and their spatial locations need t o be deter-\\nmined through the contextual relationship. For example, the “swimming pool” in the \\n“swimming pool in the west area of the university” is the name of a geographical entity,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='but its spatial location needs to be determined by the previous informatio n. \\n \\nFigure  2. Distribution for each category in TPCNER.  \\nTable 6. Comparative information between TPCNER and other datasets.  \\nDatasets  Examples  Classes  Size  Entity Size  Max Length  Min Length  Avg Length  \\nBoson  Obama [person_name]  also wel-\\ncomed Cameron [person_name]  us-\\ning a number of authentic \\nBritish [location]  vernaculars,  6 1.78 M  3417  36 1 18.5 \\nMSRA  the scope of the survey in-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='volved the Forbidden City [loca-\\ntion], the Museum of History [lo-\\ncation ], the Institute of Ancient \\nResearch [organization_name] , the Pe-\\nking University and Tsinghua \\nLibrary [location] , the Beitu [location ], \\nthe Japanese archives and \\nmore than twenty others.  3 10.4 M  80,884  40 1 20.5 \\nRenMinRiBao  New Year Concert in Bei-\\njing [location ] 3 10.1 M  12,718 35 1 18 \\nTPCNER  The sample examples are \\nlisted in Table 2.  7 7.32 M  64,063 18 2 10', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='Figure 2. Distribution for each category in TPCNER.\\n3.3. Analysis of Corpus Features\\nGeographic entity names are the most important distinction between geographic en‑\\ntities and in the field of geographic information (see Table 6). Entity names with location\\ninformation, mainly composed of basic geographic information elements, can be seen with\\nambiguity and diversity. In this paper, we analyze the descriptive features of geographic', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='entities in texts by studying relevant national standards and the literature. We then inte‑\\ngrate geographically named entity categories and descriptive features by manual collation\\nand data fusion with national standards as the benchmark. The distribution for each cate‑\\ngory in TPCNER is shown in Figure 2.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 5}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 7 of 22\\nTable 6. Comparative information between TPCNER and other datasets.\\nDatasets Examples Classes Size Entity Size Max Length Min Length Avg Length\\nBosonObama [person_name] also welcomed\\nCameron [person_name] using a number of\\nauthentic British [location] vernaculars,6 1.78 M 3417 36 1 18.5\\nMSRAthe scope of the survey involved the\\nForbidden City [location] , the Museum of\\nHistory [location] , the Institute of Ancient', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='Research [organization_name] , the Peking\\nUniversity and Tsinghua Library [location] ,\\nthe Beitu [location] , the Japanese archives and\\nmore than twenty others.3 10.4 M 80,884 40 1 20.5\\nRenMinRiBao New Year Concert in Beijing [location] 3 10.1 M 12,718 35 1 18\\nTPCNER The sample examples are listed in Table 2. 7 7.32 M 64,063 18 2 10\\nThe descriptions of geographic entity names in Chinese texts are characterized by', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='vagueness, uncertainty, and diversity. In this paper, we analyze the descriptive features\\nof geographic entity names in texts. The five main descriptive features are summarized as\\nfollows:\\n(1) The names of geographic entities are diverse, with free and scattered words or\\nphrases, but with relatively concentrated coverage, for example, the name of the commu‑\\nnity “Daijiashanzhuang”, within which there are “Daijiashanzhuang Phase 1” and “Daiji‑\\nashanzhuang Phase 2”.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='(2) The names of geographical entities have a certain pattern, often ending with char‑\\nacteristic words, such as “province, road, mountain”, for example, “Hubei Province” and\\n“Luma Road, Hongshan District, Hubei Province”.\\n(3) The names of geographical entities are often followed by location words; for ex‑\\nample, “Huangshan” is a place name, and “Huangshan North” is a complete geographical\\nnaming entity.\\n(4) Most of the names of geographical entities are in the form of nouns, but sometimes', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='they are used as modifiers to modify other entities, such as “Bagong Mountain Tofu”.\\n(5) The names of geographical entities are named and unnamed; that is, some geo‑\\ngraphical entities do not have specific names, and their spatial locations need to be de‑\\ntermined through the contextual relationship. For example, the “swimming pool” in the\\n“swimming pool in the west area of the university” is the name of a geographical entity,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='but its spatial location needs to be determined by the previous information.\\n4. The Hybrid Deep Learning Model\\n4.1. Overall Framework and Workflow of the Model\\nIn this paper, we propose a hybrid neural network model for Chinese place‑name\\nrecognition. The overall structure of the model is shown in Figure 3, and the whole model\\nis divided into five parts: the input layer, ALBERT layer, BiLSTM layer, CRF layer, and\\noutput layer.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='output layer.\\nWe present our model from bottom to top, characterizing the layers of the neural net‑\\nwork. The input layer contains the individual words of a message which are used as the\\ninput to the model.\\nThe next layer represents each word as vectors, using a pretraining approach. It uses\\npretrained word embeddings to represent the words in the input sequence. In particular,\\nwe use ALBERT, which captures the different semantics of a word under varied contexts.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='Note that the pretrained word embeddings capture the semantics of words based on their\\ntypical usage contexts and therefore provide static representations of words; by contrast,\\nALBERT provides a dynamic representation for a word by modeling the particular sen‑\\ntence within which the word is used. This layer captures four different aspects of a word,\\nand their representation vectors are concatenated together into a large vector to represent', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='each input word. These vectors are then used as the input to next layer, which is a BiLSTM', metadata={'source': 'pdfs/107. NER.pdf', 'page': 6}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 8 of 22\\nlayer consisting of two layers of LSTM cells: one forward layer capturing information be‑\\nfore the target word and one backward layer capturing information after the target word.\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  8 of 22  \\n 4. The Hybrid Deep Learning Model  \\n4.1. Overall Framework and Workflow of the Model  \\nIn this paper, we propose a hybrid neural network model for Chinese place -name', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='recognition. The overall structure of the model is shown in Figure 3, and the whole model \\nis divided into five parts: the input layer, ALBERT layer, BiLSTM layer, CRF layer, and \\noutput layer.  \\n \\nFigure  3. The overall architecture of the proposed model.  \\nWe present our model from bottom to top, characterizing the layers of the neural \\nnetwork. The input layer contains the individual words of a message which are used as \\nthe input to the model.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='The next layer represents each word as vectors , using a pretraining approach. It uses \\npretrained word embeddings to represent the words in the input sequence. In particular, \\nwe use ALBERT, which captures the different semantics of a word under varied contexts. \\nNote that the pretrained word embeddings capture the semantics of words based on their \\ntypical usage contexts and therefore provide static representations of words; by contrast,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='ALBERT provides a dynamic representation for a word by modeling the particular sen-\\ntence within which the word is used. This layer c aptures four different aspects of a word, \\nand their representation vectors are concatenated together into a large vector to represent \\neach input word. These vectors are then used as the input to next layer, which is a BiLSTM \\nlayer consisting of two layers of LSTM cells: one forward layer capturing information be-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='fore the target word and one backward layer capturing information after the target word.  \\nThe BiLSTM layer  combines the outputs of the two LSTM layers and feeds the com-\\nbined output into a fully conne cted layer. Then the next layer is a CRF layer , which takes \\nthe output from the fully connected layer and performs sequence labeling. The CRF layer \\nuses the standard BIEO model from NER research to label each word but focuses on loca-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='tions. Thus, a word is annotated as either “B –L” (the beginning of a location phrase), “I –\\nL” (inside a location phrase), “E –L” (end a location phrase), or “O” (outside a location \\nphrase).  \\nThe workflow of the model is as follows:  \\n(1) First, the dataset is composed of text X  (X1, X2, …, Xn), which is input to the AL-\\nBERT layer, where Xi denotes the i-th word in the input text.  \\nFigure 3. The overall architecture of the proposed model.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='The BiLSTM layer combines the outputs of the two LSTM layers and feeds the com‑\\nbined output into a fully connected layer. Then the next layer is a CRF layer, which takes\\nthe output from the fully connected layer and performs sequence labeling. The CRF layer\\nuses the standard BIEO model from NER research to label each word but focuses on lo‑\\ncations. Thus, a word is annotated as either “B–L” (the beginning of a location phrase),', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='“I–L” (inside a location phrase), “E–L” (end a location phrase), or “O” (outside a location\\nphrase).\\nThe workflow of the model is as follows:\\n(1) First, the dataset is composed of text X (X 1, X2,. . ., Xn), which is input to the\\nALBERT layer, where Xidenotes the i‑th word in the input text.\\n(2) The input text data are serialized in the ALBERT layer, and the model generates\\nfeature vectors, Ci, based on each word, Xi, in the text to enhance the text vector repre‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='sentation and transforms Ciinto word vectors, E = ( E1,E2,. . .,En), with location features\\nbased on Transformer (Trm) in the word vector representation layer of ALBERT.\\n(3) Using Eias the input of each time step of the bidirectional LSTM layer and perform‑\\ning feature calculation, the forward LSTM F= (F1,F2,. . .,Fn) and the reverse\\nLSTM B = ( B1,B2,. . .,Bn) of the BiLSTM layer are used to extract the contextual features', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='and generate the feature matrix, H= (H1,H2,. . .,Hn), by position splicing to capture the\\nsemantic information in both directions in the sentence.\\n(4) Consider the transfer features between annotations in the CRF layer, obtain the de‑\\npendencies between adjacent labels, and output the corresponding labels Y(Y1,Y2,. . .,Yn)\\nto obtain the final annotation results.\\n4.2. BERT and ALBERT Pretraining Models\\nThe pretraining model provides a better initialization parameter for the neural net‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='work, accelerates the convergence of the neural network, and provides better generaliza‑\\ntion ability on the target task. The development of pretraining models is divided into two\\nstages: shallow word embedding and deep coding. The shallow word embedding mod‑\\nels mainly use the current word and previous word information for training; they only\\nconsider the local information of the text and fail to effectively use the overall information', metadata={'source': 'pdfs/107. NER.pdf', 'page': 7}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 9 of 22\\nof the text [ 22,27]. BERT uses a bidirectional transformer network structure with stronger\\nepistemic capability to train the corpus and achieve a deep bidirectional representation\\nfor pretraining [ 25]. The BERT model’s “masked language model” (MLM) can fuse the\\nleft and right contexts of the current word. BERT has achieved remarkable results in tasks\\nsuch as named‑entity recognition [ 28], text classification, machine translation [ 29], etc. The', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='next sentence prediction (NSP) captures sentence‑level representations and obtains seman‑\\ntically rich, high‑quality feature representation vectors.\\nHowever, the BERT model contains hundreds of millions of parameters, and the model\\ntraining is easily limited by hardware memory. The ALBERT model is a lightweight pre‑\\ntrained language model that is based on the BERT model [ 30]. The BERT model uses a\\nbidirectional transformer encoder to obtain the feature representation of text, and its model', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='structure is shown in Figure 4. ALBERT has only 10% of the number of parameters of the\\noriginal BERT model but retains the accuracy of the BERT model.\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  9 of 22  \\n (2) The input text data are serialized in the ALBERT layer, and the model generates \\nfeature vectors , Ci, based on each word , Xi, in the text to enhance the te xt vector represen-\\ntation and transforms Ci into word vectors , E = ( E1, E2, …, En), with location features based', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='on Transformer  (Trm) in the word vector representation layer of ALBERT.  \\n(3) Using Ei as the input of each time step of the bidirectional LSTM layer and per-\\nforming feature calculation, the forward LSTM F = (F1, F2, …, Fn) and the reverse LSTM B \\n= (B1, B2, …, Bn) of the BiLSTM layer are used to extract the contextual features and gener-\\nate t he feature matrix , H = (H1, H2, …, Hn), by position splicing to capture the semantic \\ninformation in both directions in the sentence.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='(4) Consider the transfer features between annotations in the CRF layer, obtain the \\ndependencies between adjacent labels, a nd output the corresponding labels Y (Y1, Y2, …, \\nYn) to obtain the final annotation results.  \\n4.2. BERT and ALBERT Pretraining Models  \\nThe pretraining model provides a better initialization parameter for the neural net-\\nwork, accelerates the convergence of the neural network, and provides better generaliza-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='tion ability on the target task. The development of pretraining models is divided into  two \\nstages: shallow word embedding and deep coding. The shallow word embedding models \\nmainly use the current word and previous word information for training ; they  only con-\\nsider the local information of the text and fail to effectively use the overall info rmation of \\nthe text [22,27]. BERT uses a bidirectional transformer network structure with stronger', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='epistemic capability to train the corpus and achieve a deep bidirectional representation \\nfor pretraining [25]. The BERT model’s “masked language model” (MLM)  can fuse the left \\nand right contexts of the current word. BERT has achieved remarkable results in tasks \\nsuch as named -entity recognition [28], text classification, machine translation [29], etc. The \\nnext sentence prediction (NSP) captures sentence -level r epresentations and obtains se-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='mantically rich, high -quality feature representation vectors.  \\nHowever, the BERT model contains hundreds of millions of parameters, and the \\nmodel training is easily limited by hardware memory. The ALBERT model is a light-\\nweight p retrained language model that is based on the BERT model [30]. The BERT model \\nuses a bidirectional transformer encoder to obtain the feature representation of text, and \\nits model structure is shown in Figure 4. ALBERT has only 10% of the number of param-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='eters of the original BERT model but retains the accuracy of the BERT model.  \\n \\nFigure 4 . Basic structure of the ALBERT model.  \\nThe transformer structure of the BERT model is composed of an encoder and de-\\ncoder . The encoder part mainly consists of six identical layers, and each layer consists of \\ntwo sub -layers, the multi -head self -attention mechanism and the fully connected feed -\\nFigure 4. Basic structure of the ALBERT model.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='The transformer structure of the BERT model is composed of an encoder and decoder.\\nThe encoder part mainly consists of six identical layers, and each layer consists of two\\nsub‑layers, the multi‑head self‑attention mechanism and the fully connected feed‑forward\\nnetwork, respectively. Since each sub‑layer is added with residual connection and normal‑\\nization, the output of the sub‑layer can be represented as shown in the following equation:\\nsub_layer _output =LayerNorm (x+(SubLayer (x))) (1)', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='The multi‑head self‑attention mechanism projects the three matrices, namely Q,V, and\\nK, by hdifferent linear transformations and finally splices the different attention results.\\nThe main calculation equation is shown below:\\nattention _output =Attention (Q,K,V) (2)\\nMultiHead =Concat (head 1, . . . , head h)WO(3)\\nhead i=Attention\\x10\\nQW iQ,KW iK,VW iV\\x11\\n(4)\\nFor the decoder part, the basic structure is similar to the encoder part, but with the\\naddition of a sub‑layer of attention.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='ALBERT uses two methods to reduce the number of parameters: (i) factorized em‑\\nbedding parameterization, which separates the size of the hidden layer from the size of\\nthe lexical embedding matrix by decomposing the huge lexical embedding matrix into\\ntwo smaller matrices; and (ii) cross‑layer parameter sharing, which significantly reduces', metadata={'source': 'pdfs/107. NER.pdf', 'page': 8}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 10 of 22\\nthe number of parameters of the model by sharing the parameters of the neural layer of\\nthe model without significantly affecting its performance.\\nIn the figure, C= (C1,C2,. . .,Cn) indicates that each character in the sequence is\\ntrained by a multilayer bidirectional transformer (Trm) encoder to finally obtain the feature\\nvector of the text, denoted as E= (E1,E2,. . .,En). After the input text is first processed by', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='word embedding, the positional information encoding (positional encoding) of each word\\nin that sentence is added. The model learns more text features by combining multiple self‑\\nattentive layers to form multi‑head attention. The output of the multi‑head attention‑based\\nlayer is passed through the Add&Nom layer, where “ Add” means adding the input and\\noutput of the multi‑head attention layer, and “Norm” means normalization. The result,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='after passing through the Add&Nom layer, is passed to the feed‑forward neural layer (Feed\\nForward) and outputted by the Add&Norm layer.\\nThe ALBERT used in this paper has several design features that enhance its perfor‑\\nmance on the task of toponym recognition from social media messages. First, our pre‑\\nsented ALBERT uses the pretrained word embeddings that are specifically derived from\\nsocial media messages. We performed the following steps on the basis of the collected', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='text data: (1) cleaning the data—we removed the messy codes and incomplete sentences\\nto ensure that the sentences were smooth; (2) cutting the sentences—we added [CLS],\\n[SEP], [MASK], etc., to each text item to obtain 25.6 GB of training data; and (3) training\\ncorpus—we trained on 3090 GPU for 4 days, with the epoch set to 100,000 and learning\\nrate set to 5 ×10−5.\\nWe used the GloVe word embeddings (the number of tokens is 54,238, and the dictio‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='nary size is 399 KB) that were trained on 2 billion texts, with 11 billion tokens and 1.8 million\\nvocabulary items collected from Baidu Encyclopedia, Weibo, WeChat, etc. These word em‑\\nbeddings, specifically trained on a large social‑media‑messages corpus, include many ver‑\\nnacular words and unregistered words used by people in social media messages. Previous\\ngeoparsing and NER models typically use word embeddings trained on well‑formatted', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='text, such as news articles, and many vernacular words are not covered by those embed‑\\ndings. When that happens, an embedding for a generic unknown token is usually used\\nto represent this vernacular word and, as a result, the actual semantics of the word are\\nlost. Second, compared with the basic BiLSTM–CRF model, our presented model adds an\\nALBERT layer to capture the dynamic and contextualized semantics of words.\\n4.3. BiLSTM Layer', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='4.3. BiLSTM Layer\\nRecurrent neural networks are more suitable for sequence annotation tasks due to\\ntheir ability to remember the historical information of text sequences. An LSTM model was\\nproposed in the literature [ 31–34] that incorporates specially designed memory units in the\\nhidden layer compared to RNNs and can better solve the problem of gradient explosion\\nor gradient disappearance that RNNs tend to have as the sequence length increases. The', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='neuron structure of the LSTM model is shown in Figure 5.\\nThe LSTM network consists of three gate structures and one state unit; these gate\\nstructures include input gates, oblivion gates, and output gates. The input gate determines\\nhow much of the input to the network is saved to the cell state at the current moment. The\\nforgetting gate selectively discards certain information. The output gate determines the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='final output value based on the cell state. The long‑term dependency problem of recurrent\\nneural networks can be better solved by the three‑gate structure to maintain and update\\nthe state for long‑term memory function. A typical LSTM network structure can be repre‑\\nsented formally in Equations (5)–(10):\\nit=σ(Wi·[ht−1,xt]+bi) (5)\\nft=σ\\x10\\nWf·[ht−1,xt]+bf\\x11\\n(6)\\not=σ(Wo·[ht−1,xt]+bo) (7)', metadata={'source': 'pdfs/107. NER.pdf', 'page': 9}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 11 of 22\\neCt=tanh(Wc·[ht−1,xt]+bC) (8)\\nCt=ft⊗Ct−1+it⊗eCt (9)\\nht=ot⊗tan h (Ct) (10)\\nwhere xtrepresents the input word at moment t;itrepresents the memory gate; ftrepre‑\\nsents the forget gate; otrepresents the output gate; Ctrepresents the cell state; eCtrepresents\\nthe temporary cell state; htrepresents the hidden state output at each time step; ht−1repre‑\\nsents the hidden state at the previous moment; Ct−1represents the cell state at the previous', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='moment; Wi,Wf,Wo, and Wcrepresent the weight matrix at the current state; and bi,bf,bo,\\nand bCdenote the offset of the current state, respectively.\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  11 of 22  \\n Recurrent neural networks are more suitable for sequence annotation tasks due to \\ntheir ability to remember the historical information of text sequences. A n LSTM model  \\nwas proposed in the literature [31–34] that incorporates specially designed memory units', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='in the hidden layer compared to RNNs and can better solve the problem of gradient ex-\\nplosion or gradient disappearance that RNNs tend to have as the sequence length in-\\ncreases. The neuron structure of the LSTM model is shown in Figure 5.  \\n \\nFigure  5. Neuron structure of LSTM.  \\nThe LSTM network consists of three gate structures and one state unit; these gate \\nstructures include input gates, oblivion gates, and output gates. The input gate determines', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='how much of the input to the network is saved to the cell state at the current mom ent. The \\nforgetting gate selectively discards certain information. The output gate determines the \\nfinal output value based on the cell state. The long -term dependency problem of recurrent \\nneural networks can be better solved by the three -gate structure to maintain and update \\nthe state for long -term memory function. A typical LSTM network structure can be repre-\\nsented formally in Equation s (5)–(10):', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='𝑖𝑡=𝜎(𝑊𝑖⋅[ℎ𝑡−1，𝑥𝑡]+𝑏𝑖) (5) \\n𝑓𝑡=𝜎(𝑊𝑓⋅[ℎ𝑡−1，𝑥𝑡]+𝑏𝑓) (6) \\n𝑜𝑡=𝜎(𝑊𝑜⋅[ℎ𝑡−1，𝑥𝑡]+𝑏𝑜) (7) \\n𝐶̃𝑡=tanℎ(𝑊𝑐⋅[ℎ𝑡−1,𝑥𝑡]+𝑏𝐶) (8) \\n𝐶𝑡=𝑓𝑡⨂𝐶𝑡−1+𝑖𝑡⨂𝐶̃𝑡 (9) \\nℎ𝑡=𝑜𝑡⨂𝑡𝑎𝑛 ℎ(𝐶𝑡) (10) \\nwhere xt represents the input word at moment t; it represents the memory gate ; ft repre-\\nsents the forget gate ; ot represents the output gate ; Ct represents the cell state ; 𝐶̃𝑡 repre-\\nsents the temporary cell state ; ht represents the hidden state output at each time step ; ht−1', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='represents the hidden state at the previous moment ; Ct−1 represents the cell state at the \\nprevious moment ; Wi, Wf, Wo, and Wc represent the weight matrix at the current state ; and \\nbi, bf, bo, and bC denote the offset of the current state, respectively.  \\n4.4. CRF Layer  \\nThe conditional random field model is a discriminative probabilistic model [ 34]. The \\nconditional random field m odel combines the advantages of the HMM and maximum \\nFigure 5. Neuron structure of LSTM.\\n4.4. CRF Layer', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='4.4. CRF Layer\\nThe conditional random field model is a discriminative probabilistic model [ 34]. The\\nconditional random field model combines the advantages of the HMM and maximum en‑\\ntropy model (MEM). It addresses the strict independence assumption condition of the hid‑\\nden Markov model, avoids the disadvantages of the local optimum and labeling bias prob‑\\nlem of the maximum entropy model, is suitable for the labeling of sequence data CRF,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='considers the sequential problem among labels, and obtains the global optimal labeling\\nsequence through the relationship of adjacent labels, adding constraints to the final pre‑\\ndicted labels. For example, a tag starting with “B” is not followed by an “O” class tag, and\\na tag starting with “E” cannot be sequentially connected with tag “I” sequence. Assuming\\nthat the model input, x= (x1,x2,. . .,xn), has a sequence of tags, y= (y1,y2,. . .,yn), the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='score vector of the sentence can be calculated by Equation (7):\\nscore (x,y)=n\\n∑\\nj=1Pi,yi+n\\n∑\\nj=0Ayi,yi+1(11)\\nwhere Pi,yiis the probability of the yilabel of the character, and A is the transfer proba‑\\nbility matrix. The CRF score vector is normalized and trained by using the log‑likelihood\\nfunction as the loss function, as shown in Equation (8):\\nlg(P(y|x)) = score (x,y)−lg(∑\\ny′∈Yxexp\\x00\\nscore\\x00\\nx,y′\\x01\\x01) (12)\\nIn the prediction phase, the network model is labeled by using the Viterbi algorithm', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='to obtain the optimal sequence, as shown in Equation (9):\\ny∗=arg max score\\x00\\nx,y′\\x01\\ny′∈Yx (13)', metadata={'source': 'pdfs/107. NER.pdf', 'page': 10}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 12 of 22\\n5. Results and Discussion\\nOn several datasets, the proposed model’s performance was compared to that of other\\ndeep learning models. Many parts of the experimental data were evaluated and debated. Ten‑\\nsorFlow was used to implement the models on a single NVIDIA GeForce RTX 3090 GPU. Due\\nto the nature of replication, these results may change somewhat from the originals.\\n5.1. Dataset, Evaluation Metrics, and Hyperparameters', metadata={'source': 'pdfs/107. NER.pdf', 'page': 11}),\n",
              " Document(page_content='Performance measures : The experiment’s measurements were precision (P), recall\\n(R), and the F1‑score (F1). Precision measures the percentage of correctly identified to‑\\nponyms (true positives, TPs) among all the toponyms recognized by a model, which in‑\\nclude both true positives and false positives (FPs). Recall measures the percentage of cor‑\\nrectly identified toponyms among all the toponyms that are annotated as ground truth,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 11}),\n",
              " Document(page_content='which include true positives and false negatives (FNs). The F‑score is the harmonic mean\\nof precision and recall [ 35]. It is high when both precision and recall are fairly high, and it\\nis low if either of the two is low [ 36].\\nTraining process : In this study, we trained word embeddings on a Wikipedia corpus\\nby using the word2vec tool in advance, and we concatenated consecutive words to repre‑\\nsent an entity when the entity had multiple words. More importantly, the word embed‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 11}),\n",
              " Document(page_content='dings obtained by word2vec were used as the initial representation of words. We treated\\nthem as parameters and modified them in the training process, which can provide a better\\nrepresentation of words.\\nTesting methodology : We used 10‑fold cross‑validation for testing and reported the\\naverage score of 10 independent runs. This resulted in a total of 100 different splits into\\ntraining/testing subsets.\\nEach Chinese character was treated as a token, and TPCNER was coded by using the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 11}),\n",
              " Document(page_content='BIO tagging technique. To avoid overfitting, the dropout was adjusted to 0.5. Due to the\\nlikelihood of contextual reliance between neighboring phrases, the maximum length of the\\nsamples was considered to be the maximum training length, and we noted that dividing\\nthe samples might result in semantic loss. To assist the convergence of all models, the\\nnumber of epochs was fixed to 100. At a ratio of 8:2, all datasets were randomly partitioned', metadata={'source': 'pdfs/107. NER.pdf', 'page': 11}),\n",
              " Document(page_content='into training and validation sets. Table 7compares TPCNER to other datasets and provides\\ncomparative data. Table 8lists the remaining hyperparameters.\\nTable 7. Comparative information between TPCNER and other datasets.\\nDataset Classes Size Entity Size Max Length Min Length Avg Length\\nBoson 6 1827 kb 3417 36 1 19\\nMSRA 3 7.92 MB 19,871 47 1 24\\nRenMinRiBao 3 10,421 kb 12,718 35 1 18\\nTPCNER 7 7.32 MB 64,063 18 2 10\\nTable 8. ALBERT model parameter.\\nNo. Parameter Value\\n1 Hidden size 768', metadata={'source': 'pdfs/107. NER.pdf', 'page': 11}),\n",
              " Document(page_content='1 Hidden size 768\\n2 Embedding size 128\\n3 Max position embeddings 512\\n4 No. of attention heads 12\\n5 No. of hidden layers 12\\n5.2. Baselines\\nTo evaluate the effect of our presented model, we empirically compared our method\\n(ALBERT–BiLSTM–CRF) with six strong baselines (DBN, DM_NLP, NeuroTPR, Chese‑\\nBERTTP, ChineseTR, and GazPNE2). In order to guarantee a relatively fair comparison,', metadata={'source': 'pdfs/107. NER.pdf', 'page': 11}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 13 of 22\\nfor these baselines, we employed their publicly released source codes and followed the\\nparameter settings reported in their papers.\\n• DBN is an adapted toponym recognition approach based on deep belief network\\n(DBN) by exploring two key issues: word representation and model interpretation\\nproposed by [ 37].\\n• DM_NLP is a general model based on BiLSTM–CRF, proposed by [ 38].', metadata={'source': 'pdfs/107. NER.pdf', 'page': 12}),\n",
              " Document(page_content='• NeuroTPR is a Neuro‑net ToPonym Recognition model designed specifically with\\nthese linguistic irregularities in mind, proposed by [ 39].\\n• ChineseBERTTP is a deep neural network named BERT–BiLSTM–CRF, which extends\\na basic bidirectional recurrent neural network model (BiLSTM) with the pretraining\\nbidirectional encoder representation from transformers (BERT) representation to han‑\\ndle the toponym recognition task in Chinese text [ 40].', metadata={'source': 'pdfs/107. NER.pdf', 'page': 12}),\n",
              " Document(page_content='• ChineseTR is a weakly supervised Chinese toponym recognition architecture that\\nleverages a training dataset creator that generates training datasets automatically based\\non word collections and associated word frequencies from various texts and an exten‑\\nsion recognizer that employs a basic bidirectional recurrent neural network based on\\nparticular features designed for toponym recognition proposed by [ 41].\\n• GazPNE2 is a general approach for extracting place names from tweets, named GazPNE2.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 12}),\n",
              " Document(page_content='It combines global gazetteers (i.e., OpenStreetMap and GeoNames), deep learning,\\nand pretrained transformer models (i.e., BERT and BERTweet), which require no man‑\\nually annotated data [ 42].\\n5.3. Experiments on TPCNER\\nIn this study, the HMM, CRF, BiLSTM–CRF, IDCNN–CRF, IDCNN–CRF2, BiLSTM\\n–Attention–CRF, BERT–BiLSTM–CRF, BERT–BiGRU–CRF, ALBERT–BiLSTM, ALBERT old\\n–BiLSTM–CRF (original ALBERT), and ALBERT ours–BiLSTM–CRF (our presented ALBERT)', metadata={'source': 'pdfs/107. NER.pdf', 'page': 12}),\n",
              " Document(page_content='models were used to test the TPCNER dataset, and the performance of named‑entity recog‑\\nnition was evaluated by four indices: accuracy, precision, recall, and F1‑score. The experi‑\\nmental results are shown in Table 9. The following results can be observed:\\nTable 9. Results of different models on TPCNER.\\nModel Accuracy Precision Recall F1‑Score\\nHMM 80.9% −0.16% 80.4% 81.5% 80.7%\\nCRF 83.8% + 0.03% 83.8% 84.1% 84%\\nBiLSTM–CRF 86.1% −0.02% 97.9% 76.6% 86.0%\\nIDCNN–CRF 86.5% + 0.11% 97.9% 77.1% 86.2%', metadata={'source': 'pdfs/107. NER.pdf', 'page': 12}),\n",
              " Document(page_content='IDCNN–CRF2 88.2% + 0.25% 98.0% 79.5% 87.8%\\nBiLSTM–Attention–CRF 89.1% −0.09% 97.5% 72.8% 83.4%\\nBERT–BiLSTM–CRF 91.1% −0.08% 92.8% 91.5% 92.1%\\nBERT–BiGRU–CRF 93.4% −0.28% 93.9% 94.9% 94.4%\\nALBERT old–BiLSTM 88.1% + 0.12% 91.2% 90.7% 90.9%\\nALBERT ours–BiLSTM 92.7% + 0.17% 94.1% 94.5% 94.3%\\nALBERT old–BiLSTM–CRF 90.5% + 0.03% 92.5% 94.4% 93.4%\\nALBERT ours–BiLSTM–CRF 97.8% + 0.07% 96.1% 96.2% 96.1%\\n(1) Compared with the non‑neural‑network models (i.e., HMM and CRF), neural net‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 12}),\n",
              " Document(page_content='work models improve the performance significantly, as the performance of the former de‑\\nteriorates quickly, while the latter can maintain a reasonable performance. This is due to\\nthe fact that most of the features used in non‑neural‑network models come from human‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 12}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 14 of 22\\ndesigned features, which suffer from accumulated errors that may lead to performance\\ndegradation.\\n(2) We can see that these eleven models achieved a good performance on the TPC‑\\nNER dataset, and their accuracy, precision, recall, and F1‑scores frequently exceeded 80%.\\nAmong them, the ALBERT ours–BiLSTM–CRF model has the best test effect, and its accu‑\\nracy, precision, recall, and F1‑score are 97.8%, 96.1%, 96.2%, and 96.1%, respectively. Com‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='pared with the other nine models, this model has a better named‑entity recognition effect\\non the TPCNER dataset. In particular, our re‑trained ALBERT model improved by 7.8%\\ncompared to the original ALBERT model.\\n(3) In addition, IDCNN–CRF2 achieved a better performance than IDCNN–CRF, and\\nIDCNN–CRF and BiLSTM–CRF obtained almost the same performance; both of these re‑\\nsults indicate that IDCNN utilizes dilated convolution to speed up training and does not', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='enhance sequence features to improve performance.\\nWe continued our experiments by comparing ALBERT–BiLSTM–CRF with six deep‑\\nlearning‑based models. The performance of these models on the TPCNER dataset is re‑\\nported in Table 10. We made the following observations:\\nTable 10. Comparison with previous works on TPCNER.\\nModel Precision Recall F1‑Score\\nDBN 0.781 0.774 0.78\\nDM_NLP 0.838 0.841 0.84\\nNeuroTPR 0.871 0.872 0.87\\nChineseBERTTP 0.89 0.894 0.89\\nChineseTR 0.85 0.86 0.85\\nGazPNE2 0.835 0.849 0.84', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='ALBERT ours–BiLSTM–CRF 0.961 0.962 0.961\\n(1) ALBERT–BiLSTM–CRF yields the highest precision with the same recall. More‑\\nover, ALBERT–BiLSTM–CRF obtains a constant and substantial improvement over Chi‑\\nneseBERTTP, which currently has the best results reported on this dataset, with higher\\nprecision for the same recall. We believe that the combination of specially designed AL‑\\nBERT features constitutes more significant features and promotes the extractor to make\\naccurate predictions.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='(2) Compared with the basic BiLSTM–CRF model, ALBERT–BiLSTM–CRF performs\\nbetter in all three metrics, thus demonstrating the value of our improved designs, includ‑\\ning the specially designed ALBERT layers. Compared with DM_NLP and NeuroTPR,\\nALBERT–BiLSTM–CRF shows higher precision, a higher F1‑score, and similar recall.\\nAs expected from Tables 6and 7, for all datasets, ALBERT ours–BiLSTM–CRF achieves\\nthe best F1‑score, i.e., 96.1%. Compared with two weakly supervised deep‑learning mod‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='els (NeuroTPR and GazPNE2), our presented model performs better in all three metrics,\\nthus demonstrating the value of our improved design, which contains a fine‑tuned AL‑\\nBERT. The reason is that Chinese texts often include a considerable number of location\\nnames, which may not be covered by the basic BERT, including many vernacular words\\n(e.g., “Mengliang Mountains” and “Plateau”) and abbreviations (e.g., “Dida” and “CUG”)', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='applied by people. When this happens, generic unknown token embedding is usually used\\nto represent the vernacular word, and the actual semantics of the word is lost.\\n5.4. Experiments on the Public Dataset\\nTo better verify the performance of the TPCNER dataset and model, this paper also\\nuses the BiLSTM–CRF, IDCNN–CRF, IDCNN–CRF2, BiLSTM–Attention–CRF, BERT\\n–BiLSTM–CRF, BERT–BiGRU–CRF, ALBERT–BiLSTM, and ALBERT–BiLSTM–CRF mod‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='els to test on the Boson dataset, MSRA dataset, and RenMinRiBao dataset and uses preci‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 13}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 15 of 22\\nsion, recall, and F1‑scores to evaluate the named entity recognition performance. The ex‑\\nperimental results are shown in Table 11. The experimental results show that these eight\\nmodels achieve good results on these three datasets. Compared with the other seven mod‑\\nels, the ALBERT–BiLSTM–CRF model has the best performance. Its precision, recall, and\\nF1‑score of the three datasets are higher than those of the other models. Compared with', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='the three public datasets, the named entity recognition effect of the TPCNER dataset is\\nbasically the same, reaching more than 95%. In addition, this paper also counts and visual‑\\nizes the F1‑score of the BERT–BiLSTM–CRF, BERT–BiGRU–CRF, ALBERT–BiLSTM, and\\nALBERT–BiLSTM–CRF models after hyperparameter tuning, as shown in Figure 6. It can\\nbe clearly seen from the figure that the F1‑score of the ALBERT–BiLSTM–CRF model is\\nsignificantly higher than that of the other four models.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='Table 11. Results of models on the Boson, MSRA, and RenMinRiBao datasets.\\nModelsBoson MSRA RenMinRiBao\\nPrecision Recall F1‑Score Precision Recall F1‑Score Precision Recall F1‑Score\\nBiLSTM–CRF 0.887 0.791 0.836 0.901 0.859 0.876 0.922 0.915 0.918\\nIDCNN–CRF 0.891 0.809 0.848 0.979 0.777 0.866 0.931 0.933 0.932\\nIDCNN–CRF2 0.912 0.909 0.910 0.980 0.771 0.863 0.934 0.941 0.937\\nBiLSTM–Attention–CRF 0.922 0.917 0.919 0.973 0.765 0.841 0.953 0.960 0.956', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='BERT–BiLSTM–CRF 0.932 0.933 0.932 0.974 0.813 0.886 0.961 0.965 0.963\\nBERT–BiGRU–CRF 0.941 0.945 0.943 0.979 0.822 0.894 0.976 0.971 0.973\\nALBERT our–BiLSTM 0.951 0.956 0.953 0.981 0.881 0.928 0.981 0.979 0.980\\nALBERT our–BiLSTM–CRF 0.961 0.962 0.961 0.989 0.895 0.940 0.976 0.986 0.981\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  16 of 22  \\n  \\n \\nFigure  6. The F1 -scores of each model after hyperparameter tuning.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='A total of 36 controlled experiments were performed  to determine the best number \\nof labeled phrases from the created dataset and to analyze the size of the labeled dataset. \\nThe first trial used 1000 sentences, whereas the remaining tests used a range of 20 00 to \\n9000 sentences (with a step size of 1000). Figure 7 depicts the experimental outcomes in \\nterms of average accuracy and recall.  \\n  \\nFigure  7. Average F1 -score of the presented and baseline NER algorithms with different sizes of', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='labeled data.  \\nAs seen in Figure 7, when 9000 sentences were used, the proposed algorithm \\nachieved an average F1 -score of 96.2%. The BERT –BiLSTM–CRF, BERT –BiGRU–CRF, and \\nALBERT –BiLSTM (the baseline) only achieved F1 -scores of 92.1%, 94.4%, and 95.3%, re-\\nspectively. This shows that the proposed NER algorithm outperforms the baseline.  \\n5.5. Ablation Analysis  \\nTo verify the effectiveness of pretraining on our approach, we design the following', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='variant models and conduct experiments on the constructed dataset ( see Table 12). \\nTable 12. Experimental performance of variant models on the TPCNER dataset.  \\nModel  Precision  Recall  F1-Score  \\nFigure 6. The F1‑scores of each model after hyperparameter tuning.\\nA total of 36 controlled experiments were performed to determine the best number\\nof labeled phrases from the created dataset and to analyze the size of the labeled dataset.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='The first trial used 1000 sentences, whereas the remaining tests used a range of 2000 to\\n9000 sentences (with a step size of 1000). Figure 7depicts the experimental outcomes in\\nterms of average accuracy and recall.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 14}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 16 of 22\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  16 of 22  \\n  \\n \\nFigure  6. The F1 -scores of each model after hyperparameter tuning.  \\nA total of 36 controlled experiments were performed  to determine the best number \\nof labeled phrases from the created dataset and to analyze the size of the labeled dataset. \\nThe first trial used 1000 sentences, whereas the remaining tests used a range of 20 00 to', metadata={'source': 'pdfs/107. NER.pdf', 'page': 15}),\n",
              " Document(page_content='9000 sentences (with a step size of 1000). Figure 7 depicts the experimental outcomes in \\nterms of average accuracy and recall.  \\n  \\nFigure  7. Average F1 -score of the presented and baseline NER algorithms with different sizes of \\nlabeled data.  \\nAs seen in Figure 7, when 9000 sentences were used, the proposed algorithm \\nachieved an average F1 -score of 96.2%. The BERT –BiLSTM–CRF, BERT –BiGRU–CRF, and \\nALBERT –BiLSTM (the baseline) only achieved F1 -scores of 92.1%, 94.4%, and 95.3%, re-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 15}),\n",
              " Document(page_content='spectively. This shows that the proposed NER algorithm outperforms the baseline.  \\n5.5. Ablation Analysis  \\nTo verify the effectiveness of pretraining on our approach, we design the following \\nvariant models and conduct experiments on the constructed dataset ( see Table 12). \\nTable 12. Experimental performance of variant models on the TPCNER dataset.  \\nModel  Precision  Recall  F1-Score  \\nFigure 7. Average F1‑score of the presented and baseline NER algorithms with different sizes of', metadata={'source': 'pdfs/107. NER.pdf', 'page': 15}),\n",
              " Document(page_content='labeled data.\\nAs seen in Figure 7, when 9000 sentences were used, the proposed algorithm achieved\\nan average F1‑score of 96.2%. The BERT–BiLSTM–CRF, BERT–BiGRU–CRF, and ALBERT\\n–BiLSTM (the baseline) only achieved F1‑scores of 92.1%, 94.4%, and 95.3%, respectively.\\nThis shows that the proposed NER algorithm outperforms the baseline.\\n5.5. Ablation Analysis\\nTo verify the effectiveness of pretraining on our approach, we design the following', metadata={'source': 'pdfs/107. NER.pdf', 'page': 15}),\n",
              " Document(page_content='variant models and conduct experiments on the constructed dataset (see Table 12).\\nTable 12. Experimental performance of variant models on the TPCNER dataset.\\nModel Precision Recall F1‑Score\\nBiLSTM–CRF 0.979 0.766 0.860\\n+BERT 0.928 0.915 0.921\\n+ALBERT old 0.925 0.844 0.883\\n+ALBERT our 0.961 0.962 0.961\\nTable 9show the experimental results of BiLSTM–CRF as a baseline method. In Table 9,\\nthe performance of BiLSTM–CRF in all evaluation metrics is poor, compared with other', metadata={'source': 'pdfs/107. NER.pdf', 'page': 15}),\n",
              " Document(page_content='models. Moreover, from the overall model F1‑score in Table 9, we found that the use of\\nthe BERT layer or the use of ALBERT layer is higher than the baseline method. This phe‑\\nnomenon shows the effectiveness of the combination of pretraining model.\\nCompared with BiLSTM–CRF, the F1 value of the model can be improved by using\\npretraining model (BERT) in Table 9. The reason may be that pretraining enables better\\ncharacterization of text sequence features. This phenomenon shows the effectiveness of', metadata={'source': 'pdfs/107. NER.pdf', 'page': 15}),\n",
              " Document(page_content='using pretraining model.\\nCompared with Bi‑LSTM–CRF, the model using spatial attention has improved in\\nregard to the P, R, and F1‑score in Table 9. The reason may be that domain pretrained\\nmodels can better characterize geographic text features and then improves the extraction\\nability of BiLSTM encoding features. This phenomenon shows the effectiveness of using\\ngeographic pretraining model.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 15}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 17 of 22\\n5.6. Discussion\\n5.6.1. Ablation Study\\nWe focused on analyzing the constructed dataset (TPCNER). We examined an exam‑\\nple from the TPCNER corpus to see if the provided model could better detect items in the\\ngeographic domain. In this example, the entity “Gulou Hospital of Harbin Engineering\\nUniversity” appeared just twice in the training set. The entity “Gulou Hospital of Harbin', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='Engineering University” is recognized by the BERT–BiLSTM–CRF model as two entities,\\n“Harbin Engineering University” and “Gulou Hospital”, as shown in Table 13. Because\\nthese two items are more abundant in the training set, recognition without augmentation\\ninformation will be deceptive. Because of inaccurate boundary information, the BERT–\\nBiLSTM–CRF model wrongly classifies “Gulou Hospital of Harbin Engineering Univer‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='sity” as an entity. Because more extensive augmentation information is incorporated into\\nour suggested model, it provides accurate predictions. Furthermore, the terms “Harbin En‑\\ngineering University” and “Gulou Hospital” in the sample are similar, implying a tighter\\nrelationship between the entity’s characteristics.\\nTable 13. Results of an instance being predicted by different models. B represents begin, I represents\\ninside, E represents end, and O represents other.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='Original sentence 黑龙江中部出现强降雨，其中哈尔滨工程大学古楼医院周边伴有冰雹。\\nSentence translation Heavy rainfall in central Heilongjiang, including hail in Harbin Yilan County.\\nSentence pinyin (Chinese\\nromanization)Hei Long Jiang Zhong Bu Chu Xian Qiang Jiang Yu, Qi Zhong Ha Er Bin Gong Cheng\\nDa Xue Gu Lou Yi Yuan Zhou Bian Ban You Bing Bao.\\nCorrect LabelHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/I–L gu/I–L', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='lou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\nbHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/E–L gu/B–L\\nlou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\nBERT–BiGRU–CRF predictHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/E–L gu/B–L', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='lou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\nALBERT ours–BiLSTM predictHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O\\nyu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/E–L gu/B–L\\nlou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\nALBERT ours–BiLSTM–CRF predictHei/B–L long/I–L jiang/E–L zhong/O bu/O di/O qu/O chu/O xian/O qiang/O jiang/O', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='yu/O, /O qi/O zhong/ha/B–L er/I–L bin/I–L gong/I–L cheng/I–L da/I–L xue/I–L gu/I–L\\nlou/I–L yi/I–L yuan/E–L zhou/O bian/O ban/O you/O bing/O bao/O. /O\\n5.6.2. Error Analysis\\nWe chose many sentences from the testing set and assessed their sample mistakes to\\nevaluate the real output of different models. Figure 8shows the recognition results of the\\nBERT–BiLSTM–CRF, BERT–BiGRU–CRF, ALBERT–BiLSTM, and ALBERT–BiLSTM–CRF\\nmodels in sample texts, where the red characters denote errors.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='As demonstrated in Figure 8, our model outperformed the others in terms of recog‑\\nnition, whereas the BERT–BiLSTM–CRF model failed to distinguish nested entities. For\\ninstance, the ALBERT–BiLSTM–CRF recognized “Yang Xinhe” as an entity in Case 1, but\\nother models cannot recognize this entity because it is a place name consisting of a per‑\\nson’s name, and many algorithms will recognize the person’s name. In Case 2, the BERT', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='–BiLSTM–CRF identifies “Horqin” as an entity, but the related characters “Right Wing\\nFront Banner Debs Town” placed at a long distance in the context were missed. The\\nALBERT –BiLSTM–CRF model successfully identified the fine‑grained nested entities “Deebs\\nTownship, Horqin Right Wing Front Banner” in Case 2.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 16}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 18 of 22\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  18 of 22  \\n ALBERT ours–BiLSTM pre-\\ndict Hei/B–L long /I–L jiang /E–L zhong /O bu/O di/O qu/O chu/O xian /O qiang /O jiang /O yu/O, \\n/O qi/O zhong /ha/B–L er/I–L bin/I–L gong /I–L cheng /I–L da/I–L xue/E–L gu/B–L lou/I–L \\nyi/I–L yuan /E–L zhou /O bian /O ban/O you/O bing /O bao/O. /O \\nALBERT ours–BiLSTM–CRF', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='predict  Hei/B–L long /I–L jiang /E–L zhong /O bu/O di/O qu/O chu/O xian /O qiang /O jiang /O yu/O, \\n/O qi/O zhong /ha/B–L er/I–L bin/I–L gong /I–L cheng /I–L da/I–L xue/I–L gu/I–L lou/I–L \\nyi/I–L yuan /E–L zhou /O bian /O ban/O you/O bing /O bao/O. /O \\n5.6.2. Error Analysis  \\nWe chose many sentences from the testing set and assessed their sample mistakes to \\nevaluate the real output of different models. Figure 8 shows the recognition results of the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='BERT–BiLSTM–CRF, BERT –BiGRU–CRF, ALBERT –BiLSTM, and ALBERT –BiLSTM–CRF \\nmodels in sample texts, where the red characters denote erro rs. \\n \\nFigure  8. Error analysis of some typical cases. Blue represents standard place -name labeling, and \\nred represents model identification place names.  The translation of the sentence “杨信河以北的李庆\\n县发生了特大暴雨 ” is “Very heavy rainfall occurred in Liqing County north of Yang Xin River ”; the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='translation of the sentence “风雹灾害致科尔沁右翼前旗德伯斯镇作物倒伏 ” is “Wind and hail disas-\\nter caused the Khorqin Right Wing Front Banner Debs town crop collapse ”. \\nAs demonstrated in Figure 8, our model outperformed the others in  terms of recog-\\nnition, whereas the BERT –BiLSTM–CRF model failed to distinguish nested entities. For \\ninstance, the ALBERT –BiLSTM–CRF recognized “Yang Xinhe” as an entity in Case 1, but \\nother models cannot recognize this entity because it is a place name con sisting of a per-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='son’s name, and many algorithms will recognize the person’s name. In Case 2, the BERT –\\nBiLSTM–CRF identifies “Horqin” as an entity, but the related characters “Right Wing \\nFront Banner Debs Town” placed at a long distance in the context were missed. The AL-\\nBERT–BiLSTM–CRF model successfully identified the fine -grained nested entities “Deebs \\nTownship, Horqin Right Wing Front Banner” in Case 2.  \\nBy analyzing the recognition results, we found that (1) the reason for affecting the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='accuracy of the mo del is that some of the names in the data contain toponymic words, \\nresulting in incorrect recall, e.g., “Yang Xinhe” and “Li Qingxian”; (2) the reason for the \\nlow recall is that some of the complex names are not correctly recognized, e.g., “Deebs \\nTown of H orqin Right -wing Front Banner” is not correctly recognized. For example, in \\n“wind and hail disaster caused crop collapse in Debs town of horqin right -wing front ban-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='ner”, “Debs town of horqin right -wing front banner” was not correctly identified; in the \\nface of rain and flood, Qiongzhong Li and Miao autonomous county urgently relocated. \\nThe name “Qiongzhong Li and Miao Autonomous County” was not correctly identified \\nin “35 people”. The reason is that the place name is long, the frequency of occurrence in \\nthe corpus is low, and the model does not learn enough, so it is not correctly recalled.  \\n5.6.3. Annotated Quality Analysis', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='Figure 8. Error analysis of some typical cases. Blue represents standard place‑name label‑\\ning, and red represents model identification place names. The translation of the sentence “\\n杨信河以北的李庆县发生了特大暴雨 ” is “Very heavy rainfall occurred in Liqing County north of\\nYang Xin River”; the translation of the sentence “ 风雹灾害致科尔沁右翼前旗德伯斯镇作物倒伏 ” is\\n“Wind and hail disaster caused the Khorqin Right Wing Front Banner Debs town crop collapse”.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='By analyzing the recognition results, we found that (1) the reason for affecting the\\naccuracy of the model is that some of the names in the data contain toponymic words,\\nresulting in incorrect recall, e.g., “Yang Xinhe” and “Li Qingxian”; (2) the reason for the low\\nrecall is that some of the complex names are not correctly recognized, e.g., “Deebs Town\\nof Horqin Right‑wing Front Banner” is not correctly recognized. For example, in “wind', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='and hail disaster caused crop collapse in Debs town of horqin right‑wing front banner”,\\n“Debs town of horqin right‑wing front banner” was not correctly identified; in the face\\nof rain and flood, Qiongzhong Li and Miao autonomous county urgently relocated. The\\nname “Qiongzhong Li and Miao Autonomous County” was not correctly identified in “35\\npeople”. The reason is that the place name is long, the frequency of occurrence in the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='corpus is low, and the model does not learn enough, so it is not correctly recalled.\\n5.6.3. Annotated Quality Analysis\\nBiLSTM–CRF and IDCNN are considered the most basic models and were used to assess\\nthe quality of the annotated corpus, using hierarchical 10‑fold cross‑validation [ 36,37].At the\\nmacro level, the detailed experimental results presented in Table 6show that BiLSTM–CRF\\nand IDCNN achieve F1‑scores of 86% and 87%, respectively. At the micro level, BiLSTM', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='–CRF and IDCNN show excellent performance for traffic, water systems, and organization,\\nthus indicating the ease of identification of these categories. In particular, for organiza‑\\ntional agencies, the F1‑score of both models is 92.16% and 93.79%, respectively. Due to\\ndiscrepancies created by the absence of boundary characteristics and the mixed usage of\\ncharacters, digits, and letters, some things, such as extremely specified place names, mixed', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='place names, and merged place names, are difficult to recognize. Figure 2demonstrated\\nhow the lack of data for some categories has an impact on performance. In addition, as\\nindicated in Figure 5, we mentioned several forecast mistakes. Overall, the assessment\\nfindings show that the corpus annotated in this study is reliable and may be utilized to\\nrecognize geographic domain entities.\\nThe confusion matrix in Figures 9and 10shows the number of toponyms that were', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='extracted from the dataset by using the proposed algorithm, as well as the number of\\ngold‑standard annotations, for each toponym class. Figure 10shows that the proposed\\nalgorithm has a relatively lower precision for the TRA toponym classes. This could be\\nattributed to data imbalance. The imbalance in entity number causes the algorithm to\\nfocus on minimizing classification errors for the entities with a larger number, while insuf‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='ficiently considering the errors for the entities with a smaller number.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 17}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 19 of 22\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  19 of 22  \\n BiLSTM–CRF and IDCNN are considered the most basic models and were used to \\nassess the quality of the annotated corpus , using hierarchical 10 -fold cross -validation \\n[36,37]. At the macro level, the detailed experimental results presented in Table 6 show \\nthat BiLSTM –CRF and IDCNN achieve F1 -scores of 86% and 87%, respectively. At the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='micro level, BiLSTM –CRF and IDCNN show ex cellent performance for traffic, water sys-\\ntems, and organization, thus  indicat ing the ease of identification of these categories. In \\nparticular, for organizational agencies, the F1 -score of both models is 92.16% and 93.79%, \\nrespectively. Due to discrepanci es created by the absence of boundary characteristics and \\nthe mixed usage of characters, digits, and letters, some things, such as extremely specified', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='place names, mixed place names, and merged place names, are difficult to recognize. Fig-\\nure 2 demonstrate d how the lack of data for some categories has an impact on perfor-\\nmance. In addition, as indicated in Figure 5, we mention ed several forecast mistakes. \\nOverall, the assessment findings show that the corpus annotated in this study is reliable \\nand may be util ized to recognize geographic domain entities.  \\nThe confusion matrix in Figures 9 and 10 shows the number of toponyms that were', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='extracted from the dataset by using the proposed algorithm, as well as the number of gold -\\nstandard annotations, for each toponym c lass. Figure 10 shows that the proposed algo-\\nrithm has a relatively lower precision for the TRA toponym classes. This could be at-\\ntributed to data imbalance. The imbalance in entity number causes the algorithm to focus \\non minimizing classification errors for  the entities with a larger number, while insuffi-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='ciently considering the errors for the entities with a smaller number.  \\n  \\nFigure  9. Confusion matrix for all extracted and gold -standard toponym from the constructed da-\\ntaset based on ALBERT –BiLSTM–CRF. WAT  = water system; RLF  = residential land and facilities; \\nTRA  = transportation; PIP  = pipelines; BRO  = boundaries, regions, and other areas; LAN  = land-\\nforms; ORG  = organization.  \\nFigure 9. Confusion matrix for all extracted and gold‑standard toponym from the constructed', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='dataset based on ALBERT–BiLSTM–CRF. WAT = water system; RLF = residential land and facilities;\\nTRA = transportation; PIP = pipelines; BRO = boundaries, regions, and other areas; LAN = landforms;\\nORG = organization.\\nISPRS Int. J. Geo -Inf. 2022 , 11, x FOR PEER REVIEW  20 of 22  \\n   \\nFigure 10. Confusion matrix for all extracted and gold -standard toponym from the constructed da-\\ntaset based on ALBERT –BiLSTM–CRF. WAT  = water system; RLF  = residential land and facilities;', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='TRA  = transportation; PIP  = pipelines; BRO  = boundaries, regions, and other areas; LAN  = land-\\nforms; ORG  = organization.  \\n6. Conclusio ns and Future Work  \\nIn this paper, we propose a hybrid neural network method for Chinese place -name \\nrecognition that solves the above problems by learning word -level feature representations \\nin the ALBERT layer, extracting contextual semantic features in the BiLSTM layer, and', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='generating optimal label sequences in the CRF layer. The experimental results show that \\nthe prop osed toponym recognition method has good performance in all evaluation indi-\\nces. We train ALBERT –BiLSTM–CRF by using a constructed human -annotated dataset \\nand three public datasets. We experimented with several training procedures and discov-\\nered that a mix of human -annotated data produces the greatest results. Evaluation exper-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='iments based on three test datasets, namely Boson, MSRA, and RenMinRiBao, demon-\\nstrate the improved performance of ALBERT –BiLSTM–CRF in comparison with a set of \\ndeep learning models. Thi s work attempted to serve as a resource for named -entity -recog-\\nnition studies in various geographic areas. We  will work on including more features and \\nmaking more sensible modifications to the weights of these features in the future.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='Author Contributions:   Conceptualization, Liufeng Tao and Qinjun Qiu; methodology,  Qinjun \\nQiu; validation, Dexin Xu and Shengyong Pan; formal analysis, Kai Ma; investigation, Liufeng Tao; \\nresources, Qinjun Qiu; data curation, Liufeng  Tao; writing —original draft preparation, Liufeng Tao; \\nwriting—review and editing, Qinjun Qiu; supervision, Qinjun Qiu; funding acquisition, Zhong Xie, \\nQinjun Qiu and Bo Huang. All authors have read and agreed to the published version of the man-', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='uscript.  \\nFunding:   This study was financially supported by the National Natural Science Foundation of \\nChina (42050101), Beijing Key Laboratory of Urban Spatial Information Engineering (No.20220108) , \\nthe China Postdoctoral Science Foundation (No.2021M702991), Wu han Multi -Element Urban Geo-\\nlogical Survey Demonstration Project (WHDYS -2021 -014), the Open Research Project of The Hubei \\nFigure 10. Confusion matrix for all extracted and gold‑standard toponym from the constructed', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='dataset based on ALBERT–BiLSTM–CRF. WAT = water system; RLF = residential land and facilities;\\nTRA = transportation; PIP = pipelines; BRO = boundaries, regions, and other areas; LAN = landforms;\\nORG = organization.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 18}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 20 of 22\\n6. Conclusions and Future Work\\nIn this paper, we propose a hybrid neural network method for Chinese place‑name\\nrecognition that solves the above problems by learning word‑level feature representations\\nin the ALBERT layer, extracting contextual semantic features in the BiLSTM layer, and gen‑\\nerating optimal label sequences in the CRF layer. The experimental results show that the', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='proposed toponym recognition method has good performance in all evaluation indices.\\nWe train ALBERT–BiLSTM–CRF by using a constructed human‑annotated dataset and\\nthree public datasets. We experimented with several training procedures and discovered\\nthat a mix of human‑annotated data produces the greatest results. Evaluation experiments\\nbased on three test datasets, namely Boson, MSRA, and RenMinRiBao, demonstrate the im‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='proved performance of ALBERT–BiLSTM–CRF in comparison with a set of deep learning\\nmodels. This work attempted to serve as a resource for named‑entity‑recognition studies\\nin various geographic areas. We will work on including more features and making more\\nsensible modifications to the weights of these features in the future.\\nAuthor Contributions: Conceptualization, Liufeng Tao and Qinjun Qiu; methodology, Qinjun Qiu; val‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='idation, Dexin Xu and Shengyong Pan; formal analysis, Kai Ma; investigation, Liufeng Tao; resources,\\nQinjun Qiu; data curation, Liufeng Tao; writing—original draft preparation, Liufeng Tao; writing—\\nreview and editing, Qinjun Qiu; supervision, Qinjun Qiu; funding acquisition, Zhong Xie, Qinjun Qiu\\nand Bo Huang. All authors have read and agreed to the published version of the manuscript.\\nFunding: This study was financially supported by the National Natural Science Foundation of China', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='(42050101), Beijing Key Laboratory of Urban Spatial Information Engineering (No.20220108), the\\nChina Postdoctoral Science Foundation (No.2021M702991), Wuhan Multi‑Element Urban Geologi‑\\ncal Survey Demonstration Project (WHDYS‑2021‑014), the Open Research Project of The Hubei Key\\nLaboratory of Intelligent Geo‑Information Processing (No. KLIGIP‑2021A01), and Wuhan Science\\nand Technology Plan Project (No.2020010602012022).\\nInstitutional Review Board Statement: Not applicable.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='Informed Consent Statement: Not applicable.\\nData Availability Statement: All original data and codes can be found in the Zenodo ( https://zenodo.\\norg/record/6482711#.YmZxWMjAiAc ) and accessed on 1 January 2022.\\nAcknowledgments: The authors thank the four anonymous reviewers for the positive, constructive,\\nand valuable comments and suggestions.\\nConflicts of Interest: The authors declare that they have no known competing financial interests or', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='personal relationships that could have appeared to influence the work reported in this paper.\\nReferences\\n1. Imran, M.; Castillo, C.; Diaz, F.; Vieweg, S. Processing social media messages in mass emergency: A survey. ACM Comput. Surv.\\n2015 ,47, 67. [ CrossRef ]\\n2. Silverman, L. Facebook, Twitter Replace 911 Calls for Stranded in Houston. 2017. Available online: https://www.npr.\\norg/sections/alltechconsidered/2017/08/28/546831780/texas‑police‑and‑residents‑turn‑to‑social‑media‑to‑communicateamid‑', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='harvey (accessed on 12 September 2017).\\n3. Yu, M.; Huang, Q.; Qin, H.; Scheele, C.; Yang, C. Deep learning for real‑time social media text classification for situation\\nawareness—Using hurricanes Sandy, Harvey, and Irma as case studies. Int. J. Digit. Earth 2019 ,12, 1230–1247. [ CrossRef ]\\n4. Hu, Y.; Mao, H.; McKenzie, G. A natural language processing and geospatial clustering framework for harvesting local place', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='names from geotagged housing advertisements. Int. J. Geogr. Inf. Sci. 2018 ,33, 714–738. [ CrossRef ]\\n5. Freire, N.; Borbinha, J.; Calado, P.; Martins, B. A metadata geoparsing system for place name recognition and resolution in\\nmetadata records. In Proceedings of the 11th International ACM/IEEE Joint Conference on Digital Libraries, Ottawa, ON, Canada,\\n13–17 June 2011; pp. 339–348.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='6. Gelernter, J.; Balaji, S. An algorithm for local geoparsing of microtext. Geoinformatica 2013 ,17, 635–667. [ CrossRef ]\\n7. Gritta, M.; Pilehvar, M.T.; Limsopatham, N.; Collier, N. What’s missing in geographical parsing? Lang. Resour. Eval. 2018 ,52,\\n603–623. [ CrossRef ]\\n8. Jones, C.B.; Purves, R.S. Geographical information retrieval. Int. J. Geogr. Inf. Sci. 2008 ,22, 219–228. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='9. Purves, R.S.; Clough, P.; Jones, C.B.; Hall, M.H.; Murdock, V. Geographic Information Retrieval: Progress and Challenges in\\nSpatial Search of Text. Found. Trends®Inf. Retr. 2018 ,12, 164–318. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 19}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 21 of 22\\n10. Derczynski, L.; Nichols, E.; Van Erp, M.; Limsopatham, N. Results of the WNUT2017 shared task on novel and emerging entity\\nrecognition. In Proceedings of the Third Workshop on Noisy User‑Generated Text, Copenhagen, Denmark, 7 September 2017;\\npp. 140–147.\\n11. Li, H.; Wang, M.; Baldwin, T.; Tomko, M.; Vasardani, M. UniMelb at SemEval‑2019 Task 12: Multi‑model combination for', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='toponym resolution. In Proceedings of the 13th International Workshop on Semantic Evaluation, Minneapolis, MN, USA, 6–7\\nJune 2019; ACL: Stroudsburg, PA, USA; pp. 1313–1318.\\n12. Qiu, Q.; Xie, Z.; Wu, L.; Tao, L.; Li, W. BiLSTM‑CRF for geological named entity recognition from the geoscience literature. Earth\\nSci. Inform. 2019 ,12, 565–579. [ CrossRef ]\\n13. Qiu, Q.; Xie, Z.; Wu, L.; Tao, L. GNER: A generative model for geological named entity recognition without labeled data using', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='deep learning. Earth Space Sci. 2019 ,6, 931–946. [ CrossRef ]\\n14. Santos, R.; Murrieta‑Flores, P.; Calado, P.; Martins, B. Toponym matching through deep neural networks. Int. J. Geogr. Inf. Sci.\\n2018 ,32, 324–348. [ CrossRef ]\\n15. Wang, J.; Hu, Y. Enhancing spatial and textual analysis with EUPEG: An extensible and unified platform for evaluating geop‑\\narsers. Trans. GIS 2019 ,23, 1393–1419. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='16. Herskovits, A. Language and Spatial Cognition: An interdisciplinary Study of Prepositions in English ; Cambridge University Press:\\nCambridge, UK, 1986.\\n17. Talmy, L. Toward a Cognitive Semantics: Concept Structuring Systems ; The MIT Press: Cambridge, MA, USA, 2000.\\n18. Stock, K.; Yousaf, J. Context‑aware automated interpretation of elaborate natural language descriptions of location through\\nlearning from empirical data. Int. J. Geogr. Inf. Sci. 2018 ,32, 1087–1116. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='19. Cohen, W.; Ravikumar, P.; Fienberg, S. A comparison of string distance metrics for namematching tasks. In Proceedings of KDD\\nWorkshop on Data Cleaning and Object Consolidation, Washington, DC, USA, 24–27 August 2003.\\n20. Moreau, E.; Yvon, F.; Capp, E.O. Robust similarity measures for named entities matching. In Proceedings of the International\\nConference on Computational Linguistics, Manchester, UK, 18–22 August 2008.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='21. Santos, R.; Murrieta‑Flores, P.; Martins, B. Learning to combine multiple string similarity metrics for effective toponym matching.\\nInt. J. Digit. Earth 2018 ,11, 913–938. [ CrossRef ]\\n22. Ma, K.; Tan, Y.; Tian, M.; Xie, X.; Qiu, Q.; Li, S.; Wang, X. Extraction of temporal information from social media messages using\\nthe BERT model. Earth Sci. Inform. 2022 ,15, 573–584. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='23. Qiu, Q.; Xie, Z.; Ma, K.; Chen, Z.; Tao, L. Spatially oriented convolutional neural network for spatial relation extraction from\\nnatural language texts. Trans. GIS 2021 ,26, 839–866. [ CrossRef ]\\n24. Qiu, Q.; Xie, Z.; Ma, K.; Chen, Z.; Tao, L. Spatially oriented convolutional neural network for spatial relation extraction from\\nnatural language texts. Trans. GIS 2022 ,26, 839–866. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='25. Devlin, J.; Chang, M.W.; Lee, K.; Toutanova, K. Bert: Pre‑training of deep bidirectional transformers for language understanding.\\narXiv 2018 , arXiv:1810.04805.\\n26. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I. Language models are unsupervised multitask learners. OpenAI\\nblog 2019 ,1, 9.\\n27. Ling, W.; Dyer, C.; Black, A.W.; Trancoso, I. Two/too simple adaptations of word2vec for syntax problems. In Proceedings', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Denver, CO, USA, May–June 2015; pp. 1299–1304.\\n28. Lv, X.; Xie, Z.; Xu, D.; Jin, X.; Ma, K.; Tao, L.; Qiu, Q.; Pan, Y. Chinese Named Entity Recognition in the Geoscience Domain Based\\non BERT. Earth Space Sci. 2022 ,9, e2021EA002166. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='29. Ma, K.; Tian, M.; Tan, Y.; Xie, X.; Qiu, Q. What is this article about? Generative summarization with the BERT model in the\\ngeosciences domain. Earth Sci. Inform. 2021 ,15, 21–36. [ CrossRef ]\\n30. Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; Soricut, R. Albert: A lite bert for self‑supervised learning of language\\nrepresentations. arXiv 2019 , arXiv:1909.11942.\\n31. Hochreiter, S.; Schmidhuber, J. Long short‑term memory. Neural Comput. 1997 ,9, 1735–1780. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='32. Graves, A. Long short‑term memory. In Supervised Sequence Labelling with Recurrent Neural Networks ; Springer: Berlin/Heidelberg,\\nGermany, 2012; pp. 37–45.\\n33. Qiu, Q.; Xie, Z.; Wu, L.; Li, W. DGeoSegmenter: A dictionary‑based Chinese word segmenter for the geoscience domain. Comput.\\nGeosci. 2018 ,121, 1–11. [ CrossRef ]\\n34. Song, S.; Zhang, N.; Huang, H. Named entity recognition based on conditional random fields. Clust. Comput. 2017 ,22, 5195–5206.\\n[CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='[CrossRef ]\\n35. Guo, X.; Zhou, H.; Su, J.; Hao, X.; Tang, Z.; Diao, L.; Li, L. Chinese agricultural diseases and pests named entity recognition with\\nmulti‑scale local context features and self‑attention mechanism. Comput. Electron. Agric. 2020 ,179, 105830. [ CrossRef ]\\n36. Leitner, E.; Rehm, G.; Moreno‑Schneider, J. A dataset of german legal documents for named entity recognition. arXiv 2020 ,\\narXiv:2003.13016.', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='arXiv:2003.13016.\\n37. Wang, S.; Zhang, X.; Ye, P.; Du, M. Deep Belief Networks Based Toponym Recognition for Chinese Text. ISPRS Int. J. Geo‑Inf.\\n2018 ,7, 217. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 20}),\n",
              " Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 22 of 22\\n38. Wang, X.; Ma, C.; Zheng, H.; Liu, C.; Xie, P.; Li, L.; Si, L. DM NLP at SemEval 2018 Task 12: A pipeline system for toponym\\nresolution. In Proceedings of the 13th International Workshop on Semantic Evaluation, Minneapolis, MN, USA, 6–7 June 2019;\\npp. 917–923.\\n39. Wang, J.; Hu, Y.; Joseph, K. NeuroTPR: A neuro‑net toponym recognition model for extracting locations from social media\\nmessages. Trans. GIS 2020 ,24, 719–735. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 21}),\n",
              " Document(page_content='40. Ma, K.; Tan, Y.; Xie, Z.; Qiu, Q.; Chen, S. Chinese toponym recognition with variant neural structures from social media messages\\nbased on BERT methods. J. Geogr. Syst. 2022 ,24, 143–169. [ CrossRef ]\\n41. Qiu, Q.; Xie, Z.; Wang, S.; Zhu, Y.; Lv, H.; Sun, K. ChineseTR: A weakly supervised toponym recognition architecture based on\\nautomatic training data generator and deep neural network. Trans. GIS 2022 ,26, 1256–1279. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 21}),\n",
              " Document(page_content='42. Hu, X.; Zhou, Z.; Sun, Y.; Kersten, J.; Klan, F.; Fan, H.; Wiegmann, M. GazPNE2: A General Place Name Extractor for Microblogs\\nFusing Gazetteers and Pretrained Transformer Models. IEEE Internet Things J. 2022 ,9, 16259–16271. [ CrossRef ]', metadata={'source': 'pdfs/107. NER.pdf', 'page': 21})]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRMys8_I2hLj",
        "outputId": "53dffc42-cec5-44ea-8dbc-7e8d047300cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "214"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXEmh1kL2HJh",
        "outputId": "5b02be5d-3d16-4123-cba5-43637a986969"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Citation: Tao, L.; Xie, Z.; Xu, D.; Ma,\\nK.; Qiu, Q.; Pan, S.; Huang, B.\\nGeographic Named Entity\\nRecognition by Employing Natural\\nLanguage Processing and an\\nImproved BERT Model. ISPRS Int. J.\\nGeo‑Inf. 2022 ,11, 598. https://doi.org/\\n10.3390/ijgi11120598\\nAcademic Editors: Maria Antonia\\nBrovelli and Wolfgang Kainz\\nReceived: 15 September 2022\\nAccepted: 24 November 2022\\nPublished: 28 November 2022\\nPublisher’s Note: MDPI stays neutral\\nwith regard to jurisdictional claims in', metadata={'source': 'pdfs/107. NER.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJjBK4DH2Lc0",
        "outputId": "511e92c1-f7dd-4197-f2bc-c3544fbfe2e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='ISPRS Int. J. Geo‑Inf. 2022 ,11, 598 2 of 22\\nfirst subprocess of our approach is to identify the location of the mentioned contents; this\\nsubprocess is called entity recognition (NER) in NLP [ 10–13].\\nThere are single‑word place names, such as Beijing, Shanghai, Zhejiang, etc. There are\\nalso long place names composed of multiple words, such as Ejin Jinqi Saihantaolai Sumu\\nTownship (Inner Mongolia Autonomous Region); however, most of the place names are', metadata={'source': 'pdfs/107. NER.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_chunks[100].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oxuDUM_2OE2",
        "outputId": "82551a70-a0f8-4e96-8285-7d360d0387fc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text, such as news articles, and many vernacular words are not covered by those embed‑\n",
            "dings. When that happens, an embedding for a generic unknown token is usually used\n",
            "to represent this vernacular word and, as a result, the actual semantics of the word are\n",
            "lost. Second, compared with the basic BiLSTM–CRF model, our presented model adds an\n",
            "ALBERT layer to capture the dynamic and contextualized semantics of words.\n",
            "4.3. BiLSTM Layer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-I6QtQXxx4IvoTOh3QsyfT3BlbkFJf1Qve6gorT3IcMkMfpYK\""
      ],
      "metadata": {
        "id": "bGtmC_x12fLb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i4yIekW3_eq",
        "outputId": "19601ed4-5d8e-47fb-e2a2-e1d9731d5038"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.embed_query(\"Hello! How are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "VtZXw2FR4u5b",
        "outputId": "9d52f157-9b30-4ae1-b3de-2d7556dabe1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7132faf47d54>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello! How are you?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \"\"\"\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maembed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m#       than the maximum context and use length-safe embedding function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_len_safe_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     async def aembed_documents(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36m_get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mbatched_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             response = embed_with_retry(\n\u001b[0m\u001b[1;32m    495\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;34m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         )\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 931\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1016\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1016\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_api_key = os.environ.get('PINECONE_API_KEY', '5965fff2-b55f-4f7c-80ad-2dea835b8edd')\n",
        "pinecone_api_env = os.environ.get('PINECONE_API_ENV', 'gcp-starter')"
      ],
      "metadata": {
        "id": "hSCLO6if4340"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PINECONE_API_KEY\"] = '5965fff2-b55f-4f7c-80ad-2dea835b8edd'"
      ],
      "metadata": {
        "id": "dgWV4xfJlmjX"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(api_key = pinecone_api_key, environment = pinecone_api_env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "yV415mG67CJH",
        "outputId": "2a7b136d-8d97-48a6-df79-dc87b3cbdb02"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-960e0f89a4d5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpinecone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpinecone_api_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpinecone_api_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/deprecation_warnings.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc = PineconeGRPC(\n",
        "        api_key = pinecone_api_key\n",
        "    )"
      ],
      "metadata": {
        "id": "fEuRHpR_8pOD"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"testing\" # put in the name of your pinecone index here"
      ],
      "metadata": {
        "id": "PIhI8DY09CDR"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes().names()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujal-r2mjvdH",
        "outputId": "00667d82-aa56-40d4-f290-cf26fb2ee759"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['testing']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name = index_name,\n",
        "        dimension = 1536,\n",
        "        metric = \"cosine\",\n",
        "        spec = ServerlessSpec(\n",
        "            cloud = 'aws',\n",
        "            region = 'us-east-1'\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "TonJ7Ogcjlgv"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes().names()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV8rGHCjjzjn",
        "outputId": "c15e3a6c-0e79-49c4-c048-6eaae6eb354e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['testing']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index(index_name) #Accessing pinecone index"
      ],
      "metadata": {
        "id": "PU0ziRYKTskc"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "namespace = \"wondervector5000\"\n",
        "\n",
        "docsearch = PineconeVectorStore.from_documents(\n",
        "    documents = text_chunks,\n",
        "    index_name = index_name,\n",
        "    embedding = embeddings,\n",
        "    namespace = namespace\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "XqbWKnCBj9XE",
        "outputId": "5b176f82-1f59-4b2f-abc9-50efb0f76566"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-f708aa9d716e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"wondervector5000\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m docsearch = PineconeVectorStore.from_documents(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_chunks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindex_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, batch_size, text_key, namespace, index_name, upsert_kwargs, pool_threads, embeddings_chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mpinecone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpinecone_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         pinecone.add_texts(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mchunk_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membedding_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mchunk_metadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membedding_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             async_res = [\n\u001b[1;32m    159\u001b[0m                 self._index.upsert(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m#       than the maximum context and use length-safe embedding function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_len_safe_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     async def aembed_documents(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36m_get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mbatched_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             response = embed_with_retry(\n\u001b[0m\u001b[1;32m    495\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;34m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         )\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 931\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1016\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1016\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"YOLOv7 outperforms which models\""
      ],
      "metadata": {
        "id": "TdhqziSHbnMS"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "8CVZKxNN0O9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs\n",
        "\n",
        "#We get numbers\n",
        "'''\n",
        "[Document(page_content='YOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%'),\n",
        " Document(page_content='YOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% /55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% /56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% /56.8% 74.4% 62.1%'),\n",
        " Document(page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known'),\n",
        " Document(page_content='YOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - - - - -\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7% 31.7% 55.3% 64.7%\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9% 33.7% 57.1% 66.8%\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7% 18.8% 42.4% 51.9%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9% 31.8% 55.5% 65.0%\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8% 33.8% 57.1% 67.4%')]\n",
        " '''"
      ],
      "metadata": {
        "id": "K3nGTRPZ0TLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8AgAua81DVc",
        "outputId": "0e684706-65e8-4887-9e95-f9544b6d622f"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm = llm, chain_type = \"stuff\", retriever = docsearch.as_retriever())"
      ],
      "metadata": {
        "id": "n3wueHGk1HJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.run(query)\n",
        "#' YOLOv7 outperforms YOLOv5-L6, YOLOX-X, YOLOR-E6, PPYOLOE-X, YOLOv5-X6, and YOLOR-CSP.'"
      ],
      "metadata": {
        "id": "kN49HI3B1MyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == '':\n",
        "    continue\n",
        "  result = qa({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")\n",
        "\n",
        "'''\n",
        "Input Prompt:  what is a yolo?\n",
        "Answer:  YOLO (You Only Look Once) is a type of object detector, specifically a deep learning algorithm used for object detection in images and videos. It was developed by Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao from the Institute of Information Science at Academia Sinica in Taiwan. YOLOv7 is the latest version of the algorithm, which has set a new state-of-the-art for real-time object detectors in terms of speed and accuracy.\n",
        "Input Prompt:  who is invented the yolo?\n",
        "Answer:  Joseph Redmon and Ali Farhadi\n",
        "Input Prompt:  what was the accuracy of the yolov7?\n",
        "Answer:  The accuracy of the YOLOv7 was 56.8% AP test-dev / 56.8% AP min-val.\n",
        "Input Prompt:  exit\n",
        "Exiting\n",
        "'''"
      ],
      "metadata": {
        "id": "KunzUahJ1lz3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}